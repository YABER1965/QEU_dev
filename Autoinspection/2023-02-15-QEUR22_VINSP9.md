---
title: QEUR22_VINSP9: 　外観検査機の精度はなぜ必要か
date: 2023-02-15
tags: ["QEUシステム", "メトリックス", "Julia言語", "SOART", "外観検査", "機械学習"]
excerpt: julia言語とテクノメトリックスを使った外観検査自動機
---

## QEUR22_VINSP9: 　外観検査機の精度はなぜ必要か

## ～　残念！失敗！、祝！課題が見えてきた！！　～

### ・・・　今回は失敗だが、ここで「マイルストーン」になりました　・・・

D先生 ： “いよいよマハラノビス距離を使って異常検出をすることになります。ドキドキしますね・・・（笑）。まずは検出すべき不良モードを紹介します。“

![image3-10-1](/2023-02-15-QEUR22_VINSP9/image3-10-1.jpg)

D先生 ： “端子が傾いた状態と後退した状態をBlenderでバーチャルに設定して、画像をとり、それを分析します。次、FOUNDER、ヨロシク・・・。“

**（PINアドレス）**

![image3-10-2](/2023-02-15-QEUR22_VINSP9/image3-10-2.jpg)

**（画像）**

![image3-10-3](/2023-02-15-QEUR22_VINSP9/image3-10-3.jpg)

QEU:FOUNDER ： “計測するPINのアドレスは上下ROWの中間にあたる行であり、列は３番と6番で同じマハラノビス空間「Area-B」に属していると処理しています。この写真は、左カメラのものですが・・・。”

![image3-10-4](/2023-02-15-QEUR22_VINSP9/image3-10-4.jpg)

QEU:FOUNDER ： “左右のカメラを入力とした「両目RTメトリックス」を使った差分画像を形成すると上図のようになります。**「D」とは端子の後退、「X」とは端子の傾きを意味**しています。”

D先生 ： “うまく発見できていると評価する人もいるし、同時に「この程度しかできないのか」とネガティブが印象をうける人もいるかも・・・。**人間であればあきらかな不良なんですが機械が見つけるとなると簡単ではない**です。最先端の画像認識技術のResNet（例）だと見つかるかな・・・。”

QEU:FOUNDER ： “そんな強力なモノをつかうのであれば、人間が見つけるものは必ず検出するって・・・。QEUシステムの外観検査機のテーマ**はRaspberry Piレベルの軽い機械でも動けるようにする**ことです。それでは、前回に取ったSOARTメトリックスをプログラムに入力して、検出できるかを見てみましょう。”

```python
# mahalanobis example
import pandas as pd
import scipy as sp
from scipy.stats import chi2
import math
import numpy as np
# Graphic(Histgram)
import matplotlib.pyplot as plt

# ----
# データを読み込む
# ----
# train
filepath_train = 'soart_mtxout_train.csv'
df_org_train = pd.read_csv(filepath_train)

# test
filepath_test  = 'soart_mtxout_test.csv'
df_org_test  = pd.read_csv(filepath_test)

# ----
# DeltaをDropする
# ----
# train
df_org_train = df_org_train.drop(labels=['delta1', 'delta2', 'delta3', 'delta4', 'delta5', 'delta6'], axis=1)

# test
df_org_test  = df_org_test.drop(labels=['delta1', 'delta2', 'delta3', 'delta4', 'delta5', 'delta6'], axis=1)

```

![image3-10-5](/2023-02-15-QEUR22_VINSP9/image3-10-5.jpg)

D先生 ： “いきなり、「ダウト」～！！今回のプロジェクトでせっかく入れた**「Deltaメトリックス」**を消したんですか？“

QEU:FOUNDER ： “まず結論を言います。今回の実験は失敗です。ですから、それであえて、Deltaを外してシンプルに評価するようにしました。Deltaメトリックスについては、ある程度メドがついてから別途評価しましょう。”

D先生 ： “２つのデータベース、**「train」**と**「test」**があります。“

QEU:FOUNDER ： “「train」は前回の分散共分散マトリックスを形成するのに使用したモノです。全部が正常な状態です。一方、「test」には**「OK,61X10,61D05,31X10,31D05」**という群が50レコードづつ入っています。これは評価用です。”

D先生 ： “61と31は不良ピンアドレス、DとXは端子の後退と傾き。どれでは、10とか05は？”

QEU:FOUNDER ： “X10は**「X方向に傾きが10度」**という意味、D05は**「端子後退量が0.5」**といういみです。かなり大きめにとっているんですよ。それでは、つづきを見ていきましょう。”

```python
# ----
# マハラノビス距離を計算する
def mahalanobis(x=None, data=None, cov=None):
    """Compute the Mahalanobis Distance between each row of x and the data  
    x    : vector or matrix of data with, say, p columns.
    data : ndarray of the distribution from which Mahalanobis distance of each observation of x is to be computed.
    cov  : covariance matrix (p x p) of the distribution. If None, will be computed from data.
    """
    x_minus_mu = x - np.mean(data)
    if not cov:
        cov = np.cov(data.values.T)
    inv_covmat = np.linalg.inv(cov)
    left_term = np.dot(x_minus_mu, inv_covmat)
    mahal = np.dot(left_term, x_minus_mu.T)
    return mahal.diagonal()

# ----
# IDを生成する
def create_ID(df_x, area_df):

    # ----
    # temp_dfをdf_xに当てはめる
    area_df_iRow = area_df.loc[:,"iRow"].values
    area_df_jCol = area_df.loc[:,"jCol"].values
    arr_defect   = area_df.loc[:,"defect"].values
    df_x["iRow"] = area_df_iRow
    df_x["jCol"] = area_df_jCol
    # ----
    arr_ID = []
    for i in range(len(area_df_iRow)):
        val_ID = area_df_iRow[i]*10 + area_df_jCol[i]
        arr_ID.append(val_ID)
    # ----
    df_x["ID"] = arr_ID
    df_x["defect"] = arr_defect
    
    return df_x

# ----
# エリア別データベースを生成する
def create_Database(area_no, df_org_train, df_org_test):

    # train
    cut_dftrain = df_org_train[df_org_train["area"]==area_no]
    df_train = cut_dftrain.iloc[:, 5:17]

    # test
    cut_dftest  = df_org_test[df_org_test["area"]==area_no]
    df_test  = cut_dftest.iloc[:, 5:17]

    # ----
    # train <- train
    df_xtrain = df_train.copy()
    df_xtrain['mahala'] = mahalanobis(x=df_xtrain, data=df_train)
    #df_xtrain.head(20)

    # test <- train
    df_xtest = df_test.copy()
    df_xtest['mahala'] = mahalanobis(x=df_xtest, data=df_train)
    #df_xtest.head(20)

    # ----
    # train
    df_xtrain = create_ID(df_xtrain, cut_dftrain)

    # test
    df_xtest  = create_ID(df_xtest, cut_dftest)

    return df_xtrain, df_xtest

# ----
# エリア別データベースを生成する
# ----
# AREA-1のみを抽出する
area_no = 1
df_xtrain_no1, df_xtest_no1 = create_Database(area_no, df_org_train, df_org_test)

# AREA-2のみを抽出する
area_no = 2
df_xtrain_no2, df_xtest_no2 = create_Database(area_no, df_org_train, df_org_test)

# ----
# グラフを生成する(Distance)
def draw_Graph_distance(area_no, df_x):

    if area_no == 1:
    
        fig = plt.figure(figsize=(14,10))
        # ---
        arr_mahala_id11 = df_x[df_x.loc[:,"ID"]==11]["mahala"].values
        arr_mahala_id12 = df_x[df_x.loc[:,"ID"]==12]["mahala"].values
        arr_mahala_id21 = df_x[df_x.loc[:,"ID"]==21]["mahala"].values
        arr_mahala_id22 = df_x[df_x.loc[:,"ID"]==22]["mahala"].values
        arr_mahala_id31 = df_x[df_x.loc[:,"ID"]==31]["mahala"].values
        arr_mahala_id32 = df_x[df_x.loc[:,"ID"]==32]["mahala"].values
        # ---
        ax1 = fig.add_subplot(2,2,1)
        ax1.hist([arr_mahala_id11, arr_mahala_id12], bins=2*2*8, label=['ID11', 'ID12'])
        ax1.set_xlabel('Distance')
        ax1.set_ylabel('Freq')
        ax1.set_xlim(0,80)
        ax1.grid(True)
        ax1.legend(loc="best")
        # ---
        ax2 = fig.add_subplot(2,2,2)
        ax2.hist([arr_mahala_id21, arr_mahala_id22], bins=2*2*8, label=['ID21', 'ID22'])        
        ax2.set_xlabel('Distance')
        ax2.set_ylabel('Freq')
        ax2.set_xlim(0,80)
        ax2.grid(True)
        ax2.legend(loc="best")
        # ---
        ax3 = fig.add_subplot(2,2,3)
        ax3.hist([arr_mahala_id31, arr_mahala_id32], bins=2*2*8, label=['ID31', 'ID32'])
        ax3.set_xlabel('Distance')
        ax3.set_ylabel('Freq')
        ax3.set_xlim(0,80)
        ax3.grid(True)
        ax3.legend(loc="best")
        plt.show()

    else:
    
        fig = plt.figure(figsize=(14,10))
        # ---
        arr_mahala_id13 = df_x[df_x.loc[:,"ID"]==13]["mahala"].values
        arr_mahala_id14 = df_x[df_x.loc[:,"ID"]==14]["mahala"].values
        arr_mahala_id15 = df_x[df_x.loc[:,"ID"]==15]["mahala"].values
        arr_mahala_id16 = df_x[df_x.loc[:,"ID"]==16]["mahala"].values
        arr_mahala_id17 = df_x[df_x.loc[:,"ID"]==17]["mahala"].values
        # -
        arr_mahala_id23 = df_x[df_x.loc[:,"ID"]==23]["mahala"].values
        arr_mahala_id24 = df_x[df_x.loc[:,"ID"]==24]["mahala"].values
        arr_mahala_id25 = df_x[df_x.loc[:,"ID"]==25]["mahala"].values
        arr_mahala_id26 = df_x[df_x.loc[:,"ID"]==26]["mahala"].values
        arr_mahala_id27 = df_x[df_x.loc[:,"ID"]==27]["mahala"].values
        # -
        arr_mahala_id33 = df_x[df_x.loc[:,"ID"]==33]["mahala"].values
        arr_mahala_id34 = df_x[df_x.loc[:,"ID"]==34]["mahala"].values
        arr_mahala_id35 = df_x[df_x.loc[:,"ID"]==35]["mahala"].values
        arr_mahala_id36 = df_x[df_x.loc[:,"ID"]==36]["mahala"].values
        arr_mahala_id37 = df_x[df_x.loc[:,"ID"]==37]["mahala"].values
        
        # ---
        ax1 = fig.add_subplot(2,2,1)
        ax1.hist([arr_mahala_id13, arr_mahala_id14, arr_mahala_id15, arr_mahala_id16, arr_mahala_id17], 
                    bins=5*2*8, label=['ID13', 'ID14', 'ID15', 'ID16', 'ID17'])           
        ax1.set_xlabel('Distance')
        ax1.set_ylabel('Freq')
        ax1.set_xlim(0,80)        
        ax1.grid(True)
        ax1.legend(loc="best")
        # ---
        ax2 = fig.add_subplot(2,2,2)
        ax2.hist([arr_mahala_id23, arr_mahala_id24, arr_mahala_id25, arr_mahala_id26, arr_mahala_id27], 
                    bins=5*2*8, label=['ID23', 'ID24', 'ID25', 'ID26', 'ID27'])       
        ax2.set_xlabel('Distance')
        ax2.set_ylabel('Freq')
        ax2.set_xlim(0,80)       
        ax2.grid(True)
        ax2.legend(loc="best")
        # ---
        ax3 = fig.add_subplot(2,2,3)
        ax3.hist([arr_mahala_id33, arr_mahala_id34, arr_mahala_id35, arr_mahala_id36, arr_mahala_id37], 
                    bins=5*2*8, label=['ID33', 'ID34', 'ID35', 'ID36', 'ID37'])
        ax3.set_xlabel('Distance')
        ax3.set_ylabel('Freq')
        ax3.set_xlim(0,80)
        ax3.grid(True)
        ax3.legend(loc="best")
        plt.show()

# ----
# test(全体感)
# ----
# AREA = 1
area_no = 1
draw_Graph_distance(area_no, df_xtest_no1)

```

![image3-10-6](/2023-02-15-QEUR22_VINSP9/image3-10-6.jpg)

```python
# AREA = 2
area_no = 2
draw_Graph_distance(area_no, df_xtest_no2)
```

![image3-10-7](/2023-02-15-QEUR22_VINSP9/image3-10-7.jpg)

D先生 ： “ひごろ、FOUNDERが「わかりにくくて嫌い（笑）」とおっしゃるマハラノビス距離 ですね。“

QEU:FOUNDER ： “評価用の「testデータベース」を使って計算しています。AREA-1とAREA-2の事例を出していますが、今回の不良はAREA-2だけなので、後はAREA-2のみを扱います。続けましょう。”

```python
# ----
# pValueメトリックスの変換
def change_pValue(df_x):

    # Compute the P-Values
    arr_pVal = 1 - chi2.cdf(df_x.loc[:,'mahala'], 12)
    arr_logP = []
    for i in range(len(arr_pVal)):
        if arr_pVal[i] > 0.000001:
            val_logP = math.log10(arr_pVal[i])
        else:
            val_logP = -6.0
        arr_logP.append(-val_logP)
    # ---
    df_x.loc[:, 'p_value'] = arr_logP

    return df_x

# ----
# グラフを生成する(pValue)
def draw_Graph_pValue(area_no, df_x):

    # pValueメトリックスの変換
    df_x = change_pValue(df_x)
    
    # ---
    if area_no == 1:
    
        fig = plt.figure(figsize=(14,10))
        # ---
        arr_pVal_id11 = df_x[df_x.loc[:,"ID"]==11]["p_value"].values
        arr_pVal_id12 = df_x[df_x.loc[:,"ID"]==12]["p_value"].values
        arr_pVal_id21 = df_x[df_x.loc[:,"ID"]==21]["p_value"].values
        arr_pVal_id22 = df_x[df_x.loc[:,"ID"]==22]["p_value"].values
        arr_pVal_id31 = df_x[df_x.loc[:,"ID"]==31]["p_value"].values
        arr_pVal_id32 = df_x[df_x.loc[:,"ID"]==32]["p_value"].values
        # ---
        ax1 = fig.add_subplot(2,2,1)
        ax1.hist([arr_pVal_id11, arr_pVal_id12], bins=2*4*4, label=['ID11', 'ID12'])
        ax1.set_xlabel('LOG(p_val)')
        ax1.set_ylabel('Freq')
        ax1.set_xlim(0,4)
        ax1.grid(True)
        ax1.legend(loc="best")
        # ---
        ax2 = fig.add_subplot(2,2,2)
        ax2.hist([arr_pVal_id21, arr_pVal_id22], bins=2*4*4, label=['ID21', 'ID22'])
        ax2.set_xlabel('LOG(p_val)')
        ax2.set_ylabel('Freq')
        ax2.set_xlim(0,4)
        ax2.grid(True)
        ax2.legend(loc="best")
        # ---
        ax3 = fig.add_subplot(2,2,3)
        ax3.hist([arr_pVal_id31, arr_pVal_id32], bins=2*4*4, label=['ID31', 'ID32'])
        ax3.set_xlabel('LOG(p_val)')
        ax3.set_ylabel('Freq')
        ax3.set_xlim(0,4)
        ax3.grid(True)
        ax3.legend(loc="best")
        plt.show()

    # ---
    elif area_no == 3:
    
        fig = plt.figure(figsize=(14,10))
        # ---
        arr_pVal_id14 = df_x[df_x.loc[:,"ID"]==14]["p_value"].values
        arr_pVal_id17 = df_x[df_x.loc[:,"ID"]==17]["p_value"].values
        arr_pVal_id24 = df_x[df_x.loc[:,"ID"]==24]["p_value"].values
        arr_pVal_id27 = df_x[df_x.loc[:,"ID"]==27]["p_value"].values
        arr_pVal_id34 = df_x[df_x.loc[:,"ID"]==34]["p_value"].values
        arr_pVal_id37 = df_x[df_x.loc[:,"ID"]==37]["p_value"].values
        # ---
        ax1 = fig.add_subplot(2,2,1)
        ax1.hist([arr_pVal_id14, arr_pVal_id17], bins=2*4*4, label=['ID14', 'ID17'])
        ax1.set_xlabel('LOG(p_val)')
        ax1.set_ylabel('Freq')
        ax1.set_xlim(0,4)
        ax1.grid(True)
        ax1.legend(loc="best")
        # ---
        ax2 = fig.add_subplot(2,2,2)
        ax2.hist([arr_pVal_id24, arr_pVal_id27], bins=2*4*4, label=['ID24', 'ID27'])
        ax2.set_xlabel('LOG(p_val)')
        ax2.set_ylabel('Freq')
        ax2.set_xlim(0,4)
        ax2.grid(True)
        ax2.legend(loc="best")
        # ---
        ax3 = fig.add_subplot(2,2,3)
        ax3.hist([arr_pVal_id34, arr_pVal_id37], bins=2*4*4, label=['ID34', 'ID37'])
        ax3.set_xlabel('LOG(p_val)')
        ax3.set_ylabel('Freq')
        ax3.set_xlim(0,4)
        ax3.grid(True)
        ax3.legend(loc="best")
        plt.show()

    else:
    
        fig = plt.figure(figsize=(14,10))
        # ---
        arr_pVal_id13 = df_x[df_x.loc[:,"ID"]==13]["p_value"].values
        arr_pVal_id14 = df_x[df_x.loc[:,"ID"]==14]["p_value"].values
        arr_pVal_id15 = df_x[df_x.loc[:,"ID"]==15]["p_value"].values
        arr_pVal_id16 = df_x[df_x.loc[:,"ID"]==16]["p_value"].values
        arr_pVal_id17 = df_x[df_x.loc[:,"ID"]==17]["p_value"].values
        # -
        arr_pVal_id23 = df_x[df_x.loc[:,"ID"]==23]["p_value"].values
        arr_pVal_id24 = df_x[df_x.loc[:,"ID"]==24]["p_value"].values
        arr_pVal_id25 = df_x[df_x.loc[:,"ID"]==25]["p_value"].values
        arr_pVal_id26 = df_x[df_x.loc[:,"ID"]==26]["p_value"].values
        arr_pVal_id27 = df_x[df_x.loc[:,"ID"]==27]["p_value"].values
        # -
        arr_pVal_id33 = df_x[df_x.loc[:,"ID"]==33]["p_value"].values
        arr_pVal_id34 = df_x[df_x.loc[:,"ID"]==34]["p_value"].values
        arr_pVal_id35 = df_x[df_x.loc[:,"ID"]==35]["p_value"].values
        arr_pVal_id36 = df_x[df_x.loc[:,"ID"]==36]["p_value"].values
        arr_pVal_id37 = df_x[df_x.loc[:,"ID"]==37]["p_value"].values
        
        # ---
        ax1 = fig.add_subplot(2,2,1)
        ax1.hist([arr_pVal_id13, arr_pVal_id14, arr_pVal_id15, arr_pVal_id16, arr_pVal_id17],
                    bins=5*4*2, label=['ID13', 'ID14', 'ID15', 'ID16', 'ID17'])
        ax1.set_xlabel('LOG(p_val)')
        ax1.set_ylabel('Freq')
        ax1.set_xlim(0,4)
        ax1.grid(True)
        ax1.legend(loc="best")
        # ---
        ax2 = fig.add_subplot(2,2,2)
        ax2.hist([arr_pVal_id23, arr_pVal_id24, arr_pVal_id25, arr_pVal_id26, arr_pVal_id27], 
                    bins=5*4*2, label=['ID23', 'ID24', 'ID25', 'ID26', 'ID27'])
        ax2.set_xlabel('LOG(p_val)')
        ax2.set_ylabel('Freq')
        ax2.set_xlim(0,4)
        ax2.grid(True)
        ax2.legend(loc="best")
        # ---
        ax3 = fig.add_subplot(2,2,3)
        ax3.hist([arr_pVal_id33, arr_pVal_id34, arr_pVal_id35, arr_pVal_id36, arr_pVal_id37], 
                    bins=5*4*2, label=['ID33', 'ID34', 'ID35', 'ID36', 'ID37'])
        ax3.set_xlabel('LOG(p_val)')
        ax3.set_ylabel('Freq')
        ax3.set_xlim(0,4)
        ax3.grid(True)
        ax3.legend(loc="best")
        plt.show()

# ----
# test(全体感)
# ----
# AREA = 1
area_no = 1
draw_Graph_pValue(area_no, df_xtest_no1)

```

![image3-10-8](/2023-02-15-QEUR22_VINSP9/image3-10-8.jpg)

D先生 ： “ん？なんだこのメトリックスは！？マハラノビス距離でもないし、カイ2乗分布から計算したP値でもない。 “

QEU:FOUNDER ： “**P値をLOG１０で変換し、さらにマイナス変換をしたもの**です。こうすれば、いかなる項目数にもかかわらず、**その値が2以上になると数学的に異常になります**。”

D先生 ： “なるほど、それは便利・・・。そうすると、AREA-1ではごく少数の過検出が発生していますね。 “

QEU:FOUNDER ： “そういうこと。じゃあ、つづけます・・・。”

```python
# AREA = 3
area_no = 3
draw_Graph_pValue(area_no, df_xtest_no2)
```


     (image3-10-9)
![image3-10-1](/2023-02-15-QEUR22_VINSP9/image3-10-1.jpg)

D先生 ： “なに？Area_noが3も評価するんですか？ “

QEU:FOUNDER ： “これは便宜的な数字であり、実際にはArea-2をやっています。Area-2の場合、各行には5つのピンがあり、それをヒストグラムに貼り付けるとわかりにくくなるものですから、3番と6番だけを拾ってきました。”

D先生 ： “異常が出て来ていませんね。testデータベースのほとんどは不良データなのに・・・。“

QEU:FOUNDER ： “不良モード毎にみてみましょう・・・。”

```python
# ----
# test(層別)
# defect = OK, 31X10, 31D05, 61X10, 61D05
# ----
# AREA = 3
area_no = 3

# defect = OK
df_xtest_temp = df_xtest_no2[df_xtest_no2.loc[:,"defect"] == "OK"]
draw_Graph_pValue(area_no, df_xtest_temp)
```

![image3-10-10](/2023-02-15-QEUR22_VINSP9/image3-10-10.jpg)

```python
# defect = 31X10
df_xtest_temp = df_xtest_no2[df_xtest_no2.loc[:,"defect"] == "31X10"]
draw_Graph_pValue(area_no, df_xtest_temp)
```

![image3-10-11](/2023-02-15-QEUR22_VINSP9/image3-10-11.jpg)

```python
# defect = 31D05
df_xtest_temp = df_xtest_no2[df_xtest_no2.loc[:,"defect"] == "31D05"]
draw_Graph_pValue(area_no, df_xtest_temp)
```

![image3-10-12](/2023-02-15-QEUR22_VINSP9/image3-10-12.jpg)

```python
# defect = 61X10
df_xtest_temp = df_xtest_no2[df_xtest_no2.loc[:,"defect"] == "61X10"]
draw_Graph_pValue(area_no, df_xtest_temp)
```

![image3-10-13](/2023-02-15-QEUR22_VINSP9/image3-10-13.jpg)

```python
# defect = 61D05
df_xtest_temp = df_xtest_no2[df_xtest_no2.loc[:,"defect"] == "61D05"]
draw_Graph_pValue(area_no, df_xtest_temp)
```

![image3-10-14](/2023-02-15-QEUR22_VINSP9/image3-10-14.jpg)

D先生 ： “なるほど、失敗だ・・・。確認しますが、deltaメトリックスを入れても同じ感じだったんですね？ “
 
![image3-10-15](/2023-02-15-QEUR22_VINSP9/image3-10-15.jpg)

QEU:FOUNDER ： “ほとんどかわりません。さて、失敗の総括だが、この微分写真(↑)をどう思いますか？”

D先生 ： “**ばらつきすぎ**です。**「地合の明るさ」**が違いすぎます。・・・ということは、**検査機の環境精度**に問題があったか・・・。 “

QEU:FOUNDER ： “一言でいえばそうだが、ロジックで少しは補正してほしいよね。**微分画像をつくるロジックに問題があったんじゃない**かというのが、小生の第一の推測です。当該部分のプログラムを紹介します。”

```python
	# -----
	# 画像化
	norm_metrics = (mx_metrics .- minimum(mx_metrics))/(maximum(mx_metrics) - mini-mum(mx_metrics))
	img_uint = convert(Array{Gray{Normed{UInt8, 8}}, 2}, norm_metrics)

```

D先生 ： “min-maxで規格化(normalize)していますね。”

QEU:FOUNDER ： “Julia言語のGrayscaleは、画素の値が0～1になっている必要があるので単純にそうしました。ただし、このデータってRTメトリックスの値なので本来は画像ではないです。もうちょっと気の利いた変換をすべきだったと・・・(笑)。”

D先生 ： “min-max変換が画像に悪影響を及ぼし、色が変わってしまったと・・・。ついでにいえば、前回(ROUND2-1)の実験では、異常を比較的うまく検出していたと思います。何がかわったんですか？ “
 
![image3-10-16](/2023-02-15-QEUR22_VINSP9/image3-10-16.jpg)

QEU:FOUNDER ： “ロジックがかなり変わっているので、細かくは説明できません。ただし、今回は敢えて強気でばらつきを増やしたのが失敗の原因の一つです。あと、前回はSTEP1の中間メトリックスの段階で一回マハラノビス距離を計算してました。前回は**ダブル・マハラノビス距離**だったんです。”

D先生 ： “今回は、2種類のRTメトリックス（感度、SN比）のうち感度を敢えて外しました。マハラノビス距離の意味が少ないだろうと・・・。その判断の妥当性を評価する必要がありますね。まだまだ、やれることが多くありますね。 “

QEU:FOUNDER ： “外観検査自動機の開発をしていつも思うことは、**「人間の脳（認識力）は偉大だな」**ということだね・・・。じゃあ、次は一旦閑話休題をやります。せっかく失敗したんだから、このデータを活用しましょう。”


## ～　まとめ　～

C部長 : “この人の話(↓)、面白かったですね。”

[![MOVIE1](http://img.youtube.com/vi/ReoJcerYtuI/0.jpg)](http://www.youtube.com/watch?v=ReoJcerYtuI "あなたの仕事が劇的に変わる!? チャットAI使いこなし最前線")

QEU:FOUNDER ： “こんな使い方もあるんですね。ホントに驚きました。しかし、会話に「確率分布」って言葉が頻繁に出て来て、「おや？」とは思ったが・・・。”

C部長 : “強化学習には確率分布という概念がありましたっけ・・・。”

QEU:FOUNDER ： “しらん。小生もGPTの技術を知らないからね。Policy Basedであれば、（確率分布の考え方は）あるかな・・・。まあ、いいや・・・。とにかく彼の考え方は、この人（↓）の話にうまくシンクロするんですね。”

[![MOVIE1](http://img.youtube.com/vi/QmgmEyPJhp8/0.jpg)](http://www.youtube.com/watch?v=QmgmEyPJhp8 "「ChatGPT」強化学習リーダーは、日本生まれで６カ国渡った元Google Brainの研究者。OpenAIのシェイン･グウが語る、”生成AI時代”の日本の「強みと新たな可能性」とは？")

C部長 : “おおっ・・・。あの**「G-OA社」のエンジニア**ですね。”

QEU:FOUNDER ： “G社は、いま「どのようなテクストを入力すれば、イケているアウトプットがでるのか」を研究しているんです。”

C部長 : “これからは、プログラムも数学もいらないですね。”

QEU:FOUNDER ： “とうとう、そういう時代になったんです。”

