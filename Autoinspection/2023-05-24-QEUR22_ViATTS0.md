---
title: QEUR22_ViATTS0:　今話題のAttentionを使ってみる(外観検査- その1)
date: 2023-05-24
tags: ["QEUシステム", "メトリックス", "Julia言語", "SOART", "外観検査", "機械学習"]
excerpt: julia言語とテクノメトリックスを使った外観検査自動機
---

## QEUR22_ViATTS0:　今話題のAttentionを使ってみる(外観検査- その1) 

## ～　よくも、こんなにすごいメトリックスを発明したもんだ　～

D先生 ： “この外観検査自動機のプロジェクトでCNNの代替としてRT法が使えることをすでに証明しています。さらに、Embeddingの代替としてT法が使えることがわかったわけです。”

![image3-40-1](/2023-05-24-QEUR22_ViATTS0/image3-40-1.jpg)

QEU:FOUNDER  ： “率直、今後ますますディープラーニングを使ったAIが存在感を高めることは自明です。そして、必然的にタグチメソッドは「昔の名前で・・・」になっていくでしょう。それでも、我々の試みを通じて、次世代に何らかの遺伝子を残せるとうれしい・・・。”

D先生 ： “そうですね・・・。でも、なぜいきなり外観検査自動機のプロジェクトを再開することにしたんですか？”

QEU:FOUNDER  ： “今、話題のAttentionでも使ってみたいんで・・・。”

C部長  ： “ん？注意(Attention)がなんだって・・・？”

QEU:FOUNDER  ： “そう、そのAttentionです・・・（笑）。今回は、**Attention**についてのわかりやすい説明をしたブログ(↓)を見つけたので、それをまずは見てみましょう。”

![image3-40-2](/2023-05-24-QEUR22_ViATTS0/image3-40-2.jpg)

C部長  ： “Attentionって、今調べたら**NLP（自然言語処理）分野で圧倒的な性能をたたき出したディープラーニング手法**じゃないですか・・・。そんなもの、我々が手を出すのはちょっと・・・。”

QEU:FOUNDER  ： “しかし、やってみましょう。何はともあれ、なにも知らないよりマシ。それではプログラムと実行結果をドン！！前述のブログの内容とほとんど同じですが、この手法の本質をわかりやすくするために重みの形を正方行列(3x3)ではなく、長方形(4x3)にしています。”

```python
# 今話題のAttentionをやってみる
from numpy import array
from numpy import random
from numpy import dot
from scipy.special import softmax

# この例題では、入力文は「word1-2-3-4」で成り立っている
# 語彙はエンベディングでベクトル（4ケタ）に変換されている
word_1 = array([1, 0, 1, 0])
word_2 = array([0, 1, 0, 1])
word_3 = array([1, 1, 0, 0])
word_4 = array([0, 0, 1, 0])

words = array([word_1, word_2, word_3, word_4])
print(words)
#[[1 0 1 0]
# [0 1 0 1]
# [1 1 0 0]
# [0 0 1 0]]

# -----
# 重み（Weight）を作る
# このWを機械学習でつくるか、それとも他の方法で作るのかはアナタ次第
# ここでは、デモのためにランダム生成
random.seed(42) # to allow us to reproduce the same attention values
W_Q = random.randint(3, size=(4, 3))
W_K = random.randint(3, size=(4, 3))
W_V = random.randint(3, size=(4, 3))
print("--Q--")
print(W_Q)
print("--K--")
print(W_K)
print("--V--")
print(W_V)
#--Q--
#[[2 0 2]
# [2 0 0]
# [2 1 2]
# [2 2 2]]
#--K--
#[[0 2 1]
# [0 1 1]
# [1 1 0]
# [0 1 1]]
#--V--
#[[0 0 0]
# [2 2 2]
# [1 2 1]
# [1 2 1]]

# -----
# ここで重み行列を正方にしていないことに注意。エンベッドはタテです。
# 行列計算が行われます。
Q = words @ W_Q
K = words @ W_K
V = words @ W_V
print("--Q--")
print(Q)
print("--K--")
print(K)
print("--V--")
print(V)
#--Q--
#[[4 1 4]
# [2 0 0]
# [4 0 2]
# [2 1 2]]
#--K--
#[[2 3 3]
# [0 2 1]
# [2 4 3]
# [0 1 1]]
#--V--
#[[1 1 0]
# [0 1 1]
# [1 2 1]
# [0 0 0]]

# -----
scores = Q @ K.transpose()
print(scores)
#[[11 10 11  5]
# [12  8 10  6]
# [ 6  4  4  4]
# [ 7  6  7  3]]

# -----
# Softmax関数で確率値(probability value)の群に計算されます
probs = softmax(scores / K.shape[1] ** 0.5)
print(probs)
#[[0.1804795  0.10131829 0.1804795  0.00564921]
# [0.32149034 0.03193065 0.10131829 0.01006301]
# [0.01006301 0.00317138 0.00317138 0.00317138]
# [0.01792535 0.01006301 0.01792535 0.00178036]]

# -----
# 最後にattention値が計算されます
# 大きな値を持つ要素が1個と、あとは無視できる値の要素群になる
attention = probs @ V 
print(probs)
#[[9.92901880e-01 1.62883281e+00 6.35930933e-01]
# [1.22920950e-05 1.90485723e-05 6.75647734e-06]
# [3.95390534e-03 5.93279494e-03 1.97888961e-03]
# [3.08678210e-03 5.06988081e-03 1.98309872e-03]]

```

C部長  ： “Attentionって、ずいぶん面白い手法なんですね。インプットと学習データ（重み）を比較し、処理をすると、**インプットの中で最も注意すべき要素をピンポイントで指摘してくれる**んですね。”

QEU:FOUNDER ： “このマジックのタネあかしです。この処理（↓）が効いているんですね。”


```python
# -----
# ここで重み行列を正方にしていないことに注意。エンベッドはタテです。
# 行列計算が行われます。
Q = words @ W_Q
K = words @ W_K
V = words @ W_V
```

QEU:FOUNDER ： “インプットと重みの行列計算がポイントです。この処理はインプットベクトルと重みベクトルの内積計算と解釈できます。この先生（↓）によると、内積とは類似度のメトリックス化のことだから、重みが意味する情報に最も近いインプットベクトルの要素はどこになるのかを示しているんです。”

[![MOVIE1](http://img.youtube.com/vi/bPdyuIebXWM/0.jpg)](http://www.youtube.com/watch?v=bPdyuIebXWM "【深層学習】Attention - 全領域に応用され最高精度を叩き出す注意機構の仕組み")


C部長  ： “この重み行列って、ディープラーニングで計算するしかないんですか？”

D先生 ： “さあ・・・。バック・プロパゲーションで形成するのが一般的ではあるのだが・・・。”

QEU:FOUNDER  ： “小生はAttentionという手法をあくまでメトリックスを生成する手段としてみているんです。そうすることで、いったい何ができるのか？是非、カンパをお願いします！！”


### [＞寄付のお願い(click here)＜](https://www.paypal.com/paypalme/QEUglobal?v=1&utm_source=unp&utm_medium=email&utm_campaign=RT000481&utm_unptid=29844400-7613-11ec-ac72-3cfdfef0498d&ppid=RT000481&cnac=HK&rsta=en_GB%28en-HK%29&cust=5QPFDMW9B2T7Q&unptid=29844400-7613-11ec-ac72-3cfdfef0498d&calc=f860991d89600&unp_tpcid=ppme-social-business-profile-created&page=main%3Aemail%3ART000481&pgrp=main%3Aemail&e=cl&mchn=em&s=ci&mail=sys&appVersion=1.71.0&xt=104038)
 
 
D先生 ： “常に新しい着眼点を持つことは良いことです。”


## ～　まとめ　～

QEU:FOUNDER ： “さいきん、イマいちなんだよね・・・。”

[![MOVIE1](http://img.youtube.com/vi/ZOemMNtkvio/0.jpg)](http://www.youtube.com/watch?v=ZOemMNtkvio "中國汽車竟超越日本！今年首季中國汽車榮登出口世界第一！詳細分析背後有哪三大原因？")

C部長 : “えっ！？J国の**自動車輸出量が負けた**！？”

QEU:FOUNDER ： “短い期間の「いまのところは」だよ。このおじさん（↑）は、やさしいのでフォローしています。問題ないって・・・。”


- J国メーカは生産拠点を海外展開されているので、もともと輸出は縮小傾向
- 流行り病などの影響で半導体が少なくなったが、C国は影響がすくなかった
- C国の電気自動車（EV）の輸出が増加傾向


C部長 : “な～んだ・・・。問題ないんですね。じゃあ、この話題はここまで・・・（笑）。“

QEU:FOUNDER ： “最後にこれだけを見ておけば、終わりでよし。”

![image3-40-3](/2023-05-24-QEUR22_ViATTS0/image3-40-3.jpg)

C部長 : “なんですか、コレ？”

QEU:FOUNDER ： “C国自動車の主要輸出国の推移表です。”

C部長 : “あれ？**東南アジアの国の名前**が・・・。”

D先生： “なんででしょうか・・・。”

