---
title: 技術まとめ　～　ワイヤーハーネスのコネクタ端子検査における外観検査自動機
date: 2023-02-23
tags: ["QEUシステム", "メトリックス", "Julia言語", "SOART", "外観検査", "機械学習"]
excerpt: julia言語とテクノメトリックスを使った外観検査自動機
---

## 技術まとめ　～　ワイヤーハーネスのコネクタ端子検査における外観検査自動機

### 【発明の名称】　
ワイヤーハーネスのコネクタ端子検査における外観検査自動機

### 【技術分野】  
【０００１】
本発明はワイヤーハーネスのコネクタ端子検査におけるコンピューターによる外観検査自動機に関するものです。

### 【背景技術】
【０００２】
機械学習の技術コンペを主宰しているKaggleによれば、2022年の機械学習のトレンドの一つに**TinyML**があります。TinyMLはIoTのエッジコンピューターでも動く小規模の機械学習アプリケーションのことであり、機械学習の技術はすでに普及段階に入ったといえます (図1,図2)。

**(図1:Kaggleの記事)**

![image3-20-1](/2023-02-23-QEUR22_VINSP19/image3-20-1.jpg)

**(図2:TinyMLについて)**

![image3-20-2](/2023-02-23-QEUR22_VINSP19/image3-20-2.jpg)

【０００３】
人間の目視観察による検査（外観検査）のパフォーマンスを図3に示します。一般に、**外観検査員による欠陥検出率は最大で80％程度**です。そして、そのパフォーマンスは検査員の肉体的、精神的なコンディションによって大きく変化します(図4)。

**(図3: 外観検査のパフォーマンス)**

![image3-20-3](/2023-02-23-QEUR22_VINSP19/image3-20-3.jpg)

**(図4: 検査員のコンディションと見逃し)**

![image3-20-4](/2023-02-23-QEUR22_VINSP19/image3-20-4.jpg)

【０００４】
本発明ではワイヤーハーネス（以下WH）のコネクタ端子検査にTinyMLを応用します。WHとは図5に示すような電気系統の配線(組立)を容易するために電線とコネクタをまとめたものです。WHの製造作業は複雑であり、人間が手作業で製造しています。その結果、製品の不良率がほかの製品よりも高くなります。また、製品不良のうちコネクタと端子の不良は製品の基本性能（導通の安定性）に直結するために重大不良とみなされます。ちなみに、コネクタにはオスとメスの2種類あり、(端子)不良が発生しやすいのはオスコネクタの方です。“

**(図5：ワイヤーハーネスとは)**

![image3-20-5](/2023-02-23-QEUR22_VINSP19/image3-20-5.jpg)

**(図6: コネクタの端子検査)**

![image3-20-6](/2023-02-23-QEUR22_VINSP19/image3-20-6.jpg)

【０００５】
自動検査を行う場合、図7に示す製品実現プロセスにおける量産試作段階で学習データを収集する必要があります。ただし、一般には試作する製品数はせいぜい200pcs程度であり複雑な機械学習に必要な学習データを得ることができません。

**(図7:  製品実現プロセス)**

![image3-20-7](/2023-02-23-QEUR22_VINSP19/image3-20-7.jpg)

【０００６】
そこで、量産試作より前の設計開発(R&D)段階でデータを集める必要があります。そのためには**VR（仮想空間, Virtual Reality）**で学習データを準備する必要があります。オス端子のコネクタの場合、3DCG制作ツールがあれば簡単に学習データを収集できます(図8)。

**(図8: VRによるコネクタのイメージの制作)**

![image3-20-8](/2023-02-23-QEUR22_VINSP19/image3-20-8.jpg)

【０００７】
人間がコネクタの端子検査をする場合には非常に簡単に見えるのですが、この作業を機械が行うことは著しく難しくなります。ここで例として、「端子抜け」という代表的な不良項目を考えてみましょう。端子抜けとは端子をコネクタに挿入されたとき、コネクタの設計通りに端子をロック（固定）できす、端子が後退したり傾いたりする事象です。

**(図9: 端子が後退した状態)**

![image3-20-9](/2023-02-23-QEUR22_VINSP19/image3-20-9.jpg)

**(図10: 端子が傾いた状態)**

![image3-20-10](/2023-02-23-QEUR22_VINSP19/image3-20-10.jpg)

【０００８】
端子の傾きは1枚の画像だけで検出できます。しかし、垂直に端子が後退した場合には1枚の画像では検出できないこともあります(図11)。つまり、立体物の異常を検出することは、画像に写った物体の種類を高精度のDeep learningで分類することとは別の難しさがあるのです。

**(図11: 画像認識の例)**

![image3-20-11](/2023-02-23-QEUR22_VINSP19/image3-20-11.jpg)

### 【発明の概要】
### 【発明が解決しようとする課題】
【０００９】
品質管理の理論的基礎を作ったエドワード・デミング博士、田口玄一博士は検査作業に関して**コストに基づくコントロールをすべき**だと提唱しました。その考え方が、図12に示す**「臨界検査点」**に基づく全数検査の要否判定です。

**(図12: 「臨界検査点」に基づく全数検査の要否判定)**

![image3-20-12](/2023-02-23-QEUR22_VINSP19/image3-20-12.jpg)

【００１０】
検査にかかわるトータル損失を可視化する場合、生産（検査）量に対して一定の設備費の線と量に比例する損失量の線を引きます。全損失はその2つの線の和になります。そして、その2つの線が交差する点が「検査臨界点(臨界量x、臨界損失y)」になり、その点以下の生産量では全数検査は割が合わず、その生産量以上では割があいます。

**(図13:  Cpk不良率と検査戦略)**

![image3-20-13](/2023-02-23-QEUR22_VINSP19/image3-20-13.jpg)

【００１１】
Six SigmaやPPAPなどの一般的な技術標準では、**Cpkが1.6以上は無検査、1.2以上がサンプリング検査、1.0以下が全数検査**になります。しかし、業界や製品特性によって事情が違うため、上記のコストで評価したほうが実情にあうでしょう。

【００１２】
以上の検査臨界点の議論から2つの知見が得られます。**不良率が高いものは全数検査の割があいやすいこと。もう一つは設備コストが安いほど割に合うこと**です。

【００１３】
全数外観検査の実際のコスト構造をグラフにまとめてみました。現在の技術では多くの場合、人間が検査をしています。そのため、生産量が多くなると検査員を増員する必要があります。不良率や検査員の人件費と臨界不良率の関係は前述のとおりです。

**(図14: （全数）外観検査の実際)**

![image3-20-14](/2023-02-23-QEUR22_VINSP19/image3-20-14.jpg)

【００１４】
上記のグラフでは、外観検査設備の費用は人件費より高いと設定しています。ただし、図15に示すように、**月当たりの減価償却金額は人件費よりも高いケースはほとんどない**でしょう。

**(図15: 検査戦略における設備費用の実際)**

![image3-20-15](/2023-02-23-QEUR22_VINSP19/image3-20-15.jpg)

【００１５】
つまり、全数外観検査が必要な場合には「技術的に可能か（発見できるか）」がもっとも重要な要素になります。

### 【課題を解決するための手段】
【００１６】
量産直後に外観検査自動機を稼働させるには、設計開発段階の前に学習を完了させておく必要があります。そのためにVRテクノロジを利用します。さらに、立体の異常を検出するために図16と図17に示すようにVR内に複数のカメラを設置します。

**(図16: 複数カメラの効用)**

![image3-20-16](/2023-02-23-QEUR22_VINSP19/image3-20-16.jpg)

**(図17: VRに設置された複数のカメラ[ファイブアイズ])**

![image3-20-17](/2023-02-23-QEUR22_VINSP19/image3-20-17.jpg)

【００１７】
本発明では立体を検査するためにカメラを5台設置します(図18)。ここで、中央のカメラは両目RT法における「標準ベクトル（データ）」を生成するために使われます。他のカメラは左右方向と上下方向における「計測ベクトル（データ）」を生成するために使われます。

**(図18: ファイブアイズによる両目RT法の考え方)**

![image3-20-18](/2023-02-23-QEUR22_VINSP19/image3-20-18.jpg)

【００１８】
本発明の処理フローを図19に示します。本発明では異常を検出するために、両目RTメトリックス、SOARTメトリックスという特徴量計算法を併用し、そのアウトプットを「教師あり学習（SVMなど）」に入力して予測します。

**(図19:　本発明のフロー図)**

![image3-20-19](/2023-02-23-QEUR22_VINSP19/image3-20-19.jpg)

【００１９】
両目RT法では中央カメラの平均画像を基準(図20)とし、同時に撮影した右カメラと左カメラの画像を比較(図21)して感度とSN比のメトリックスを生成します。

**（図20: 中央カメラの画像）**

![image3-20-20](/2023-02-23-QEUR22_VINSP19/image3-20-20.jpg)

**（図21: 右、左カメラの画像）**

![image3-20-21](/2023-02-23-QEUR22_VINSP19/image3-20-21.jpg)

【００２０】
RT法はタグチメソッドのパターン認識技術の一つですが、本発明ではそれをそのまま判別に使わずテクノ・メトリックスとして使用します。RT法の考え方は図22に示すように、基準ベクトルと計測ベクトルを入力し、回転成分を感度（Y1）とし、回転後の差異を距離で表現したものをSN比(Y2)を出力します。

**（図22： RT法の考え方）**

![image3-20-22](/2023-02-23-QEUR22_VINSP19/image3-20-22.jpg)

【００２１】
ただし、両目RT法のメトリックス計算では右カメラと中央カメラで計算したRTメトリックスと左カメラと中央カメラのRTメトリックスの差をとります(図23)。上-中央-下カメラにおいても同様です。

**（図23： 両目RTメトリックスの考え方 [RT差分]）**

![image3-20-23](/2023-02-23-QEUR22_VINSP19/image3-20-23.jpg)

【００２２】
両目RTメトリックスの値を変換して画像を生成した結果を図24に示します。その感度及びSN比にはそれぞれ特有の「立体感」が入っています。ここで31Xは3番PIN（中央近く）のX方向倒れをしめします。一方、61Xは6番ピン（右端近く）を示します。

**（図24： 3番PINにおけるX方向への傾きのデータ処理結果）**

![image3-20-24](/2023-02-23-QEUR22_VINSP19/image3-20-24.jpg)

**（図25： 6番PINにおけるX方向への傾きのデータ処理結果）**

![image3-20-25](/2023-02-23-QEUR22_VINSP19/image3-20-25.jpg)

【００２３】
このようにPIN場所とPINの倒れ方によってメトリックスの分布が違います。次に、Y方向に端子が倒れた画像に対して両目RTメトリックス処理を行い、その数字を画像化しました。Y方向の倒れの場合、3番PINでも6番PINでも同様に検出できます(図26、27)。

**（図26：3番PINにおけるY方向への傾きのデータ処理結果）**

![image3-20-26](/2023-02-23-QEUR22_VINSP19/image3-20-26.jpg)

**（図27：6番PINにおけるY方向への傾きのデータ処理結果）**

![image3-20-27](/2023-02-23-QEUR22_VINSP19/image3-20-27.jpg)

【００２４】
前述のフロー図で示したように、本発明ではSOARTメトリックスというRT法の一種を用いて、両目RTメトリックスを少数のメトリックスに変換します。SOART法は画像などのパターン処理に使われ、CNN(Convolution Neural Network)と同様に畳み込みをおこないます。畳み込みを行うための部品群の一例を図28と図29に示します。

**（図28 ：畳み込みRT法で使用する部品その１）**

![image3-20-28](/2023-02-23-QEUR22_VINSP19/image3-20-28.jpg)

**（図29 ：畳み込みRT法で使用する部品その２）**

![image3-20-29](/2023-02-23-QEUR22_VINSP19/image3-20-29.jpg)

【００２５】
ここで、BEND群とLINE群を畳み込んでRT法の**計測用ベクトル**を生成し、DATUM群は畳み込みによっ**て基準ベクトル**を生成して比較し、感度とSN比(距離)を計算します。このように、図30に示す12種のメトリックスが得られます。

**（図30 ：特徴量エンジニアリングその１）**

![image3-20-30](/2023-02-23-QEUR22_VINSP19/image3-20-30.jpg)

【００２６】
このように画像から抽出された特徴量がSVM（サポートベクトルマシン）などの**「教師あり学習」**に入力されます。本発明（ファイブアイズ）で、SVMに入力される特徴量を図31に示します。

**（図31 ：特徴量エンジニアリングその２）**

![image3-20-31](/2023-02-23-QEUR22_VINSP19/image3-20-31.jpg)

【００２７】
教師あり学習ロジックへの入力情報には、特徴量のほかに検査位置の情報も必要です。SOARTメトリックスの場合、**1つのブロブに対して1束の特徴量群が対応する場合に判別精度が上がります(図32)**。外観検査の対象物には多くのブロブが含まれるので、ブロブ毎に異常検出をすると効果があがります。

**(図32：ブロブの考え方)**

![image3-20-32](/2023-02-23-QEUR22_VINSP19/image3-20-32.jpg)

### 【実施例_1】
【００２８】
本事例では、コネクタの端子抜けについて端子傾き(X方向、Y方向)、及び端子後退を検出する事例を紹介します。今回使用するカメラ数は3台であり、左-中央-右のみです(図33,34)。

**(図33： 不良モード)**

![image3-20-33](/2023-02-23-QEUR22_VINSP19/image3-20-33.jpg)

**（図34 : 複数のカメラ画像）**

![image3-20-34](/2023-02-23-QEUR22_VINSP19/image3-20-34.jpg)

【００２９】
本事例ではBlenderソフトウェアで仮想空間(VR)データを取り、それを前述の特徴量を抽出しサポートベクトルマシンで予測をした結果を示します。ここで、サポートベクトルマシンはカーネルを使って予測パターンを切り替えることができ、Poly（多項式）モード(図35)で最良の結果を得ました。

**（図35 : SVMカーネルの種類）**

![image3-20-35](/2023-02-23-QEUR22_VINSP19/image3-20-35.jpg)

***（SVMプログラムと予測結果）***

```python
# SVM for SOART metrics example
import pandas as pd
import math
import numpy as np
# Prediction
from sklearn import metrics, preprocessing
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix

# ----
# データを読み込む
# ----
# train
filepath_train = 'soart_mtxout_train_fiveye.csv'
df_org_train = pd.read_csv(filepath_train)

# test
filepath_test  = 'soart_mtxout_test_fiveye.csv'
df_org_test  = pd.read_csv(filepath_test)

# ----
# 欠陥分類を数字(0,1,2)に変換する
def create_numberlist(arr_defect, list_defect, list_number):
    arr_result = []
    for i in range(len(arr_defect)):
        for j, jStr in enumerate(list_defect):
            if arr_defect[i] == jStr:
                arr_result.append(list_number[j])
    return arr_result

# ----
# Resultコラムを作成する 
# ----
# train
list_train_defect = ['OK', 'new', 'old', '31D05','31X10','61D05','61X10','31Y10','61Y10',]
list_train_number = [   0,     0,     0,      1 ,      2,      1,      2,      3,      3,]
arr_defect_train  = df_org_train.loc[:,"defect"].values
arr_result_train  = create_numberlist(arr_defect_train, list_train_defect, list_train_number)
#print(arr_result_train)

# test(jCol = 4)
list_test_defect  = ['OK', '31D05','31X10','61D05','61X10','31Y10','61Y10',]
list_test_number  = [    0,      1,      2,      1,      2,      3,      3,]
df_test_jCol4     = df_org_test[df_org_test["jCol"]==4.0]
arr_defect_jCol4  = df_test_jCol4.loc[:,"defect"].values
arr_result_jCol4  = create_numberlist(arr_defect_jCol4, list_test_defect, list_test_number)
#print(arr_result_jCol4)

# test(jCol = 7)
#arr_test_defect   = ['OK', '31D05','31X10','61D05','61X10',]
df_test_jCol7     = df_org_test[df_org_test["jCol"]==7.0]
arr_defect_jCol7  = df_test_jCol7.loc[:,"defect"].values
arr_result_jCol7  = create_numberlist(arr_defect_jCol7, list_test_defect, list_test_number)
#print(arr_result_jCol7)

# ----
# パフォーマンス計測
# drop columun
df_org_train  = df_org_train.drop(labels=['iPic', 'icount', 'iRow', 'area'], axis=1)
df_test_jCol4 = df_test_jCol4.drop(labels=['iPic', 'icount', 'iRow', 'area'], axis=1)
df_test_jCol7 = df_test_jCol7.drop(labels=['iPic', 'icount', 'iRow', 'area'], axis=1)

# X and y dataset into train and test dataset
X_train = df_org_train.loc[:, "jCol":"DS_S6"].values
y_train = arr_result_train
X_test_jCol4 = df_test_jCol4.loc[:, "jCol":"DS_S6"].values
y_test_jCol4 = arr_result_jCol4
X_test_jCol7 = df_test_jCol7.loc[:, "jCol":"DS_S6"].values
y_test_jCol7 = arr_result_jCol7

# ----
# パフォーマンス計測
def performance_model(model, X_test, y_test):
    # predict
    y_pred = model.predict(X_test)

    # accuracy
    print("accuracy:", metrics.accuracy_score(y_true=y_test, y_pred=y_pred), "\n")
    # cm
    print(metrics.confusion_matrix(y_true=y_test, y_pred=y_pred))
                
# SVM学習の実行
kernel_names= ['linear','rbf','poly','sigmoid']
for kernel_name in kernel_names:

    # linear model
    model = SVC(kernel=kernel_name)
    model.fit(X_train, y_train)

    # jCol = 4.0
    print("---- {}-{} ----\n".format("jCol=4", kernel_name))
    performance_model(model, X_test_jCol4, y_test_jCol4)

    # jCol = 7.0
    print("---- {}-{} ----\n".format("jCol=7", kernel_name))
    performance_model(model, X_test_jCol7, y_test_jCol7)

```

***(表 : Confusion Matrix)***

---- jCol=4-poly ----
accuracy: 0.9257142857142857 
[[200   0   0   0]
 [  0  50   0   0]
 [ 26   0  24   0]
 [  0   0   0  50]]
---- jCol=7-poly ----
accuracy: 0.9428571428571428 
[[200   0   0   0]
 [  8  42   0   0]
 [ 12   0  38   0]
 [  0   0   0  50]]


【００２２】
上表に示すように、異常検出のパフォーマンスは90％を超えています。今回の事例において、カメラが３つだけなので、入力した特徴量の数は6x4=24件です。もし、ファイブアイズ（カメラを5台。特徴量数6x4x2=48件）にした場合では、それを超える異常検出パフォーマンスが期待できます。


### 【産業上の利用可能性】
【００３０】
本発明はコネクタの端子検査だけでなく、立体物を対象とした外観検査に適用できます。VRを用いて非常に手軽に実験できる事例としてコネクタ検査を選んだだけです。

【００３１】
実用上の課題はコスト上のモノが主であり、5台のカメラが必要になる点です。今回は事例としてX軸とY軸の2軸をカバーする「ファイブ・アイズ（法）」を選択しました。しかし、検査する対象物によっては1軸だけの「両目法」で十分な正確度を出す可能性もあります。今回のオスコネクタの端子検査においては3台のカメラでもかなりの精度が出ると思います。

【００３２】
**VRにより**コンピューターで情報を生成して学習しすれば、学習データの準備にかかる時間は少なく、**生産現場の損失は発生しません**。


### 【補足】
【００３３】
本発明を用いた設備要素の原価とタスクタイムは大体以下のようになります。

**（原価償却費の概算）**
- カメラ5台：　10万円
- 照明： 5万円
- コンピュータ周辺(Raspberry Pi4相当の場合): 2万円
- トラバース装置：　10万円
- その他治具：　10万円
合計：　37万円
***原価償却年数：　2年（24か月）***
**1月当たりの償却費：　37万円/24月 = 1.54万円***

**（タスクタイム概算）**
***１つの予測作業について5秒程度（Julia言語でプログラムが組まれている場合）***

### ※人件費（特に学習に伴う）を含んでいません
タスクタイムを短縮するときには、コンピュータを強力なものに取り換える必要があります

【００３４】
今回の事例では、科学計算に適しているといるJulia言語といわれるハイパフォーマンス言語を使っています。テクノメトリックスを使っているのでカスタムメイドの計算が多く、計算速度の遅いPythonではタスクタイムが著しく長くなります。

**(図36 : Julia言語とPython言語の計算速度)**

![image3-20-36](/2023-02-23-QEUR22_VINSP19/image3-20-36.jpg)


## ～　まとめ　～

QEU:FOUNDER: “やっと外観検査自動機のプロジェクトが終わった・・・。いやぁ、めでたい。”

C部長: “本当の意味で（外観検査自動機プロジェクトが）終わりましたね・・・。”

QEU:FOUNDER: “**小生は本プロジェクトのネタを全開する方針です。皆さまは、これをテンプレートにして、各自が改良しながらドンドン使ってください。**そもそもムダでストレスの多い外観検査作業を人間にやらせるのは精神衛生、地球環境上よくないです。”

D先生: “・・・でも、カンパ欲しい・・・。”

QEU:FOUNDER: “小生もお腹が減った。皆さま、カンパをください・・・。”

### [＞寄付のお願い(click here)＜](https://www.paypal.com/paypalme/QEUglobal?v=1&utm_source=unp&utm_medium=email&utm_campaign=RT000481&utm_unptid=29844400-7613-11ec-ac72-3cfdfef0498d&ppid=RT000481&cnac=HK&rsta=en_GB%28en-HK%29&cust=5QPFDMW9B2T7Q&unptid=29844400-7613-11ec-ac72-3cfdfef0498d&calc=f860991d89600&unp_tpcid=ppme-social-business-profile-created&page=main%3Aemail%3ART000481&pgrp=main%3Aemail&e=cl&mchn=em&s=ci&mail=sys&appVersion=1.71.0&xt=104038)

QEU:FOUNDER ： “少しだけでよいですので・・・。たくさんでもいいけど・・・。あとで、Raspberry Piで同じプログラムを走らせてみましょう。実際に、どの程度の時間がかかるのか・・・。”


