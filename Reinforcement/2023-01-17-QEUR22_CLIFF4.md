---
title: QEUR22_CLIFF4:　Cliff_Walkingで遊ぼう（その5：JuliaRLAGENT編-テクノメトリックス）
date: 2023-01-17
tags: ["QEUシステム", "メトリックス", "Julia言語", "Cliff Walking", "Flux", "ディープラーニング", "強化学習"]
excerpt: Julia言語を使った強化学習
---

## QEUR22_CLIFF4:　Cliff_Walkingで遊ぼう（その5：JuliaRLAGENT編-テクノメトリックス）

## ～　テクノ・メトリックスの立場とは　～

QEU:FOUNDER(設定年齢65歳) ： “いよいよ、**テクノメトリックス(Techno-metrics)を適用した強化学習**の説明に入ります。テクノメトリックスは、機械学習の体系において如何に合理的な考え方であるかを・・・。”

D先生(設定年齢65歳)  ： “この番組は**「高齢者によるイノベーション」**です。”

![imageJRL1-13-1](/2023-01-17-QEUR22_CLIFF4/imageJRL1-13-1.jpg)

QEU:FOUNDER ： “数年前、**「AIは経営に不要」**とかいう趣旨のコンサルタント様の記事を見たことがあります。探したけど、記事が消えてた・・・(笑)。その中でも強化学習って、確かにわかりにくいんです。でも、強化学習は「必然」であることがわかります。まずは、「教師なし学習」の解説から・・・。”

![imageJRL1-13-2](/2023-01-17-QEUR22_CLIFF4/imageJRL1-13-2.jpg)

D先生 ： “上図によるとINPUTとOUTPUTをつなぐ「プロセス」が存在しないですね。”

QEU:FOUNDER ： “だから、教師なし学習の場合にはINPUTの情報（データ）の特徴を抽出して、それがOUTPUTと関係があるかを別途検討していきます。次に、「教師あり学習」の場合・・・。”

![imageJRL1-13-3](/2023-01-17-QEUR22_CLIFF4/imageJRL1-13-3.jpg)

QEU:FOUNDER ： “右側の矢印にあったバツ印「X」が消えました。そうすると、どうなりますか？”

D先生 ： “プロセスが生きてきます。INPUTとOUTPUTはどんな対応関係なんだろうかと・・・。このプロセスは別名として**「モデル」**ともいえます。・・・そういえば、図の下側に「？？？」がありますが、これはなんですか？”

QEU:FOUNDER ： “次は、いよいよ強化学習の出番です・・・。上向き矢印のバツ印が消えると、**INPUTとOUTPUTとの直接の対応関係**がなくなります。例えば、クルマの前に人が歩いています。この「状態(STATE)」とブレーキを踏むという「行動(ACTION)」には直接の関係がないでしょ？”

![imageJRL1-13-4](/2023-01-17-QEUR22_CLIFF4/imageJRL1-13-4.jpg)

D先生 ： “ドライバーという外部評価者が、「前に人が歩いている」という状態を「危ない」と考えているから、INPUT/OUTPUT対応がなりたちます。”

QEU:FOUNDER ： “ただし強化学習には、「外部評価者はゲームに勝ちたいと考えている」いう前提があります。クルマの運転の場合には「安全に目的地まで移動する」というルールがあるんですね。”

D先生 ： “なるほど、そうすると機械学習には3種類あるというのは合理的ですね。・・・でも、3種類だけなのかな？”

QEU:FOUNDER ： “ひょっとしたら、他にもあるかもね。ここらへんは深く考えていない・・・（笑）。でも、小生としては、派生版としてもう一種類の機械学習を提案することができます。「テクノメトリックス付き強化学習」です。”

![imageJRL1-13-5](/2023-01-17-QEUR22_CLIFF4/imageJRL1-13-5.jpg)

QEU:FOUNDER ： “上図では**「Data Ethics(データ倫理)」**と書いてはあるが、実際には**ゲームに勝つためのノウハウ類が多い**と思います。それを環境（ENV）側に埋め込み、STATEからテクノメトリックスを吐き出させる。”

D先生 ： “そのテクノメトリックスの計算手法の一つがminRT法というわけです。”

QEU:FOUNDER ： “minRT法については、以前に説明してあるので各自がそれらを参照してください。それでは、Julia言語による強化学習プログラムをドン・・・。”

```julia
# ---
# TRIAL【4】_Cliff_Walking_DQN_TechnoMetricsRT_ABS
# ---
using ReinforcementLearning
using Flux
using Flux: mse, onehotbatch
using Random
using Statistics
using Plots
using LinearAlgebra

# ---------------------------
# グローバル宣言
global num_state
global num_actions 
global in_dim 
global out_dim
global base_mx
# minRT距離の基準経路
base_mx = Float32[4.0 1.0; 3.0 1.0; 2.0 1.0; 2.0 2.0; 2.0 3.0
        ; 2.0 4.0; 2.0 5.0; 2.0 6.0; 2.0 7.0; 2.0 8.0
        ; 2.0 9.0; 2.0 10.0; 2.0 11.0; 2.0 12.0; 3.0 12.0; 4.0 12.0]

# ---------------------------
# Defining all the required parameters
global ε
global γ 
global e_decay  
global e_min   
global learning_rate
global total_episodes 

# ---------------------------
# Cliff_Walking_Environment
# ---------------------------
NX, NY = 4, 12
PStart = [4, 1]
PGoal  = [4, 12]
LRUD = [
    [0, -1],  # 1->left
    [0, 1],   # 2->right
    [-1, 0],  # 3->up
    [1, 0],   # 4->down
]

# ------
function iscliff(p)
    x, y = p[1], p[2]
    if x == 4 && y > 1 && y < NY
        return true
    else
        return false
    end
end

# take a look at the wordmap
aaaa = zeros(Float32, NX, NY)
for i in 1:NX
    for j in 1:NY
        position  = [i,j]
        aaaa[i,j] = iscliff(position)
    end
end

# ----
# HEATMAPを描く
heatmap(aaaa;yflip=true)

# ------
# 環境用のユーティリティ関数群
function isoutlayout(p)
    x, y = p[1], p[2]
    if x < 1 || x > NX || y < 1 || y > NY
        return true
    else
        return false
    end
end

function calc_onehot(p)
    onehot = zeros(Float32, NX*NY)
    x, y = p[1], p[2]
    number::Int64  = x + (y-1)*NX
    onehot[number] = 1.0
    return onehot
end

function calc_techRTABS(p)
    x, y = p[1], p[2]
    # minRTメトリックスの計算
    arr_techMX = []
    for i in 1:16
        temp_techMX = sqrt(mse(p,base_mx[i,:]))
        push!(arr_techMX, temp_techMX)
    end
    techMX = round(minimum(arr_techMX);digits=4)
    # START距離
    DStart = round(sqrt(mse(p, PStart));digits=4)
    # END距離
    DGoal  = round(sqrt(mse(p, PGoal));digits=4)
    # ベクトル化
    Vec_TM = [x, y, techMX, DStart, DGoal]
    return Vec_TM
end

function calc_number(p)
    x, y = p[1], p[2]
    number::Int64  = x + (y-1)*NX
    return number
end

function rev_onehot(p)
    arrseq = 1:NX*NY
    number = dot(arrseq, p)
    return number
end

function calc_rewards(p)
    reward::Float32 = 0.0
    if env.flg_layout == true
        reward = -1.0
    elseif env.flg_cliff == true
        reward = -50.0
    elseif p[1] == PGoal[1] && p[2] == PGoal[2]
        reward = 2000
    else
        reward = -0.2
    end
    return reward
end

function calc_isterm(p,cnt)
    if p[1] == PGoal[1] && p[2] == PGoal[2]
        return true
    elseif cnt > 50
        return true
    else
        return false
    end
end

# ------
# 環境の構築
Base.@kwdef mutable struct CliffWalkingEnv <: AbstractEnv
    position = PStart
    count::Int = 0
    flg_layout = false
    flg_cliff  = false
end

function (env::CliffWalkingEnv)(a::Int)
    env.flg_layout = false
    env.flg_cliff  = false
    temp = LRUD[a]
    x, y = env.position[1] + temp[1], env.position[2] + temp[2]
    if isoutlayout([x,y]) == true
        env.position = env.position
        env.flg_layout = true
    elseif iscliff([x,y]) == true
        env.count     = env.count + 1
        env.position  = PStart
        env.flg_cliff = true
    else
        env.position = [x, y]
    end
end

function env_reset()
    env.position = PStart
    env.count = 0
    env.flg_layout = false
    env.flg_cliff  = false
end

# ------
RLBase.state(env::CliffWalkingEnv) = calc_techRTABS(env.position)
RLBase.state_space(env::CliffWalkingEnv) = Base.OneTo(length(calc_techRTABS(env.position)))
RLBase.action_space(env::CliffWalkingEnv) = Base.OneTo(length(LRUD))
RLBase.reward(env::CliffWalkingEnv) = calc_rewards(env.position)
RLBase.is_terminated(env::CliffWalkingEnv) = calc_isterm(env.position, env.count)
RLBase.reset!(env::CliffWalkingEnv) = env_reset()
env = CliffWalkingEnv()

# ------
# ベース経路データを作る
# -------
#    [0, -1],  # 1->left
#    [0, 1],   # 2->right
#    [-1, 0],  # 3->up
#    [1, 0],   # 4->down
# -------
# 決まったアクションの場合
arr_action = [3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4]

reset!(env)
base_mx = zeros(16, 2)
base_mx[1,:] = [env.position[1], env.position[2]]
count = 2
for action in arr_action
    env(action)
    base_mx[count,:] = [env.position[1], env.position[2]]
    if is_terminated(env) == true
        break
    end
    count = count + 1
end
print(base_mx)
#size(base_mx)
```

 　 (imageJRL1-13-6)
![imageJRL1-13-1](/2023-01-17-QEUR22_CLIFF4/imageJRL1-13-1.jpg)

D先生 ： “この部分が、minRT法のベースデータですね。”

QEU:FOUNDER ： “今回は、あえてこの経路をベースとしました・・・。”

![imageJRL1-13-7](/2023-01-17-QEUR22_CLIFF4/imageJRL1-13-7.jpg)

D先生 ： “あれ？ベスト経路ではないですね。”

QEU:FOUNDER ： “本来あるべき**最適経路にはしなかった**。標準経路ということで・・・（笑）。その意味はあとで分かるから・・・。”

```julia
# ----------------
# 命令を選択する
function choose_action(state_cur, iCnt_play)

    # ----------------------------------------
    # 移動命令
    # LRUD = [
    #    [0.0, -1.0],  # 1->left
    #    [0.0, 1.0],   # 2->right
    #    [-1.0, 0.0],  # 3->up
    #    [1.0, 0.0],   # 4->down
    # ----------------
    Qvalue = -0.0001
    #println("BIGIN:choose_action")
    # ----------------
    # 最適命令の選択
    if ε < rand()
        # DQNによる選択
        acType   = "machine"
        mx_input = zeros(4,in_dim)
        for action in 1:4
            # [array]-action を生成します。
            a_onehot = onehotbatch(action, 1:4)
            temp_s_a = cat(state_cur, a_onehot , dims=1)'
            #println("temp_s_a-choose action: ", temp_s_a)
            mx_input[action,:] = temp_s_a     # 状態(S)行動(A)マトリックス
        end
        #println("----- mx_input(choose_action) -----")
        #println(mx_input)
        # ----------------------------------------
        # predict 'y'
        y_pred  = model(mx_input')
        Qvalue  = maximum(y_pred)
        a_order = argmax(y_pred)[2]
    else
        # 乱数による選択
        acType = "random"
        a_order = rand(1:4)
    end

    return a_order, acType, round(Qvalue; digits=5)
end

# -----
#    Args:
#        ε: The degree of exploration
#        γ: The discount factor
#        num_state: The number of states
#        num_actions: The number of actions
#        action_space: To call the random action
# -----
num_state     = 5
num_actions   = 4
in_dim  = num_state + num_actions
out_dim = 1

# =================================================
# Calculation class(2) : DQN_Solver:
# =================================================
# crate instance for input
global model       = Chain(Dense(in_dim, 256, relu), Dense(256, out_dim)) # Use Chain if you want to stack layers
# パラメタ
global ps_model    = params(model)
# -----
# 損失関数
global loss(x, y)  = mse(model(x'), Matrix(y'))
# -----
# オプティマイザ
global opt         = ADAM()

# ----------------
# メモリを蓄積する
function remember_memory(memory, state_cur, action, reward, state_next, done)
    push!(memory, (state_cur, action, reward, state_next, done))
    if length(memory) > MEMORY_CAPACITY
        memory = memory[2:end]
    end
    return memory
end

# ----------------
# (REPLAY EXPERIENCE)学習する
function replay_experience(iCnt_play, memory, batch_size)
    #println("BIGIN:replay_experience")
    # --------------------------------------------------
    batch_size = min(batch_size, length(memory))
    minibatch  = rand(memory, batch_size)
    X, Y   = zeros(batch_size,in_dim), zeros(batch_size)
    # --------------------------------------------------
    for ibat in 1:batch_size
        state_cur, action, reward_cur, state_next, done = minibatch[ibat]
        # CURRENT: < X >-MATRIX
        a_onehot = onehotbatch(action, 1:4)
        state_action_cur = cat(state_cur, a_onehot, dims=1)'
        #println("state_action_cur: $state_action_cur")
        X[ibat,:]  = state_action_cur
        # ----------------
        # NEXT: < Y >-ARRAY
        if done == true
            Y[ibat] = reward_cur
        else
            mx_input = zeros(4,in_dim)  # 状態(state)と行動(action)マトリックス
            for imov in 1:4
                # [array]-action を結合します。
                a_onehot = onehotbatch(imov, 1:4)
                temp_s_a = cat(state_next, a_onehot , dims=1)'
                #print("temp_s_a-choose action: ", temp_s_a)
                mx_input[imov,:] = temp_s_a      # 状態(S)行動(A)マトリックス
            end
            #println("--- mx_input(replay_experience) ---")
            #println(mx_input)
            # --------------------------------------------------
            # predict 'y'
            y_pred   = model(mx_input')
            target_f = reward_cur + γ * maximum(y_pred)
            #println("target_f: $target_f")
            Y[ibat]  = target_f
        end
    end
    # --------------------------------------------------
    # TRAINING
    # calculate loss -> gradient
    gs = gradient(() -> loss(X, Y), ps_model)
    # update weights
    Flux.Optimise.update!(opt, ps_model, gs)
    # -----
    val_loss = round(loss(X, Y); digits=5)
    #println("iPlay: $iCnt_play, loss: $val_loss")

    return val_loss

end

# =================================================
# ENVIRONMENT class
# =================================================
# リセット
function env_reset!(env)
    reset!(env)
    state_cur = state(env)
    position_cur = ( env.position[1], env.position[2] )
    #println("state_cur: $state_cur, position_cur: $position_cur")
    return state_cur, position_cur
end

# 運用中
function env_step!(env, action)
    env(action)
    state_next = state(env)
    position_next = ( env.position[1], env.position[2] )
    reward_cur = reward(env)
    done       = is_terminated(env)
    #println("state_next: $state_next, position_next: $position_next, reward: $reward_cur, done: $done")
    return state_next, position_next, reward_cur, done
end

# =================================================
# Calculation class
# =================================================
# エピソードを運用する
function get_episode(iCnt_play, memory)

    # ---------------------------
    # 機械学習用のパラメタ群
    Qvalue, reward_cur, done = -0.0001, 0, false
    maxturn, maxscore = 0, 0
    # ---------------------------
    # 記録用パラメタ類(ターンベース)
    arr_iturn  = []  # ターン・カウンタリスト
    orders_row = []  # 指示リスト(row)
    orders_col = []  # 指示リスト(col)
    arr_orders = []  # 指示リスト(命令)
    arr_acType = []  # 指示のタイプ
    arr_scores = []  # ゲームスコアリスト
    arr_dones  = []  # ゲームオーバーフラグ
    arr_predQV = []  # Q値のリスト
	arr_RTdist = []  # RTテクノメトリックスのリスト
    # ---------------------------
    # ゲームをリセットする
    state_cur, position_cur = env_reset!(env)
    # ---------------------------
    # ゲームをプレイする
    iCnt_turn = 0
    while true
        # 命令(a_order)を作成する
        a_order, acType, Qvalue = choose_action(state_cur, iCnt_play)
        # ---------------------------
        # ゲームをもう一歩進める
        state_next, position_next, reward_cur, done = env_step!(env, a_order)
		# ベクトル化：  Vec_TM = [x, y, techMX, DStart, DGoal]
		
        #if iCnt_play % 5 < 0.01 && iCnt_turn % 20 < 0.01 || done == true
        #    println("iPlay: $iCnt_play, iTurn: $iCnt_turn, position_cur: $position_cur, action: $a_order, done: $done, reward: $reward_cur")
        #end
        # ---------------------------
        # 記録用リストを追記する
        push!(arr_iturn, iCnt_turn)  # ターン・カウンタリスト
        push!(orders_row, position_cur[1])  # 指示リスト(ROW)
        push!(orders_col, position_cur[2])  # 指示リスト(COL)
        push!(arr_orders, a_order)  # 指示リスト(命令)
        push!(arr_acType, acType)  # 指示のタイプ
        push!(arr_scores, reward_cur)  # ゲームスコアリスト
        push!(arr_dones, done)  # ゲームオーバーフラグ
        push!(arr_predQV, Qvalue)  # Q値のリスト
		push!(arr_RTdist, state_cur[3])  # RTテクノメトリックスのリスト
        # ----------------
        # Experience Replay配列に保管する
        memory = remember_memory(memory, state_cur, a_order, reward_cur, state_next, done)
        # ----------------
        # 実行継続するか分岐
        if done == true
            # Game over
            break  # (game over rip)
        else
            # count of game turn
            position_cur  = position_next
            state_cur     = state_next
            iCnt_turn     = iCnt_turn + 1
        end
    end
    # ---------------------------
    # ゲーム代表値のリスト
	rep_predQV = quantile(arr_predQV, [0.25, 0.75, 1])
	rep_RTdist = sum(arr_RTdist)
    # ---------------------------
    # プレイデータの引継ぎ
    maxturn, maxscore = iCnt_turn, sum(arr_scores)

    return maxturn, round(maxscore; digits=2), memory, rep_predQV, rep_RTdist

end

# =================================================
# MAIN ROUTINE
# =================================================
# Defining all the required parameters
ε     = 0.80
γ     = 0.95
e_decay       = 0.9995
e_min         = 0.01
learning_rate  = 0.005
total_episodes = 5000

# ---------------------------
# ハイパーパラメタ
memory         = []
BATCH_SIZE     = 128  # サンプルサイズ
MEMORY_CAPACITY = 128 * 8  # メモリ容量

# ---------------------------
# 記録用パラメタ類(プレイベース)
arr_iplay     = []  # count game play    プレイ番号
arr_maxturn   = []  # turn game play    ターン数
arr_maxscore  = []  # rl_score game play    報酬の総和
arr_loss      = []  # DQN-Experience Replay学習
arr_εs        = []  # ε-Greedy
# ---------------------------
# Q値の分析用(プレイベース)
arr_maxQV     = []  # QV値の最大値
arr_q25QV     = []  # QV値の4分の1値
arr_q75QV     = []  # QV値の4分の3値
arr_sumRT	  = []  # minRT距離の合計
# ---------------------------
# エピソードを実行する
for iCnt_play in 1:total_episodes   # total_episodes
    # ゲームする
    maxturn, maxscore, memory, rep_predQV, rep_RTdist = get_episode(iCnt_play, memory)
    # ----------
    # DQN-Experience Replay学習
    val_loss = replay_experience(iCnt_play, memory, BATCH_SIZE)
    # 結果の出力
    if iCnt_play % 20 < 0.01
        println("iCnt_play: $iCnt_play, maxturn:$maxturn, maxscore:$maxscore, ε:$ε, loss:$val_loss")
    end
    # ----------
    # 記録用パラメタ類(プレイベース)の追加
    push!(arr_iplay, iCnt_play)  # count game play    プレイ番号
    push!(arr_maxturn, maxturn)  # max_turn   ゲームのターン数
    push!(arr_maxscore, maxscore)  # rl_score game play    最終プレイスコア
    push!(arr_loss, val_loss)    # DQN-Experience Replay学習
    push!(arr_εs, ε)  # イプシロン
    # ----------
    # ゲーム代表値の保管
    push!(arr_maxQV, rep_predQV[3])  # QV値の最大値
    push!(arr_q75QV, rep_predQV[2])  # QV値の4分の3値
    push!(arr_q25QV, rep_predQV[1])  # QV値の4分の1値
	push!(arr_sumRT, rep_RTdist)  # minRT距離の合計
    # ----------
    # Change EPSILON
    if ε > e_min
        ε *= e_decay
        ε = round(ε; digits=5)
    end
end

```

![imageJRL1-13-8](/2023-01-17-QEUR22_CLIFF4/imageJRL1-13-8.jpg)

```julia
# ----------
# Create Graph
#using Plots

# A-GROUP
pltA1 = plot(arr_εs, label="EPSILON")
pltA2 = plot(arr_loss, label="loss")
pltA3 = plot(arr_maxturn, label="maxturn")
pltA4 = plot(arr_maxscore, label="maxscore")
plot(pltA1, pltA2, pltA3, pltA4)

```

![imageJRL1-13-9](/2023-01-17-QEUR22_CLIFF4/imageJRL1-13-9.jpg)

D先生 ： “前回のDQN48のパフォーマンスとはずいぶん違いますね。”

QEU:FOUNDER ： “今回と前回のパフォーマンスを比較すると、今回の手法には**いい面もあるし、悪い面もあります**。ただし、忘れてはいけないのは、前回のSTATEは48次元であり、今回は5次元です。”


**(前回:DQN48の場合)**

![imageJRL1-13-10](/2023-01-17-QEUR22_CLIFF4/imageJRL1-13-10.jpg)

D先生 ： “今回は、前回よりも学習収束が圧倒的に速いのですが、いまいち**LOSS（学習損失）の落ち着きが悪い**ので少し不安がありますね。”

```julia
# B-GROUP
pltB1 = plot(arr_maxQV, label="max")
		plot!(arr_q75QV, label="q75")
		plot!(arr_q25QV, label="q25")
pltB2 = plot(arr_sumRT, label="sumRT")
plot(pltB1, pltB2)
```

![imageJRL1-13-11](/2023-01-17-QEUR22_CLIFF4/imageJRL1-13-11.jpg)

D先生 ： “やった！とうとう**Q値グラフ**が出てきた！！いつ見ても、キレイだし、わかりやすいなぁ・・・。テクノメトリックスはminRT距離の合計値（sumRT）を取ったんですね。”

QEU:FOUNDER ： “今回のsumRT値の変動はmaxturn値の値に近くなります。・・・まあ、次回は変わりますけどね・・・。”

D先生 ： “次回のトライアルでは、minRT値を変更するんですか？”

QEU:FOUNDER ： “**現在の値は絶対値（必ず0またはプラス値）ですが、次回はブラスマイナス符号付きにします**よ。”

D先生 ： “たかが符号、されど符号・・・。符号がパフォーマンスに与える影響がわかります。”

QEU:FOUNDER ： “わかるといいなぁ・・・。”



## ～　まとめ　～

C部長 : “Ｙ大先生が面白いことをいっていました。**「Ｊ国の金利が高くなると企業の新陳代謝が進むと」**・・・。これは画期的な見方ですね。”

[![MOVIE1](http://img.youtube.com/vi/uueIRF1XeT0/0.jpg)](http://www.youtube.com/watch?v=uueIRF1XeT0 "日本銀行が敗北する日･･･その時何が起きる？中央銀行が国債2日で約10兆円購入する異常事態。海外ヘッジファンド達の空売りの先に何が？安冨歩東大教授。一月万冊")

QEU:FOUNDER ： “陳陳代謝になるかも・・・（笑）。でも、**経営者が価値観の異なる外国人になれ**ば、情勢が変わるでしょうね。”

![imageJRL1-13-12](/2023-01-17-QEUR22_CLIFF4/imageJRL1-13-12.jpg)

QEU:FOUNDER ： “その意味で、**創造性にあふれるC国の経営者と、(モラル+パワー)ハラスメントにあふれる（ファインチューニングに優れた）Ｊ国の労働者は絶妙なバランス**ですね。”

C部長 : “まだ、「Made In JXPXN」という言葉もまだのれん代があるし、J国の会社を買って、設計はC国、最終アセンブリだけをＪ国でやりたいと思うＣ国経営者も増えるかもしれないです。”

QEU:FOUNDER ： “小生は、いままで間違っていたと思っていて・・・。この大先生の「インバウンド」というのは、実は**投資インバウンド**のことなんだろうね。”

[![MOVIE1](http://img.youtube.com/vi/5fhNArCQAUA/0.jpg)](http://www.youtube.com/watch?v=5fhNArCQAUA "【経済大展望2023】今年は賃金上昇の転換点／GAFAのトップ人材が日本で働く／日本はG7で最も実質成長率が高い／低賃金が技術革新を遅らせる／英語に頼らないインバウンド対応")

C部長 : “そんなモン、あたりまえでしょ？大先生が間違うわけがない！**これからは投資インバウンド**ですよ！そもそも**Ｊ国がＣ国を敵視した状態で、Ｃ国の観光客を狙ったインバウンドが成立するわけがない**です。”

QEU:FOUNDER ： “小生も勉強不足、まだまだですね・・・。”

