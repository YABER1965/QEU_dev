---
title: QEUR22_JRLINT1:　A国とC国のこととか、イントロダクションを・・・
date: 2022-12-30
tags: ["QEUシステム", "メトリックス", "Julia言語", "機械学習", "Flux", "ディープラーニング", "強化学習"]
excerpt: Julia言語を使った強化学習
---

## QEUR22_JRLINT1:　A国とC国のこととか、イントロダクションを・・・

## ～　最先端技術の戦いの行く末は？　～

QEU:FOUNDER （設定65歳） ： “「高齢者によるイノベーション」は、さらに新たな段階へ・・・。強化学習がテーマになりましたが、まずはこの動画から・・・。”

[![MOVIE1](http://img.youtube.com/vi/ckIIxKM14Ow/0.jpg)](http://www.youtube.com/watch?v=ckIIxKM14Ow "ReinforcementLearning jl | Jun Tian | Julia User Group Munich - Share&Code")

D先生 （設定65歳）： “このスピーチをしている人（↑）はダレ？”

![imageJRL1-2-1](/2022-12-30-QEUR22_JRLINT1/imageJRL1-2-1.jpg)

QEU:FOUNDER ： “この**紋所**が名に入らぬかぁ～！！控エおろう！！！”

![imageJRL1-2-2](/2022-12-30-QEUR22_JRLINT1/imageJRL1-2-2.jpg)

D先生 ： “ハハァ～！**天下のM社の公認「大ハッカー」様**でいらっしゃいましたカー・・・。”

QEU:FOUNDER ： “すごいだろう！？でも、動画を見るとさらに面白いです。質問者から、**「Asian RL package」**なんて呼ばれています**(注意：動画を見直していたら、これは自動翻訳のミスです。でも面白いので、この話は残します)**。・・・というのは、Julia 言語の強化学習パッケージには**東西横綱がある**からね。”

**(東の横綱)**

![imageJRL1-2-3](/2022-12-30-QEUR22_JRLINT1/imageJRL1-2-3.jpg)

**(西の横綱)**

![imageJRL1-2-4](/2022-12-30-QEUR22_JRLINT1/imageJRL1-2-4.jpg)

D先生 ： “バージョンや人気度を見ると、両者は「いい勝負」をしていますね。FOUNDERは、どちらに「肩入れ」しているんですか？”

QEU:FOUNDER ： “単に気持ちだけだが、**Asia**かな・・・？でも、結局、我々にとってソレが使いやすいかどうかが決めてです。Pythonのディープラーニングのソルーションが複数種類あるように、長期的には両者は共存していくんじゃないかと思います。”

![imageJRL1-2-5](/2022-12-30-QEUR22_JRLINT1/imageJRL1-2-5.jpg)

D先生 ： “えっ？KerasはASEANで大人気とか・・・。”

QEU:FOUNDER ： “ちょっと驚き・・・。さてと、まずは、この動画（↓）を見ましょうか。”

[![MOVIE1](http://img.youtube.com/vi/GXnCm_6uyeM/0.jpg)](http://www.youtube.com/watch?v=GXnCm_6uyeM "[05x12] Markov Decision Process (MDP) with POMDPs.jl | Julia Reinforcement Machine Learning")

D先生 ： “これは**「西側」**の事例紹介ですよね・・・。”

QEU:FOUNDER ： “このJulia事例をやってみましょう。ソースは注記にあります。今回のシリーズは特別に、ソレを「そのままコピペして実行」しましょう。小生は、そもそもCopyright上、コピペそのままの実行はやらないですが、今回は特別です。ドン！！”

```julia
################################################################################
# ML > Reinforcement Learning > Markov Decision Process (MDP)
################################################################################

# load packages
using POMDPs, POMDPModelTools, QuickPOMDPs

# load solver
using DiscreteValueIteration

# define State data type
struct State
    x::Int
    y::Int
end

# define Action data type (@enum from Julia Base.Enums)
@enum Action UP DOWN LEFT RIGHT
Action
```

![imageJRL1-2-6](/2022-12-30-QEUR22_JRLINT1/imageJRL1-2-6.jpg)

```julia
# define state space
null = State(-1, -1)
S = [
    [State(x, y) for x = 1:4, y = 1:3]..., null
]
```

![imageJRL1-2-7](/2022-12-30-QEUR22_JRLINT1/imageJRL1-2-7.jpg)

```julia
# define action space
A = [UP, DOWN, LEFT, RIGHT]

# define transition function
const MOVEMENTS = Dict(
    UP => State(0, 1),
    DOWN => State(0, -1),
    LEFT => State(-1, 0),
    RIGHT => State(1, 0)
)

Base.:+(s1::State, s2::State) = State(
    s1.x + s2.x, s1.y + s2.y
)

function T(s::State, a::Action)
    # Deterministic() from POMDPModelTools.jl
    if R(s) != 0
        return Deterministic(null)
    end

    # initialize variables (index 1 is current state)
    len_a = length(A)
    next_states = Vector{State}(undef, len_a + 1)
    probabilities = zeros(len_a + 1)

    # enumerate() from Julia Base.Iterators
    for (index, a_prime) in enumerate(A)
        prob = (a_prime == a) ? 0.7 : 0.1
        dest = s + MOVEMENTS[a_prime]
        next_states[index + 1] = dest

        if dest.x == 2 && dest.y == 2
            probabilities[index + 1] = 0
        elseif 1 <= dest.x <= 4 && 1 <= dest.y <= 3
            probabilities[index + 1] += prob
        end
    end

    # handle out-of-bounds transitions
    next_states[1] = s
    probabilities[1] = 1 - sum(probabilities)

    # SparseCat from POMDPModelTools.jl
    return SparseCat(next_states, probabilities)
end

# define reward function
function R(s, a = missing)
    if s == State(4, 2)
        return -100
    elseif s == State(4, 3)
        return 10
    end
    return 0
end

# set discount factor
gamma = 0.95

# define mdp using QuickPOMDPs.jl
termination(s::State) = s == null
abstract type GridWorld <: MDP{State, Action} end
mdp = QuickMDP(GridWorld,
    states = S,
    actions = A,
    transition = T,
    reward = R,
    discount = gamma,
    isterminal = termination
)

# select solver from DiscreteValueIteration.jl
solver = ValueIterationSolver(max_iterations = 30)

# solve mdp
policy = solve(solver, mdp)

```

![imageJRL1-2-8](/2022-12-30-QEUR22_JRLINT1/imageJRL1-2-8.jpg)

```julia
# view values (utility)
value_view = [S policy.util]
```

![imageJRL1-2-9](/2022-12-30-QEUR22_JRLINT1/imageJRL1-2-9.jpg)

D先生 ： “これは、Jupyterで実行した結果ですね。動画ではVScodeで操作しています。”

QEU:FOUNDER ： “Grid Worldゲームの環境定義は以下の通りです。”

![imageJRL1-2-10](/2022-12-30-QEUR22_JRLINT1/imageJRL1-2-10.jpg)

QEU:FOUNDER  ： “計算後にValue(価値)の計算ができました。結構、その結果に納得がいくでしょう？”

![imageJRL1-2-11](/2022-12-30-QEUR22_JRLINT1/imageJRL1-2-11.jpg)

D先生 ： “最適経路は壁を周り込むのですね？歩く距離によって損しないのであればしょうがないですが・・・。”

QEU:FOUNDER ： “今回のJulia言語による強化学習の前半は、この東西パッケージを少しづつやってみて比較してみようと思います。そして、我々の目的にどちらが適しているのかを考えましょう。”

D先生 ： “「我々の目的」とは・・・？”

QEU:FOUNDER ： “大したもんじゃない。**ディープラーニングとテクノメトリックスを使うことです。・・・あとは、報酬(Reward)にフィードバックを使うこと**ぐらいかな？”

D先生 ： “今回もJulia言語のイントロダクションが長くなりそうですね。”

QEU:FOUNDER ： “前回のイントロよりもはるかに大変・・・（泣）。ぼちぼち行きましょう・・・。あっ、そうだ。重要なこと・・・。是非、カンパをください・・・。”

[＞寄付のお願い(click here)＜](https://www.paypal.com/paypalme/QEUglobal?v=1&utm_source=unp&utm_medium=email&utm_campaign=RT000481&utm_unptid=29844400-7613-11ec-ac72-3cfdfef0498d&ppid=RT000481&cnac=HK&rsta=en_GB%28en-HK%29&cust=5QPFDMW9B2T7Q&unptid=29844400-7613-11ec-ac72-3cfdfef0498d&calc=f860991d89600&unp_tpcid=ppme-social-business-profile-creat-ed&page=main%3Aemail%3ART000481&pgrp=main%3Aemail&e=cl&mchn=em&s=ci&mail=sys&appVersion=1.71.0&xt=104038)

D先生 ： “これも時代の流れです。”


## ～　まとめ　～

QEU:FOUNDER ： “強化学習って、さすがに経営者も敬遠するだろうと思って調査したら、ホント驚いた・・・。”

C部長 : “なにが？”

QEU:FOUNDER ： “**「強化学習は現場では使えない」って、Web検索してみたの・・・。そうしたら、逆の結果が出てきた。**”

![imageJRL1-2-12](/2022-12-30-QEUR22_JRLINT1/imageJRL1-2-12.jpg)

C部長 : “需要側でなく、供給側の**「色気」**でしょ？”

QEU:FOUNDER ： “それでも、**「こんなタイトルの本が出てくる」のか**と、しみじみ思いましたね。”

C部長 : “本当に使えるんですかね？”

QEU:FOUNDER ： “知らん・・・(笑)。ただ言えることは**、C国とA国はすでにその段階に行っている**んだよね。”


