---
title: QEUR22_2048S4:　ゲーム2048で遊ぼう（その5：Julia編-MAXPOOL vs SOART）
date: 2023-01-30
tags: ["QEUシステム", "メトリックス", "Julia言語", "2048", "SOART", "ディープラーニング", "強化学習"]
excerpt: julia言語とテクノメトリックスを使った強化学習
---

## QEUR22_2048S4:　ゲーム2048で遊ぼう（その5：Julia編-MAXPOOL vs SOART）

## ～　ロバスト性とは　～

QEU:FOUNDER(設定年齢65歳) ： “この番組、**「高齢者によるイノベーション」**は、ここでちょっと立ち止まる。ここで閑話休題です。ちょっと「単純な畳み込みによる機械学習」をやってみないか？”

D先生(設定年齢65歳)  ： “それは、PythonでやってみたCNNのことですか？”

![imageJRL1-24-1](/2023-01-30-QEUR22_2048S4/imageJRL1-24-1.jpg)

QEU:FOUNDER： “違います。SOART法で使用した畳み込み部品を使い、RT法で処理せずにそのままディープラーニングに入力します。”

D先生  ： “あ～あ・・・、そういうことね・・・。ひょっとしたら今までよりも学習が良くなる可能性がありますよね。そうするとSOART法って何なのかという話が・・・(笑)。”

![imageJRL1-24-2](/2023-01-30-QEUR22_2048S4/imageJRL1-24-2.jpg)

QEU:FOUNDER ： “これは**「ロバストネス」を追求する**ためです・・・。ここで、ロバストネスとは**当てはめ関数を簡単にする**ということです。当てはめ関数が簡単になると、マシンスペックも軽く、学習量も少なく、さらに動作も安定して「いいことづくめ」になります。それをテクノメトリックスをうまく使って行いたい。”

QEU:FOUNDER ： “テクノメトリックス(techno=metrics)と大げさにいっていますが、いわゆる**特徴量エンジニアリング(feature-engineering)**のことなんですがね。カスタムメイドの・・・。”

D先生  ： “その理屈はわかるんですが、そもそも**「測る」**ことができるんですかね？そのロバストネスを・・・。”

QEU:FOUNDER ： “その実験の準備を、今、やっているわけです。それでは、単純な畳み込み実験をやってみましょう。プログラムをドン！！前回と同じ実験になるので重複部分を省略しています。いっとくけど、またもや結果は「大失敗」だよ！！”

```julia
# ------
# テクノ・メトリックスの可能性を試すシリーズ
# Julia - SOARTメトリックス@Game2048の環境設定
# ------
using DataFrames, CSV
using LinearAlgebra
using Distances
using Plots
using Statistics
# ----
# Read the file using CSV.File and convert it to DataFrame
# ベンド系部品

# 省略

# =================================================
# TRIAL【1】_Game2048_DQN_TechnoMetricsRT_with_FEEDBACK
# =================================================
#using ReinforcementLearning
using Flux
using Flux: mse, huber_loss, onehotbatch
using Random
#using Statistics
#using Plots
#using LinearAlgebra

# ---------------------------
# グローバル宣言
global num_state
global num_actions 
global in_dim 
global out_dim

# ---------------------------
# Defining all the required parameters
global memory
global ε
global γ 
global e_decay  
global e_min   
global learning_rate
global total_episodes 

# =================================================
#    Args:
#        ε: The degree of exploration
#        γ: The discount factor
#        num_state: The number of states
#        num_actions: The number of actions
#        action_space: To call the random action
# =================================================
num_state     = 7
num_actions   = 4
in_dim  = num_state + num_actions
out_dim = 1

# Defining all the required parameters
ε     = 0.95
γ     = 0.99
e_decay    = 0.9999
e_min      = 0.01
total_episodes = 20000

# ---------------------------
# ハイパーパラメタ
const BATCH_SIZE    = 128  # サンプルサイズ
const MEMORY_CAPACITY = 128 * 32  # メモリ容量

# =================================================
# AGENT functions
# =================================================
# リセット：基本変数を初期化する
function reset!()
    # ---------------------------
    # ゲームの初期化(2タイルが２か所)
    grid = init_game()
    # ---------------------------
    arr_acType    = []
    arr_action    = []
    arr_Qvalue    = []
    arr_reward    = []
    mx_states     = []
    return grid, arr_acType, arr_action, arr_Qvalue, arr_reward, mx_states
end

# 省略

# =================================================
# EPISODE CONTROL functions
# =================================================
# エピソードを運用する
function get_episode!(iCnt_play, memory, rep_bonuscore)

    # リセット：基本変数を初期化する
    (grid, arr_acType, arr_action, arr_Qvalue, arr_reward, mx_states) = reset!()
    # -----
    # SOARTテクノメトリックスを計算する[RESET-CURRENT]
    # -----
    board = convert(Matrix{Float64}, grid)
    state_cur = calc_techmetrics(conv_cross, mx_cvparts, board)
    # -----
    flg_moved, flg_gameover, iTurn, total_score = true, false, 1, 0
    non_valid_count, valid_count = 0, 0
    while flg_gameover == false        # ゲームオーバーでループから外れる
        # 初期化
        Qvalue, acType  = -0.0001, "random"        
        # ----------------
        # 命令を選択し、ゲームを動かす
        if ε < rand() && flg_moved == true
            # DQNでコマを動かす
            (action, acType, Qvalue, grid, reward, flg_moved) = step_machine!(grid, state_cur, iCnt_play, Qvalue)
            # ゲーム盤が動かなければ報酬はマイナスになる
            #println("flg_moved: ", flg_moved)
        else
            # ランダムでコマを動かす
            (action, acType, Qvalue, grid, reward, flg_moved) = step_random!(grid)
        end
        #if iTurn%10 == 0
        #    println("Play: $iCnt_play, Turn: $iTurn, grid: $grid, acType: $acType, action: $action, reward: $reward, flg_moved: $flg_moved")
        #end
        # ----------------
        # SOARTテクノメトリックスを計算する[RUN-NEXT]
        # ----------------
        board_next = convert(Matrix{Float64}, grid)
        state_next = calc_techmetrics(conv_cross, mx_cvparts, board_next)
        #println("UPDATED - state_next: $state_next")
        # ----------------
         # 記録用リストを追記する
        push!(arr_acType, acType)
        push!(arr_action, action)
        push!(arr_Qvalue, Qvalue)
        push!(arr_reward, reward)
        if iTurn == 1
            mx_states = state_cur
        else
            mx_states = cat(mx_states, state_cur, dims=2)
        end
        # ----------------
        state_cur   = state_next
        total_score = total_score + reward
        # ----------------
        # 報酬修正(1) : FEED_BACK
        reward = round(reward + 5*(maximum(state_cur[1:4]) - minimum(state_cur[1:4])); digits=3)
        #println("報酬量修正後 : ", reward)
        # ----------------        
# カウント && ゲームオーバーしているか？ 
        if flg_moved == false
            non_valid_count = non_valid_count + 1
            flg_gameover = is_gameover!(grid)
        else
            valid_count = valid_count + 1
        end 
        # ----------------
        # ゲームオーバーした場合、ブレイクする
        if flg_gameover == true
            #println("BREAK!! - Play: $iCnt_play, Turn: $iTurn, flg_gameover: $flg_gameover")
            # 報酬修正(2)  ボーナス
            if total_score > rep_bonuscore*1.5 && iCnt_play > 20
                reward = 0.5*valid_count
            elseif total_score > rep_bonuscore && iCnt_play > 20
                reward = 0.3*valid_count
            else
                reward = -1.0*non_valid_count
            end
        else
            iTurn = iTurn + 1
        end
        # ----------------
        # Experience Replay配列に保管する
        memory = remember_memory(iCnt_play, iTurn, memory, state_cur, action, reward, state_next, flg_gameover)
    end
    # ---------------------------
    # ゲーム代表値のリスト
    rep_predQV = quantile(arr_Qvalue, [0.25, 0.75, 1])
    
    return (iTurn, round(total_score; digits=2), memory, rep_predQV, valid_count, non_valid_count, mx_states')
end


# =================================================
# MAIN ROUTINE
# =================================================
# 記録用パラメタ類
memory        = []  # メモリのリスト
# ---------------------------
# 記録用パラメタ類
arr_iplay     = []  # count game play    プレイ番号
arr_maxturn   = []  # turn game play    ターン数
arr_maxscore  = []  # rl_score game play    報酬の総和
arr_loss      = []  # DQN-Experience Replay学習
arr_εs       = []  # ε-Greedy
# ---------------------------
# Q値の分析用
arr_maxQV     = []  # QV値の最大値
arr_q25QV     = []  # QV値の4分の1値
arr_q75QV     = []  # QV値の4分の3値
arr_nvalid    = []  # 無効命令率
# ---------------------------
# 初期化
val_loss      = 0.0
iCnt_play     = 1
rep_bonuscore = 1000
# ---------------------------
# エピソードを実行する
while true   
    # ゲームを動かす
    (maxturn, maxscore, memory, rep_predQV, valid_count, non_valid_count, mx_states) = get_episode!(iCnt_play, memory, rep_bonuscore)
    # 無効命令率の計算
    ratio_nvalid = round(non_valid_count/maxturn; digits=3)
    # 簡単な出力
    #println("maxturn: $maxturn, maxscore: $maxscore, ratio_nvalid: $ratio_nvalid")

    # ----------
    # DQN-Experience Replay学習(重み付き)
    if valid_count >= 150
        for iLearn in 1:50
            val_loss = replay_experience(iCnt_play, memory, BATCH_SIZE)
        end
    elseif valid_count > 100 && valid_count < 150
        for iLearn in 1:20
            val_loss = replay_experience(iCnt_play, memory, BATCH_SIZE)
        #    println("Learn_turn:$iLearn, loss:$val_loss")
        end
    else
        val_loss = replay_experience(iCnt_play, memory, BATCH_SIZE)
    end

    # ----------
    # 学習結果の出力
    if iCnt_play%40 == 0
        println("iPlay: $iCnt_play, maxturn: $maxturn, maxscore: $maxscore, ε:$ε, loss:$val_loss, bonuscore:$rep_bonuscore, ratio_nvalid:$ratio_nvalid")
    end
    #println(mx_states)
    #println(size(mx_states))
    # ----------
    # 記録用パラメタ類(プレイベース)の追加
    push!(arr_iplay, iCnt_play)  # count game play    プレイ番号
    push!(arr_maxturn, maxturn)  # max_turn   ゲームのターン数
    push!(arr_maxscore, maxscore)  # rl_score game play    最終プレイスコア
    push!(arr_loss, val_loss)    # DQN-Experience Replay学習
    push!(arr_εs, ε)  # イプシロン
    # ----------
    # ゲーム代表値の保管
    push!(arr_maxQV, rep_predQV[3])  # QV値の最大値
    push!(arr_q75QV, rep_predQV[2])  # QV値の4分の3値
    push!(arr_q25QV, rep_predQV[1])  # QV値の4分の1値
    push!(arr_nvalid, ratio_nvalid)  # 無効命令率
    # ---------------------------
    # ボーナス支給のしきい値を設定する
    #rep_bonuscore = quantile(arr_maxscore, [0.75])[1]
    # ----------
    # Change EPSILON
    if ε > e_min && iCnt_play < total_episodes        # total_episodes
        ε *= e_decay
        ε = round(ε; digits=5)
        iCnt_play = iCnt_play + 1
    else
        break
    end
end

```

**(SOART)**

[A]

![imageJRL1-24-3](/2023-01-30-QEUR22_2048S4/imageJRL1-24-3.jpg)

[B]

![imageJRL1-24-4](/2023-01-30-QEUR22_2048S4/imageJRL1-24-4.jpg)

**(MAXPOOL)**

[A]

![imageJRL1-24-5](/2023-01-30-QEUR22_2048S4/imageJRL1-24-5.jpg)

[B]

![imageJRL1-24-6](/2023-01-30-QEUR22_2048S4/imageJRL1-24-6.jpg)

D先生  ： “この結果では、SOART法の場合と今回の結果はあまり変わらないですね。”

QEU:FOUNDER ： “判断はまだ早い。次のパフォーマンス指標を見てみましょう・・・。”

**(SOART)**

![imageJRL1-24-7](/2023-01-30-QEUR22_2048S4/imageJRL1-24-7.jpg)


**(MAXPOOL)**

![imageJRL1-24-8](/2023-01-30-QEUR22_2048S4/imageJRL1-24-8.jpg)

D先生  ： “いわぁ・・・。今回の単純畳み込み（MAXPOOL）のQ値は、すごく大きいですね。こっちの方が性能が高いという意味ですか？”

```julia

        # ----------------
        # 報酬修正(1) : FEED_BACK
        reward = round(reward + 5*(maximum(state_cur[1:4]) - minimum(state_cur[1:4])); digits=3)
        #println("報酬量修正後 : ", reward)
        # ----------------        
```

QEU:FOUNDER ： “今回も前回と同様に**報酬(reward)に「FEEDBACK」を使っています**。テクノメトリックスの大きなメリットって、（うまく行けば）Feedbackをつかえることですよね。今回のFeedback量は大きいんですよ。その影響がでたんじゃないかなあ・・・。一方、前回のnewRTとSOARTの比較ではFeedback量を同じにしているので比較ができるわけ・・・。”

![imageJRL1-24-9](/2023-01-30-QEUR22_2048S4/imageJRL1-24-9.jpg)

QEU:FOUNDER ： “ちなみに、今回実験（MAXPOOL）のFeedbackの有無を比較して見ました。Q値の成長速度が全然ちがいますね。”

D先生  ： “すいません。もう一回だけ、計算してください。「延長戦」という意味ですが・・・。”

QEU:FOUNDER ： “つまり、今回の学習結果をセーブし、もう一回実験してくださいということですね。前回の繰り返しが20000回なので、今回の追加実験も20000回にします。ただし、εは少し小さくさせて、0.3から始めることにします。”


**(2回目パフォ―マンス：その１)**

[A]

![imageJRL1-24-10](/2023-01-30-QEUR22_2048S4/imageJRL1-24-10.jpg)

[B]

![imageJRL1-24-11](/2023-01-30-QEUR22_2048S4/imageJRL1-24-11.jpg)


**(2回目パフォ―マンス：その２)**

![imageJRL1-24-12](/2023-01-30-QEUR22_2048S4/imageJRL1-24-12.jpg)

D先生  ： “あんまり（結果が）変わらないな・・・。いや、ほんの少しだけ変わったか・・・。”

QEU:FOUNDER ： “後半において、高いポイントが出るようになってきました。つまり、いままで学習した経験と同じ場面がゲーム上に現れたときに、うまくゲームを運用しているということなんでしょうね。”

D先生  ： “Julia言語で素早く計算しているとしても、さすがにさらなる延長戦はつらいですよね。”

QEU:FOUNDER ： “すでに兆候は見えたし、もうやめましょうよ・・・。”

D先生  ： “でも、趣味で一つやらせてください。**「スライドRT」法による強化学習**をやってもらえませんか？”

QEU:FOUNDER ： “えっ！？あれ？やるの・・・！？”


## ～　まとめ　～


QEU:FOUNDER ： “もう、**「やっと経済がわかった」**という感じ、今回は**決定版**ですね。”

[![MOVIE1](http://img.youtube.com/vi/FCc8cojW3ZU/0.jpg)](http://www.youtube.com/watch?v=FCc8cojW3ZU "田内学×宮台真司：人を幸せにする経済とは")


C部長 : “MMTにしろ、重税国家にしろ。結局は、**「われわれ」と「外部世界」の定義次第**という意味ですね。”

![imageJRL1-24-13](/2023-01-30-QEUR22_2048S4/imageJRL1-24-13.jpg)

QEU:FOUNDER ： “その意味で、その定義を最大限に広げた場合には国家内には貨幣は不要なんです。でも、それで**も国家間は貸し借りの記録としての貨幣は必要**になります。そして、もし「稼ぐ力」がないと、その国家は「持続可能性がない」わけです。”


C部長 : “それが現在の本質的な問題！！もっと勉強しなきゃね。それは、**「我々」のこと**ですが・・・（笑）。”

![imageJRL1-24-14](/2023-01-30-QEUR22_2048S4/imageJRL1-24-14.jpg)

QEU:FOUNDER ： “だから、我々も生産力を高めなければならない。chatGPTをつくってくれないかな・・・。”


C部長 : “最近、ちょっとchatGPTを使ったテストをしてみました。どうですか？ロボットの、**「この回答（↓）」**？”

![imageJRL1-24-15](/2023-01-30-QEUR22_2048S4/imageJRL1-23-15.jpg)

QEU:FOUNDER ： “小生が先生なら、この答えに120点をあげますね(笑)。”


