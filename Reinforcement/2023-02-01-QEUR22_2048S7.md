---
title: QEUR22_2048S7:　ゲーム2048でロバストネスを測る（その2：Julia編-SOART-1）
date: 2023-02-01
tags: ["QEUシステム", "メトリックス", "Julia言語", "2048", "SOART", "ディープラーニング", "強化学習", "ロバストネス"]
excerpt: julia言語とテクノメトリックスを使った強化学習とロバストネス評価
---

## QEUR22_2048S7:　ゲーム2048でロバストネスを測る（その2：Julia編-SOART-1）

## ～　RTメトリックスのメリットは　～

D先生(設定年齢65歳)   ： “つぎはSOART法による強化学習のリベンジ（iTurnの追加）です。総合SN比による評価の準備をしています。”

![imageJRL1-27-1](/2023-02-01-QEUR22_2048S7/imageJRL1-27-1.jpg)

QEU:FOUNDER(設定年齢65歳) ： “補足です。RT法は2つのベクトル（画像）の比較を行う方法で2つのメトリックス（感度、SN比）が出力されます。SOART法は、我々、QEUシステムが考え出した手法で、感度はRT法のようにベクトルの回転（第一主成分）ではなく、明るさ（値の合計）の比較を使用しています。そして、SN比は感度で補正したベクトルと計測対象の距離です。”

![imageJRL1-27-2](/2023-02-01-QEUR22_2048S7/imageJRL1-27-2.jpg)

D先生： “ついでにいうと、距離はユーグリット距離を使うのが伝統的手法であり、マンハッタン距離を使うのがQEU流ですね。それでは、（強化学習を）いきますか？”

```julia
# =================================================
# DQN_Solver functions
# =================================================
# crate instance for input
global model       = Chain(Dense(in_dim, 256, relu), Dense(256, 256, relu), Dense(256, out_dim)) # Use Chain if you want to stack layers
global model       = Chain(Dense(in_dim, 512, relu), Dense(512, out_dim)) # Use Chain if you want to stack layers

```

QEU:FOUNDER ： “いきましょう！SOART法のプログラムをドン！！誤差因子として、2種類の関数を考えています。今回、計算するのは構造がより複雑な方の256x256の多層関数にしました。”

```julia
# ------
# テクノ・メトリックスの可能性を試すシリーズ
# Julia - SOARTメトリックス@Game2048の環境設定
# ------
using DataFrames, CSV
using LinearAlgebra
using Distances
using Plots
using Statistics
# ----
# Read the file using CSV.File and convert it to DataFrame
# ベンド系部品
# -- bend1 --
PATH_bend1 = "./conv_parts/bend1_cnv.csv"
df_bend1 = DataFrame(CSV.File(PATH_bend1; header=false))
conv_bend1 = Matrix(df_bend1)
conv_bend1 = convert(Matrix{Float64},conv_bend1)
println("--- conv_bend1 ---")
println(conv_bend1)
# -- bend2 --
PATH_bend2 = "./conv_parts/bend2_cnv.csv"
df_bend2 = DataFrame(CSV.File(PATH_bend2; header=false))
conv_bend2 = Matrix(df_bend2)
conv_bend2 = convert(Matrix{Float64},conv_bend2)
println("--- conv_bend2 ---")
println(conv_bend2)
# -- bend3 --
PATH_bend3 = "./conv_parts/bend3_cnv.csv"
df_bend3 = DataFrame(CSV.File(PATH_bend3; header=false))
conv_bend3 = Matrix(df_bend3)
conv_bend3 = convert(Matrix{Float64},conv_bend3)
println("--- conv_bend3 ---")
println(conv_bend3)
# -- bend4 --
PATH_bend4 = "./conv_parts/bend4_cnv.csv"
df_bend4 = DataFrame(CSV.File(PATH_bend4; header=false))
conv_bend4 = Matrix(df_bend4)
conv_bend4 = convert(Matrix{Float64},conv_bend4)
println("--- conv_bend4 ---")
println(conv_bend4)
# ライン系部品
# -- line1 --
PATH_line1 = "./conv_parts/line1_cnv.csv"
df_line1 = DataFrame(CSV.File(PATH_line1; header=false))
conv_line1 = Matrix(df_line1)
conv_line1 = convert(Matrix{Float64},conv_line1)
println("--- conv_line1 ---")
println(conv_line1)
# -- line2 --
PATH_line2 = "./conv_parts/line2_cnv.csv"
df_line2 = DataFrame(CSV.File(PATH_line2; header=false))
conv_line2 = Matrix(df_line2)
conv_line2 = convert(Matrix{Float64},conv_line2)
println("--- conv_line2 ---")
println(conv_line2)
# クロス系部品
# -- cross --
PATH_cross = "./conv_parts/datum_cnv.csv"
df_cross = DataFrame(CSV.File(PATH_cross; header=false))
conv_cross = Matrix(df_cross)
conv_cross = convert(Matrix{Float64},conv_cross)
println("--- conv_cross ---")
println(conv_cross)
# ------
# 畳み込み部品をまとめる
mx_cvparts = []
push!(mx_cvparts, conv_bend1)
push!(mx_cvparts, conv_bend2)
push!(mx_cvparts, conv_bend3)
push!(mx_cvparts, conv_bend4)
push!(mx_cvparts, conv_line1)
push!(mx_cvparts, conv_line2)
# ------
println(mx_cvparts)
println(size(mx_cvparts))

# =================================================
# テクノメトリックスのベクトルを計算する
# =================================================
# SOARTパラメータの設定
num_rtm   = 6        # bend x4, line x2
conv_dim  = 3
rtm_dim   = 2
start_cutxs = [1,2]     # rtm_dim
start_cutys = [1,2]     # rtm_dim

# -----
# 畳み込みマトリックスを計算する関数
function convmx_arrays(arr_conv_parts, mx_base)
    # ------
    # 畳み込み行列の初期化
    mx_meas = zeros((rtm_dim, rtm_dim))
    # ------
    # 各ブロックのメトリックスを計算する
    for i in start_cutys        # start_cutys
        for j in start_cutxs    # start_cutxs  
            #println(mx_base[start_cutxs[i]:start_cutxs[i]+conv_dim-1, start_cutys[j]:start_cutys[j]+conv_dim-1])
            arr_mx_base = vec(mx_base[start_cutxs[i]:start_cutxs[i]+conv_dim-1, start_cutys[j]:start_cutys[j]+conv_dim-1])
            dot_value   = dot(arr_conv_parts, arr_mx_base)
            mx_meas[i,j] = dot_value
        end
    end
    #println("mx_meas: ", mx_meas)
    return mx_meas
end

# -----
# テクノ・メトリックスを計算する関数
function calc_techmetrics(conv_cross, mx_cvparts, mx_base)

    # クロス系畳み込みマトリックスを計算する
    mx_cvbase = convmx_arrays(vec(conv_cross), mx_base)
    #println("mx_cvbase: ",mx_cvbase)
    # ------
    # 新RTメトリックスの計算用(STEP1)
    xx = dot(vec(mx_cvbase),vec(mx_cvbase))
    #xx = sum(vec(mx_cvbase))
    # ------
    if xx > 0.00001
        # ------
        arr_beta     = []
        arr_distance = []
        for k in 1:num_rtm
            # ------
            # 計測用畳み込みマトリックスを計算する
            mx_cvmeas = convmx_arrays(vec(mx_cvparts[k]), mx_base)
            # ------
            # 新RTメトリックスの計算用(STEP1)
            xy = dot(vec(mx_cvbase),vec(mx_cvmeas))
            #xy = sum(vec(mx_cvmeas))
            beta = round(xy/xx; digits=5)
            push!(arr_beta, beta)
            #println("beta: ",beta)
            # ------
            # テクノ・メトリックスの計算用(STEP2)
            mDistance = round(cityblock(vec(mx_cvmeas), vec(beta*mx_cvbase)); digits=5)
            push!(arr_distance, mDistance)
            #println("mDistance: ", mDistance)
        end
        arr_beta      = convert(Array{Float64},arr_beta)
        arr_distance  = convert(Array{Float64},arr_distance)
    else
        arr_beta      = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
        arr_distance  = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
        #println("xx_value less than 0")
    end
    
    return arr_beta, arr_distance
end

# =================================================
# 2048ゲームを実行するための関数群
# =================================================
using StatsBase

# グローバル宣言
const DIRS = [:left, :up, :right, :down]
rand2_1()  = rand() < 0.1 ? 2 : 1

function init_game()
    grid = zeros(Int8,4,4)    
    grid[rand(1:4),rand(1:4)] = rand2_1()
    grid[rand(1:4),rand(1:4)] = rand2_1()
    return grid
end

# a function to simulate the move and return a reward
function move!(x, xinc, xstart, xend)
    reward = 0
    @inbounds for i = xstart:xinc:xend # for each row move the left most piece first #if move_row = 1 then i control the row
        if x[i] != 0 # if the position is occupied by a number move it
            # firstly look "behind" to see if there is a number that is the same
            # this is to deal better with situations like 2 2 4 4
            @inbounds for k = i+xinc:xinc:xend
                if x[k] != 0
                    if x[k] == x[i]
                        x[i] += 1
                        reward += 2^x[i]
                        x[k] = 0
                    end
                    break;
                end
            end
            # now place it in the first empty slot
            @inbounds for k = xstart:xinc:i
                if x[k] == 0
                    x[k] = x[i]
                    x[i] = 0
                end
            end
        end
    end
    (x, reward)
end

function move_left!(x)
    move!(x, 1, 1, 4)
end

function move_right!(x)
    move!(x, -1, 4, 1)
end

function move!(grid::Array{T,2}, direction) where T <: Integer
    reward::Int16 = zero(Int16)
    if direction == :left
        for j = 1:4
            (tmp, new_reward) = move_left!(@view grid[j,:])
            reward += new_reward
        end
    elseif direction == :right
        for j = 1:4
            (tmp, new_reward) = move_right!(@view grid[j,:])
            reward += new_reward
        end
    elseif direction == :up
        for j = 1:4
            (tmp, new_reward) = move_left!(@view grid[:,j])
            reward += new_reward
        end
    else
        for j = 1:4
            (tmp, new_reward) = move_right!(@view grid[:,j])
            reward += new_reward
        end
    end
    (grid, reward)
end

# -------
# SIMULATION - TILE MOVE
# -------
# assume no need to check for validate moves
function simulate_move!(grid, direction)
    tmp_grid = copy(grid)
    (grid, reward) = move!(grid, direction)
    if all(tmp_grid .== grid)
        return (grid, false, CartesianIndex{2}(-1,-1), -1, 0)
    else
        cart = rand(findall(grid .== 0)) # randomly choose one empty slot
        one_or_two = rand2_1()
        grid[cart] = one_or_two
        return (grid, true, cart, one_or_two, reward)
    end
end

# =================================================
# TRIAL【1】_Game2048_DQN_TechnoMetricsRT_with_FEEDBACK
# =================================================
#using ReinforcementLearning
using Flux
using Flux: mse, huber_loss, onehotbatch
using Random
#using Statistics
#using Plots
#using LinearAlgebra

# ---------------------------
# グローバル宣言
global num_state
global num_actions 
global in_dim 
global out_dim

# ---------------------------
# Defining all the required parameters
global memory
global ε
global γ 
global e_decay  
global e_min   
global learning_rate
global total_episodes 

# =================================================
#    Args:
#        ε: The degree of exploration
#        γ: The discount factor
#        num_state: The number of states
#        num_actions: The number of actions
#        action_space: To call the random action
# =================================================
num_state     = 12 + 1
num_actions   = 4
in_dim  = num_state + num_actions
out_dim = 1

# Defining all the required parameters
ε     = 0.95
γ     = 0.99
e_decay    = 0.9999
e_min      = 0.01
total_episodes = 20000

# ---------------------------
# ハイパーパラメタ
const BATCH_SIZE    = 128  # サンプルサイズ
const MEMORY_CAPACITY = 128 * 32  # メモリ容量

# =================================================
# AGENT functions
# =================================================
# リセット：基本変数を初期化する
function reset!()
    # ---------------------------
    # ゲームの初期化(2タイルが２か所)
    grid = init_game()
    # ---------------------------
    arr_acType    = []
    arr_action    = []
    arr_Qvalue    = []
    arr_reward    = []
    mx_states     = []
    return grid, arr_acType, arr_action, arr_Qvalue, arr_reward, mx_states
end

# ----------------
# 命令を選択し、ゲームを動かす(ランダム判定)
function step_random!(grid) 
    Qvalue = -0.0001
    acType = "random"
    # ---------------- 
    # DIRS = [:left, :up, :right, :down]
    # ---------------- 
    directions = sample(DIRS, 4 , replace = false)
    flg_moved = false     # falseとは「ゲーム盤が動かない」という意味
    for i in 1:3
        d1 = directions[i]
        (grid, ok, cart, two_or_four, reward) = simulate_move!(grid, d1)
        if ok
            flg_moved = true     # trueとは「動いた」という意味
            return (d1, acType, Qvalue, grid, reward, flg_moved)
        end
    end
    (grid, ok, cart, two_or_four, reward) = simulate_move!(grid, directions[4])
    return (directions[4], acType, Qvalue, grid, reward, flg_moved)
end

# ----------------
# 命令を選択し、ゲームを動かす(マシン判定)
function step_machine!(grid, state_cur, iCnt_play, Qvalue)
    # ---------------- 
    # DIRS = [:left, :up, :right, :down]
    # ----------------
    # 最適命令を選択する(DQN)
    flg_moved = true         # falseとは「ゲーム盤が動かない」という意味
    acType    = "machine"
    mx_input  = zeros(4,in_dim)
    #println(mx_input)
    for action in 1:4
        # [array]-action を生成します。
        a_onehot = onehotbatch(action, 1:4)
        temp_s_a = vcat(state_cur, a_onehot)
        #println("temp_s_a-choose action: ", temp_s_a)
        mx_input[action,:] = temp_s_a'     # 状態(S)行動(A)マトリックス
    end
    #println("----- mx_input(choose_action: machine) -----")
    #println(mx_input)
    # ----------------
    # predict 'y'
    y_pred  = model(mx_input')
    Qvalue  = maximum(y_pred)
    a_order = argmax(y_pred)[2]
    action  = DIRS[a_order]
    # ----------------
    # ゲーム盤を動かす
    (grid, flg_moved, cart, two_or_four, reward) = simulate_move!(grid, action)

    return action, acType, round(Qvalue; digits=5), grid, reward, flg_moved
end

# ----------------
# ゲームオーバーしているか？
function is_gameover!(grid) 
    # ---------------- 
    # DIRS = [:left, :up, :right, :down]
    # ---------------- 
    flg_gameover = true      # trueとは「ゲームオーバーである」という意味
    for i in 1:4
        d1 = DIRS[i]
        (grid, ok, cart, two_or_four, reward) = simulate_move!(grid, d1)
        if ok
            flg_gameover = false     # falseとは「ゲームオーバーではない」という意味
            return flg_gameover
        end
    end
    return flg_gameover
end

# =================================================
# DQN_Solver functions
# =================================================
# crate instance for input
#global model       = Chain(Dense(in_dim, 256, relu), Dense(256, 256, relu), Dense(256, out_dim)) # Use Chain if you want to stack layers
global model       = Chain(Dense(in_dim, 512, relu), Dense(512, out_dim)) # Use Chain if you want to stack layers

# パラメタ
global ps_model    = params(model)
# -----
# 損失関数
#global loss(x, y) = mse(model(x'), Matrix(y'))            # 二乗平均誤差
global loss(x, y)  = huber_loss(model(x'), Matrix(y'))    # フーバー誤差
# -----
# オプティマイザ
global opt         = ADAM()

# ----------------
# メモリを蓄積する
function remember_memory(iCnt_play, iTurn, memory, state_cur, action, reward, state_next, done)
    push!(memory, (iCnt_play, iTurn, state_cur, action, reward, state_next, done))
    len_memory = length(memory)
    if len_memory > MEMORY_CAPACITY
        memory = memory[2:end]
    end
    return memory
end

# ----------------
# (REPLAY EXPERIENCE)学習する
function replay_experience(iCnt_play, memory, batch_size)
    # --------------------------------------------------
    len_memory = length(memory)
    batch_size = min(batch_size, len_memory)
    minibatch  = rand(memory, batch_size)
    X, Y   = zeros(batch_size,in_dim), zeros(batch_size)
    # --------------------------------------------------
    for ibat in 1:batch_size
        iPlay, iTurn, state_cur, str_action, reward, state_next, done = minibatch[ibat]
        # CURRENT: < X >-MATRIX
        # ---------------- 
        # DIRS = [:left, :up, :right, :down]
        # ----------------
        if str_action == :left
            action = 1
        elseif str_action == :up
            action = 2
        elseif str_action == :right
            action = 3
        else 
            action = 4
        end
        # ---------------- 
        a_onehot = onehotbatch(action, 1:4)
        state_action_cur = vcat(state_cur, a_onehot)
        #println("state_action_cur: $state_action_cur")
        X[ibat,:]  = state_action_cur'
        # ----------------
        # NEXT: < Y >-ARRAY
        if done == true
            Y[ibat] = reward
        else
            mx_input = zeros(4,in_dim)  # 状態(state)と行動(action)マトリックス
            for imov in 1:4
                # [array]-action を結合します。
                a_onehot = onehotbatch(imov, 1:4)
                temp_s_a = vcat(state_next, a_onehot)
                #println("temp_s_a-choose action: ", temp_s_a)
                mx_input[imov,:] = temp_s_a'      # 状態(S)行動(A)マトリックス
            end
            #println("--- mx_input(replay_experience) ---")
            #println(mx_input)
            # ----------------
            # predict 'y'
            y_pred   = model(mx_input')
            target_f = reward + γ * maximum(y_pred)
            #println("target_f: $target_f")
            Y[ibat]  = target_f
        end
    end
    # ----------------
    # TRAINING
    # calculate loss -> gradient
    gs = gradient(() -> loss(X, Y), ps_model)
    # update weights
    Flux.Optimise.update!(opt, ps_model, gs)
    # ----------------
    val_loss = round(loss(X, Y); digits=5)
    #println("iPlay: $iCnt_play, loss: $val_loss")
    return val_loss
end

# =================================================
# EPISODE CONTROL functions
# =================================================
# エピソードを運用する
function get_episode!(iCnt_play, memory, rep_bonuscore)

    # リセット：基本変数を初期化する
    (grid, arr_acType, arr_action, arr_Qvalue, arr_reward, mx_states) = reset!()
    # -----
    # SOARTテクノメトリックスを計算する[RESET-CURRENT]
    # -----
    flg_moved, flg_gameover, iTurn, total_score = true, false, 1, 0
    non_valid_count, valid_count = 0, 0
    # -----
    board = convert(Matrix{Float64}, grid)
    arr_beta, arr_distance = calc_techmetrics(conv_cross, mx_cvparts, board)
    state_cur  = vcat(arr_beta, arr_distance)
    push!(state_cur, iTurn ) # iTurnを追加する
    while flg_gameover == false        # ゲームオーバーでループから外れる
        # 初期化
        Qvalue, acType  = -0.0001, "random"        
        # ----------------
        # 命令を選択し、ゲームを動かす
        if ε < rand() && flg_moved == true
            # DQNでコマを動かす
            (action, acType, Qvalue, grid, reward, flg_moved) = step_machine!(grid, state_cur, iCnt_play, Qvalue)
            # ゲーム盤が動かなければ報酬はマイナスになる
            #println("flg_moved: ", flg_moved)
        else
            # ランダムでコマを動かす
            (action, acType, Qvalue, grid, reward, flg_moved) = step_random!(grid)
        end
        #if iTurn%10 == 0
        #    println("Play: $iCnt_play, Turn: $iTurn, grid: $grid, acType: $acType, action: $action, reward: $reward, flg_moved: $flg_moved")
        #end
        # ----------------
        # SOARTテクノメトリックスを計算する[RUN-NEXT]
        # ----------------
        board_next = convert(Matrix{Float64}, grid)
        arr_beta, arr_distance = calc_techmetrics(conv_cross, mx_cvparts, board_next)
        state_next = vcat(arr_beta, arr_distance)
        push!(state_next, iTurn ) # iTurnを追加する
        #println("UPDATED - state_next: $state_next")
        # ----------------
         # 記録用リストを追記する
        push!(arr_acType, acType)
        push!(arr_action, action)
        push!(arr_Qvalue, Qvalue)
        push!(arr_reward, reward)
        if iTurn == 1
            mx_states = state_cur
        else
            mx_states = cat(mx_states, state_cur, dims=2)
        end
        # ----------------
        state_cur   = state_next
        total_score = total_score + reward
        # ----------------
        # 報酬修正(1) : FEED_BACK
        #if flg_moved == false
        #    reward = -5
        #else
        reward = round(reward + 0.5*(maximum(state_cur[7:10]) - minimum(state_cur[7:10])); digits=3)
        #end
        #println("報酬量修正後 : ", reward)
        # ----------------
        # カウント && ゲームオーバーしているか？ 
        if flg_moved == false
            non_valid_count = non_valid_count + 1
            flg_gameover = is_gameover!(grid)
        else
            valid_count = valid_count + 1
        end 
        # ----------------
        # ゲームオーバーした場合、ブレイクする
        if flg_gameover == true
            #println("BREAK!! - Play: $iCnt_play, Turn: $iTurn, flg_gameover: $flg_gameover")
            # 報酬修正(2)  ボーナス
            if total_score > rep_bonuscore*1.5 && iCnt_play > 20
                reward = 0.5*valid_count
            elseif total_score > rep_bonuscore && iCnt_play > 20
                reward = 0.3*valid_count
            else
                reward = -1.0*non_valid_count
            end
        else
            iTurn = iTurn + 1
        end
        # ----------------
        # Experience Replay配列に保管する
        memory = remember_memory(iCnt_play, iTurn, memory, state_cur, action, reward, state_next, flg_gameover)
    end
    # ---------------------------
    # ゲーム代表値のリスト
    rep_predQV = quantile(arr_Qvalue, [0.25, 0.50, 0.75, 1])
    
    return (iTurn, round(total_score; digits=2), memory, rep_predQV, valid_count, non_valid_count, mx_states')
end

# =================================================
# MAIN ROUTINE
# =================================================
# 記録用パラメタ類
memory        = []  # メモリのリスト
# ---------------------------
# 記録用パラメタ類
arr_iplay     = []  # count game play    プレイ番号
arr_maxturn   = []  # turn game play    ターン数
arr_maxscore  = []  # rl_score game play    報酬の総和
arr_loss      = []  # DQN-Experience Replay学習
arr_εs       = []  # ε-Greedy
# ---------------------------
# Q値の分析用
arr_maxQV     = []  # QV値の最大値
arr_q25QV     = []  # QV値の4分の1値
arr_q50QV     = []  # QV値の4分の2値
arr_q75QV     = []  # QV値の4分の3値
arr_nvalid    = []  # 無効命令率
# ---------------------------
# 初期化
val_loss      = 0.0
iCnt_play     = 1
rep_bonuscore = 1000
# ---------------------------
# エピソードを実行する
while true   
    # ゲームを動かす
    (maxturn, maxscore, memory, rep_predQV, valid_count, non_valid_count, mx_states) = get_episode!(iCnt_play, memory, rep_bonuscore)
    # 無効命令率の計算
    ratio_nvalid = round(non_valid_count/maxturn; digits=3)
    # 簡単な出力
    #println("maxturn: $maxturn, maxscore: $maxscore, ratio_nvalid: $ratio_nvalid")

    # ----------
    # DQN-Experience Replay学習(重み付き)
    if valid_count >= 150
        for iLearn in 1:50
            val_loss = replay_experience(iCnt_play, memory, BATCH_SIZE)
        end
    elseif valid_count > 100 && valid_count < 150
        for iLearn in 1:20
            val_loss = replay_experience(iCnt_play, memory, BATCH_SIZE)
        #    println("Learn_turn:$iLearn, loss:$val_loss")
        end
    else
        val_loss = replay_experience(iCnt_play, memory, BATCH_SIZE)
    end

    # ----------
    # 学習結果の出力
    if iCnt_play%40 == 0
        println("iPlay: $iCnt_play, maxturn: $maxturn, maxscore: $maxscore, ε:$ε, loss:$val_loss, bonuscore:$rep_bonuscore, ratio_nvalid:$ratio_nvalid")
    end
    #println(mx_states)
    #println(size(mx_states))
    # ----------
    # 記録用パラメタ類(プレイベース)の追加
    push!(arr_iplay, iCnt_play)  # count game play    プレイ番号
    push!(arr_maxturn, maxturn)  # max_turn   ゲームのターン数
    push!(arr_maxscore, maxscore)  # rl_score game play    最終プレイスコア
    push!(arr_loss, val_loss)    # DQN-Experience Replay学習
    push!(arr_εs, ε)  # イプシロン
    # ----------
    # ゲーム代表値の保管
    push!(arr_maxQV, rep_predQV[4])  # QV値の最大値
    push!(arr_q75QV, rep_predQV[3])  # QV値の4分の3値
    push!(arr_q50QV, rep_predQV[2])  # QV値の4分の2値
    push!(arr_q25QV, rep_predQV[1])  # QV値の4分の1値
    push!(arr_nvalid, ratio_nvalid)  # 無効命令率
    # ----------
    # Change EPSILON
    if ε > e_min && iCnt_play < total_episodes        # total_episodes
        ε *= e_decay
        ε = round(ε; digits=5)
        iCnt_play = iCnt_play + 1
    else
        break
    end
end

# ---------
# Create Graph(2) : 移動平均による平滑化
using MarketTechnicals

# A-GROUP
pltA1 = plot(arr_εs, label="EPSILON", legend=:topright)
pltA2 = plot(arr_loss, label="loss", legend=:topleft)
        plot!(ema(arr_loss, 10, wilder = false), label="movave", legend=:topleft)
pltA3 = plot(arr_maxturn, label="maxturn", legend=:topleft)
        plot!(ema(arr_maxturn,10, wilder = false), label="movave", legend=:topleft)
pltA4 = plot(arr_maxscore, label="maxscore", legend=:topleft)
        plot!(ema(arr_maxscore,10, wilder = false), label="movave", legend=:topleft)
plot(pltA1, pltA2, pltA3, pltA4)
```

**(MAXPOOL 256-256)**

![imageJRL1-27-3](/2023-02-01-QEUR22_2048S7/imageJRL1-27-3.jpg)

**(SOART 256-256)**

![imageJRL1-27-4](/2023-02-01-QEUR22_2048S7/imageJRL1-27-4.jpg)

```julia
# B-GROUP
pltB1 = plot(ema(arr_maxQV,10, wilder = false), label="max", legend=:topleft)
        plot!(ema(arr_q75QV,10, wilder = false), label="q75", legend=:topleft)
        plot!(ema(arr_q50QV,10, wilder = false), label="q50", legend=:topleft)
        plot!(ema(arr_q25QV,10, wilder = false), label="q25", legend=:topleft)
pltB2 = plot(ema(arr_nvalid,10, wilder = false), label="non_valid", legend=:topleft)
plot(pltB1, pltB2)
```

**(MAXPOOL 256-256)**

![imageJRL1-27-5](/2023-02-01-QEUR22_2048S7/imageJRL1-27-5.jpg)

**(SOART 256-256)**

![imageJRL1-27-6](/2023-02-01-QEUR22_2048S7/imageJRL1-27-6.jpg)

D先生： “う～ん・・・。MAXPOOLとSOARTは微妙な違いだなァ・・・。”

QEU:FOUNDER ： “そこらへんの良し悪しを敢えて測るのが今回の目的です。一つの結果だけを見てもわかりません。ちなみに、今回のロバストネス評価では、比較対象はnewRTとSOARTですからね。”

```julia
    # ------
    # 新RTメトリックスの計算用(STEP1)
    xx = dot(vec(mx_cvbase),vec(mx_cvbase))
    #xx = sum(vec(mx_cvbase))
    # ------
    if xx > 0.00001
        # ------
        arr_beta     = []
        arr_distance = []
        for k in 1:num_rtm
            # ------
            # 計測用畳み込みマトリックスを計算する
            mx_cvmeas = convmx_arrays(vec(mx_cvparts[k]), mx_base)
            # ------
            # 新RTメトリックスの計算用(STEP1)
            xy = dot(vec(mx_cvbase),vec(mx_cvmeas))
            #xy = sum(vec(mx_cvmeas))
            beta = round(xy/xx; digits=5)
```

D先生： “xxとxyのコメントアウト有無の部分ですね。うわぁ・・・。微妙な差だなぁ・・・。”

QEU:FOUNDER ： “うまくいけばメッケもの・・・。それでは、誤差因子として512の単層関数でSOARTの強化学習を行い総合SN比を計算しましょう。”


## ～　まとめ　～

### ・・・　前回のつづきです　・・・

QEU:FOUNDER ： “さぁて・・・。G国にも、もうそろそろ追い越されるかもね。彼らって**「すごい」**し・・・。”

C部長 : “すごいんですか？”

![imageJRL1-27-7](/2023-02-01-QEUR22_2048S7/imageJRL1-27-7.jpg)

QEU:FOUNDER ： “**ヨーロッパにおける自動車の発祥はG国**ですよ。彼らが、A国の工場を迎え入れたというのはすごいと思わない？もし、J国だったらT(@A)社の電気自動車生産工場の国内建設を歓迎するかねぇ・・・？”

C部長 : “うっ・・・。**「立場」**が・・・。”

[![MOVIE1](http://img.youtube.com/vi/3Fiy9Te7r9Y/0.jpg)](http://www.youtube.com/watch?v=3Fiy9Te7r9Y "Tesla股價現神奇反彈！為何？一架車淨利潤抵豐田八架、更超越Benz！介紹Tesla賺大錢秘訣！是否無人能挑戰其地位？")

QEU:FOUNDER ： “・・・でしょう？G国には、その意味での**「覚悟」がある**んですよ。あと、A国のT社の最新工場の**工程の自動化比率は95％**らしいですね。”

C部長 : “すごい！！”

QEU:FOUNDER ： “G国は、**それ（高度の生産技術）も欲しかった**んじゃないんですか？同じく大規模工場を歓迎したC国もそう・・・。こう考えると疑問がわくよね。**「T社の生産システム（TPS）」って、近い将来、まだ要るのかね？**「立場主義」じゃなく、本音で答えて？”

C部長 : “「立場なし」で答えます！**もういりません！！**人がいないし・・・。”

