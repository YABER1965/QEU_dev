---
title: QEUR22_CLIFF6:　Cliff_Walkingで遊ぼう（その7：JuliaRLAGENT編-フィードバック制御）
date: 2023-01-18
tags: ["QEUシステム", "メトリックス", "Julia言語", "Cliff Walking", "Flux", "ディープラーニング", "強化学習"]
excerpt: Julia言語を使った強化学習
---

## QEUR22_CLIFF6:　Cliff_Walkingで遊ぼう（その7：JuliaRLAGENT編-フィードバック制御）

## ～　自己中心とステークホルダー重視　～

D先生(設定年齢65歳)  ： “いよいよ来ました！つぎはフィードバック制御です。盛り上がっている途中ですが、この番組は**「高齢者によるイノベーション」**です。フィードバック制御の開発については、いままでさんざん説明したんですが・・・。”

![imageJRL1-15-1](/2023-01-18-QEUR22_CLIFF6/imageJRL1-15-1.jpg)

QEU:FOUNDER(設定年齢65歳) ： “多分、数式（プログラム命令文）が一番わかりやすいと思いますよ。”

```julia
  # フィードバック・ループ
 reward_cur = reward_cur - 0.3*state_cur[3]      # Aタイプ(自己中心型)
 reward_cur = reward_cur - 0.5*abs(state_cur[3])      # Bタイプ(ステークホルダー重視型)
```

D先生  ： “AタイプとBタイプの区別はいままでなかったですね。**数式上の差異はBタイプに絶対値ABS()が入っている**ことです。”

![imageJRL1-15-2](/2023-01-18-QEUR22_CLIFF6/imageJRL1-15-2.jpg)

QEU:FOUNDER ： “**経営環境には、ゲームプレイヤーとしての「組織」だけでなく、その「ステークホルダー」も存在します**。ゲームプレイヤーとステークホルダーの考え方が一致していると、ゲームはとてもやりやすい。Cliff_Walkingの例だと・・・。”

D先生 ： “Cliff(崖)のそばを歩いていけばいいです。”

QEU:FOUNDER ： “しかし、ステークホルダーが**「少しだけ違った考え方」**を持っていればどうなるのか？今回の例では、基準経路は最適経路にはなっていません。この基準経路はステークホルダーが要求したものとします。ゲームプレイヤーが、どのような経路を取るのかは・・・。”

D先生 ： “**ステークホルダーの声の大きさで決まります**（笑）。”

QEU:FOUNDER ： “そこらへんの事情をTypeAとTypeBの数式で表現したんです。どう？面白いでしょ？それでは実際に計算してみましょう。プログラムは前回とほとんど同じになりますので、ほとんどを省略します。”

```julia
# ---
# TRIAL【6】_Cliff_Walking_DQN_TechnoMetricsRT_FEEDBACK
# ---
using ReinforcementLearning
using Flux
using Flux: mse, onehotbatch
using Random
using Statistics
using Plots
using LinearAlgebra

# ----------
# 中間は同じなのでスキップ

# =================================================
# Calculation class
# =================================================
# エピソードを運用する
function get_episode(iCnt_play, memory)

        # ---------------------------
        # フィードバック・ループ
        reward_cur = reward_cur - 0.3*state_cur[3]      # Aタイプ(自己中心型)
        #reward_cur = reward_cur - 0.5*abs(state_cur[3])      # Bタイプ(ステークホルダー重視型)

# ----------
# Create Graph
# A-GROUP
pltA1 = plot(arr_εs, label="EPSILON")
pltA2 = plot(arr_loss, label="loss")
pltA3 = plot(arr_maxturn, label="maxturn")
pltA4 = plot(arr_maxscore, label="maxscore")
plot(pltA1, pltA2, pltA3, pltA4)
```

**(今回：フィードバック-TypeA)**

![imageJRL1-15-3](/2023-01-18-QEUR22_CLIFF6/imageJRL1-15-3.jpg)

**(今回：フィードバック-TypeB)**

![imageJRL1-15-4](/2023-01-18-QEUR22_CLIFF6/imageJRL1-15-4.jpg)

**(前回：符号化あり)**

![imageJRL1-15-5](/2023-01-18-QEUR22_CLIFF6/imageJRL1-15-5.jpg)

D先生 ： “やはり**フィードバック制御のパワーは「異次元」**ですね。ちょっと意外だったのは、タイプAとBの両方で学習収束の状況に大きな差がなかったことですね。タイプBの場合、もっと揺らくのかと思いました。”

QEU:FOUNDER ： “今回の環境（ゲーム）がシンプルだからでしょう。もしも複雑な環境であれば、もっと大きな差が出てきたとおもいますよ。”

```julia
# B-GROUP
pltB1 = plot(arr_maxQV, label="max")
		plot!(arr_q75QV, label="q75")
		plot!(arr_q25QV, label="q25")
pltB2 = plot(arr_sumRT, label="sumRT")
plot(pltB1, pltB2)
```

**(今回：フィードバック-TypeA)**

![imageJRL1-15-6](/2023-01-18-QEUR22_CLIFF6/imageJRL1-15-6.jpg)

**(今回：フィードバック-TypeB)**

![imageJRL1-15-7](/2023-01-18-QEUR22_CLIFF6/imageJRL1-15-7.jpg)

**(前回：符号化あり)**

![imageJRL1-15-8](/2023-01-18-QEUR22_CLIFF6/imageJRL1-15-8.jpg)

D先生 ： “Q値グラフだけでなく、テクノメトリックス推移でも前回と大きな差異があります。フィードバック制御になると学習が圧倒的にはやくなります。”

QEU:FOUNDER ： “このゲームでは、ゴールに到達するとはじめて正の報酬がもらえますが。フィードバックにすることで、**アクションの毎回に報酬をもらえる**ためでしょう。Q値グラフでもタイプA/Bで大きな差異が出なかったですね。”

D先生 ： “**タイプBの最適経路はどうなった**んでしょうね？”

QEU:FOUNDER ： “じゃあ、解析用プログラムを作って調べてみましょう。ドン・・・。”

```julia
# ----------------
# 最適経路を表示する
# ----------------
# 命令を選択する(最適経路の出力用)
function choose_action2(state_cur)

    # ----------------------------------------
    # 移動命令
    # LRUD = [
    #    [0.0, -1.0],  # 1->left
    #    [0.0, 1.0],   # 2->right
    #    [-1.0, 0.0],  # 3->up
    #    [1.0, 0.0],   # 4->down
    # ----------------
    Qvalue = -0.0001
    #println("BIGIN:choose_action")
    # ----------------
    # 最適命令の選択（DQNによる選択）
	mx_input = zeros(4,in_dim)
	for action in 1:4
		# [array]-action を生成します。
		a_onehot = onehotbatch(action, 1:4)
		temp_s_a = cat(state_cur, a_onehot , dims=1)'
		mx_input[action,:] = temp_s_a     # 状態(S)行動(A)マトリックス
	end
	# ----------------------------------------
	# predict 'y'
	y_pred  = model(mx_input')
	Qvalue  = maximum(y_pred)
	a_order = argmax(y_pred)[2]

    return a_order, round(Qvalue; digits=5)
end

# ---------------------------
# 機械学習用のパラメタ群
Qvalue, reward_cur, done = -0.0001, 0, false
# ---------------------------
# 記録用パラメタ類(ターンベース)
arr_iturn  = []  # ターン・カウンタリスト
orders_row = []  # 指示リスト(row)
orders_col = []  # 指示リスト(col)
arr_orders = []  # 指示リスト(命令)
arr_scores = []  # ゲームスコアリスト
arr_dones  = []  # ゲームオーバーフラグ
arr_predQV = []  # Q値のリスト
arr_RTdist = []  # RTテクノメトリックスのリスト
# ---------------------------
# ゲームをリセットする
state_cur, position_cur = env_reset!(env)
# ---------------------------
# ゲームをプレイする
iCnt_turn = 0
while true
	# 命令(a_order)を作成する
	a_order, Qvalue = choose_action2(state_cur)
	# ---------------------------
	# ゲームをもう一歩進める
	state_next, position_next, reward_cur, done = env_step!(env, a_order)
	# 経路を出力する
	println("iTurn: $iCnt_turn, position_cur: $position_cur, action: $a_order, done: $done, reward: $reward_cur")
	# ---------------------------
	# 記録用リストを追記する
	push!(arr_iturn, iCnt_turn)  # ターン・カウンタリスト
	push!(orders_row, position_cur[1])  # 指示リスト(ROW)
	push!(orders_col, position_cur[2])  # 指示リスト(COL)
	push!(arr_orders, a_order)  # 指示リスト(命令)
	push!(arr_scores, reward_cur)  # ゲームスコアリスト
	push!(arr_dones, done)  # ゲームオーバーフラグ
	push!(arr_predQV, Qvalue)  # Q値のリスト
	push!(arr_RTdist, state_cur[3])  # RTテクノメトリックスのリスト
	# ----------------
	# 実行継続するか分岐
	if done == true
		# Game over
		println("GAMEOVER: position_next: $position_next ")
		break  # (game over rip)
	else
		# count of game turn
		position_cur  = position_next
		state_cur     = state_next
		iCnt_turn     = iCnt_turn + 1
	end
end
```

![imageJRL1-15-9](/2023-01-18-QEUR22_CLIFF6/imageJRL1-15-9.jpg)

D先生 ： “なんだ・・・。タイプAと同じく崖のそばを歩いています。”

QEU:FOUNDER ： “**まだステークホルダーの声が小さい**んでしょう。あと、崖から落ちる負の報酬の大きさでも結果が変わってくるでしょう。あとは、自分で遊んで（実験して）ください。”

D先生 ： “この番組は「～で遊ぼう」のコーナーです。”

QEU:FOUNDER ： “最先端のプログラミング言語の一つであるJulia言語も覚えられるし、**おトクな番組**ですね(笑)。”


## ～　まとめ　～

C部長 : “なんか、個人的に驚きましたが・・・。**「Cliff_Walkingで遊ぼう」のシリーズは成功で終わった**ようです。”

QEU:FOUNDER ： “なにを言っとる！？今回は、いままでやってきたことをまとめただけであり、**新しい事柄はほとんどない**。しいて言えば、Julia言語をを導入したことで**「実用化（イケる）」が近づいている**ことを実感してもらったことかな・・・。”

C部長 : “次は、もう一つのテクノメトリックスである**SOARTメトリックスの応用**に行きますか？”

![imageJRL1-15-10](/2023-01-18-QEUR22_CLIFF6/imageJRL1-15-10.jpg)

QEU:FOUNDER ： “SOARTメトリックスは教師あり学習でも「もちろんイケル！」し、強化学習でもイケルからね。”

C部長 : “強化学習で、SOART法を適用した事例をやりたいですね。”

QEU:FOUNDER ： “今かね？”

C部長 : “だめですか？”

QEU:FOUNDER ： “2048ゲームの件は後に残しておきたい。ちょっと考えておくわ・・・”

