---
title: QEUR22_2048S10:　番外編～ゲーム2048でロバストネスを測る（SLIDERT-1）
date: 2023-02-02
tags: ["QEUシステム", "メトリックス", "Julia言語", "2048", "SOART", "ディープラーニング", "強化学習", "ロバストネス"]
excerpt: julia言語とテクノメトリックスを使った強化学習とロバストネス評価
---

## QEUR22_2048S10:　番外編～ゲーム2048でロバストネスを測る（SLIDERT-1）

## ～　外観検査自動機への布石　～

D先生(設定年齢65歳)   ： “さあて、QEUシステムROUND2-2の強化学習編はヤマ場を終えてウィニング・ランに来ました。あとはゆっくりやりましょう。残る課題はSLIDERTを用いて総合SN比による比較ですね。さて、この番組は「高齢者によるイノベーション」です。”

![imageJRL1-30-1](/2023-02-02-QEUR22_2048S10/imageJRL1-30-1.jpg)

QEU:FOUNDER(設定年齢65歳) ： “ついでに2種の感度である**「回転(newRT)vs明るさ(SOART)」の比較**もできればありがたい。今までは畳み込みでメトリックスを作っていたんですが、SLIDERTではゲームのシミュレーションによって、単位空間（前）と信号空間（後）をつくって比較しているので特性がちがうでしょうね。”

![imageJRL1-30-2](/2023-02-02-QEUR22_2048S10/imageJRL1-30-2.jpg)

D先生： “ここで、**「標準ベクトル=単位空間」、「計測ベクトル=信号空間」**と見ています。これは用語の問題でした・・・。これ以上は、いままでさんざんやってきたので説明が不要ですね。”

QEU:FOUNDER ： “そう？じゃあ、いきなりプログラムをドン！！予想するけど、あとで必ず質問が入ると思うよ（笑）。”

```julia
# ------
# テクノ・メトリックスの可能性を試すシリーズ
# Julia - SOARTメトリックス@Game2048の環境設定
# ------
using DataFrames, CSV
using LinearAlgebra
using Distances
using Plots
using Statistics

# =================================================
# テクノメトリックスのベクトルを計算する
# =================================================
# テクノ・メトリックスを計算する関数
function calc_techmetrics(grid, iTurn)

    # メトリックス配列を初期化する
    arr_metrics   = []
    arr_beta      = []
    arr_delta     = []
    arr_distance  = []
    # ------
    delta    　= 1.0
    beta     　= 1.0
    mDistance = 0.0
    # ベースのゲーム盤の状態
    board_temp  = convert(Matrix{Float64}, grid)
    board_cur   = vec(board_temp)
    # ------
    # 新RTメトリックスの計算用-beta(1)
    xx_beta  = dot(board_cur,board_cur)
    # 新RTメトリックスの計算用-delta(1)
    xx_delta = sum(board_cur)
    # ---------------- 
    # DIRS = [:left, :up, :right, :down]
    # ---------------- 
    for i in 1:4
        d1 = DIRS[i]
        (grid_next, ok, cart, two_or_four, reward) = simulate_move!(grid, d1)
        if ok == true
            # 移動後のゲーム盤の状態
            board_temp    = convert(Matrix{Float64}, grid_next)
            board_next    = vec(board_temp)
            # ------
            # 新RTメトリックスの計算用-beta(2)
            xy_beta  = dot(board_next, board_cur)
            # 新RTメトリックスの計算用-delta(2)
            yy_delta = sum(board_next)
            # ------
            beta  = round(xy_beta/xx_beta; digits=5)
            delta = round(yy_delta/xx_delta; digits=5)
            # ------
            # テクノ・メトリックスの計算用(距離)
			mDistance = round(cityblock(board_next, beta*board_cur); digits=5)
            #mDistance = round(cityblock(board_next, delta*board_cur); digits=5)
            #mDistance = round(mse(board_next, beta*board_cur); digits=5)
        else
            delta     = 1.0
            beta     　= 1.0
            mDistance = 0.0
        end
        # ------
        # 配列への追加
        push!(arr_delta, delta)
        push!(arr_beta, beta)
        push!(arr_distance, mDistance)
        println("delta: $delta, beta: $beta, mDistance: $mDistance")
    end

    # クロス系畳み込みマトリックスを計算する
    arr_metrics = vcat(arr_delta, arr_beta, arr_distance)
    push!(arr_metrics, iTurn)
    #println("arr_metrics: $arr_metrics")

    return arr_metrics
end

# =================================================
# 2048ゲームを実行するための関数群
# =================================================
using StatsBase

# グローバル宣言
const DIRS = [:left, :up, :right, :down]
rand2_1()  = rand() < 0.1 ? 2 : 1

function init_game()
    grid = zeros(Int8,4,4)    
    grid[rand(1:4),rand(1:4)] = rand2_1()
    grid[rand(1:4),rand(1:4)] = rand2_1()
    return grid
end

# a function to simulate the move and return a reward
function move!(x, xinc, xstart, xend)
    reward = 0
    @inbounds for i = xstart:xinc:xend # for each row move the left most piece first #if move_row = 1 then i control the row
        if x[i] != 0 # if the position is occupied by a number move it
            # firstly look "behind" to see if there is a number that is the same
            # this is to deal better with situations like 2 2 4 4
            @inbounds for k = i+xinc:xinc:xend
                if x[k] != 0
                    if x[k] == x[i]
                        x[i] += 1
                        reward += 2^x[i]
                        x[k] = 0
                    end
                    break;
                end
            end
            # now place it in the first empty slot
            @inbounds for k = xstart:xinc:i
                if x[k] == 0
                    x[k] = x[i]
                    x[i] = 0
                end
            end
        end
    end
    (x, reward)
end

function move_left!(x)
    move!(x, 1, 1, 4)
end

function move_right!(x)
    move!(x, -1, 4, 1)
end

function move!(grid::Array{T,2}, direction) where T <: Integer
    reward::Int16 = zero(Int16)
    if direction == :left
        for j = 1:4
            (tmp, new_reward) = move_left!(@view grid[j,:])
            reward += new_reward
        end
    elseif direction == :right
        for j = 1:4
            (tmp, new_reward) = move_right!(@view grid[j,:])
            reward += new_reward
        end
    elseif direction == :up
        for j = 1:4
            (tmp, new_reward) = move_left!(@view grid[:,j])
            reward += new_reward
        end
    else
        for j = 1:4
            (tmp, new_reward) = move_right!(@view grid[:,j])
            reward += new_reward
        end
    end
    (grid, reward)
end

# -------
# SIMULATION - TILE MOVE
# -------
# assume no need to check for validate moves
function simulate_move!(grid, direction)
    tmp_grid = copy(grid)
    (grid, reward) = move!(grid, direction)
    if all(tmp_grid .== grid)
        return (grid, false, CartesianIndex{2}(-1,-1), -1, 0)
    else
        cart = rand(findall(grid .== 0)) # randomly choose one empty slot
        one_or_two = rand2_1()
        grid[cart] = one_or_two
        return (grid, true, cart, one_or_two, reward)
    end
end

# =================================================
# TRIAL【1】_Game2048_DQN_TechnoMetricsRT_with_FEEDBACK
# =================================================
using Flux
using Flux: mse, huber_loss, onehotbatch
using Random

# ---------------------------
# グローバル宣言
global num_state
global num_actions 
global in_dim 
global out_dim

# ---------------------------
# Defining all the required parameters
global memory
global ε
global γ 
global e_decay  
global e_min   
global learning_rate
global total_episodes 

# =================================================
#    Args:
#        ε: The degree of exploration
#        γ: The discount factor
#        num_state: The number of states
#        num_actions: The number of actions
#        action_space: To call the random action
# =================================================
num_state     = 12 + 1
num_actions   = 4
in_dim  = num_state + num_actions
out_dim = 1

# Defining all the required parameters
ε     = 0.95
γ     = 0.99
e_decay    = 0.9999
e_min      = 0.001
total_episodes = 20000

# ---------------------------
# ハイパーパラメタ
const BATCH_SIZE    = 128  # サンプルサイズ
const MEMORY_CAPACITY = 128 * 32  # メモリ容量

# =================================================
# AGENT functions
# =================================================
# リセット：基本変数を初期化する
function reset!()
    # ---------------------------
    # ゲームの初期化(2タイルが２か所)
    grid = init_game()
    # ---------------------------
    arr_acType    = []
    arr_action    = []
    arr_Qvalue    = []
    arr_reward    = []
    mx_states     = []
    return grid, arr_acType, arr_action, arr_Qvalue, arr_reward, mx_states
end

# ----------------
# 命令を選択し、ゲームを動かす(ランダム判定)
function step_random!(grid) 
    Qvalue = -0.0001
    acType = "random"
    # ---------------- 
    # DIRS = [:left, :up, :right, :down]
    # ---------------- 
    directions = sample(DIRS, 4 , replace = false)
    flg_moved = false     # falseとは「ゲーム盤が動かない」という意味
    for i in 1:3
        d1 = directions[i]
        (grid, ok, cart, two_or_four, reward) = simulate_move!(grid, d1)
        if ok
            flg_moved = true     # trueとは「動いた」という意味
            return (d1, acType, Qvalue, grid, reward, flg_moved)
        end
    end
    (grid, ok, cart, two_or_four, reward) = simulate_move!(grid, directions[4])
    return (directions[4], acType, Qvalue, grid, reward, flg_moved)
end

# ----------------
# 命令を選択し、ゲームを動かす(マシン判定)
function step_machine!(grid, state_cur, iCnt_play, Qvalue)
    # ---------------- 
    # DIRS = [:left, :up, :right, :down]
    # ----------------
    # 最適命令を選択する(DQN)
    flg_moved = true         # falseとは「ゲーム盤が動かない」という意味
    acType    = "machine"
    mx_input  = zeros(4,in_dim)
    #println(mx_input)
    for action in 1:4
        # [array]-action を生成します。
        a_onehot = onehotbatch(action, 1:4)
        temp_s_a = vcat(state_cur, a_onehot)
        #println("temp_s_a-choose action: ", temp_s_a)
        mx_input[action,:] = temp_s_a'     # 状態(S)行動(A)マトリックス
    end
    #println("----- mx_input(choose_action: machine) -----")
    #println(mx_input)
    # ----------------
    # predict 'y'
    y_pred  = model(mx_input')
    Qvalue  = maximum(y_pred)
    a_order = argmax(y_pred)[2]
    action  = DIRS[a_order]
    # ----------------
    # ゲーム盤を動かす
    (grid, flg_moved, cart, two_or_four, reward) = simulate_move!(grid, action)

    return action, acType, round(Qvalue; digits=5), grid, reward, flg_moved
end

# ----------------
# ゲームオーバーしているか？
function is_gameover!(grid) 
    # ---------------- 
    # DIRS = [:left, :up, :right, :down]
    # ---------------- 
    flg_gameover = true      # trueとは「ゲームオーバーである」という意味
    for i in 1:4
        d1 = DIRS[i]
        (grid, ok, cart, two_or_four, reward) = simulate_move!(grid, d1)
        if ok
            flg_gameover = false     # falseとは「ゲームオーバーではない」という意味
            return flg_gameover
        end
    end
    return flg_gameover
end

```

D先生： “ちょっと待ったぁ～！！異議あり！！”

QEU:FOUNDER ： “どうしました？”

D先生： “どうしていきなり、ディープラーニングに入力される関数が「12+1」になったんですか？いままでは「8+1」でしょう？”

QEU:FOUNDER ： “そう来ると思った（笑）。まずは、このコードを見てください。”

```julia
        # ------
        # 配列への追加
        push!(arr_delta, delta)
        push!(arr_beta, beta)
        push!(arr_distance, mDistance)
        println("delta: $delta, beta: $beta, mDistance: $mDistance")
```

D先生： “感度にはarr_beta（newRTのもの）、arr_delta（SOARTのもの）を、そして補正後の距離(distance)を使用しているんですね。そうすればインプット件数は4x3=12になります。・・・わかりました。・・・で？どうして「こうした」んですか？”

QEU:FOUNDER ： “これは来る**「外観検査自動機」**への布石ですよ。タグチメソッドの感度とSN比の関係って、「変動分解法の産物」ですね。”

D先生： “そうです。それがどうしました？”

QEU:FOUNDER ： “RTメトリックスの場合には変動分解されているから回転(beta)と距離(distance)は独立の変数になります。それでは質問です。今回、全く別に明るさ(delta)を計算して追加しました。これは、betaやdistanceに対して「独立しています」か？”

D先生： “そりゃあ、beta-delta-distanceは**互いに独立している**でしょうね。”

QEU:FOUNDER ： “じゃあ、その3種のデータを組み合わせて、それらをディープラーニングに入力して予測をして何が悪いんですか？”

D先生： “問題ないです。ただし、パターン認識の能力が上がればの話ですが・・・。”

QEU:FOUNDER ： “だから、今、その検証をやっています・・・（笑）。それではプログラムを続けてみてみましょう。”

```julia
# =================================================
# DQN_Solver functions
# =================================================
# crate instance for input
global model     = Chain(Dense(in_dim, 512, relu), Dense(512, 512, relu), Dense(512, out_dim)) # Use Chain if you want to stack layers
#global model     = Chain(Dense(in_dim, 512, relu), Dense(512, 512, relu), Dense(512, 256, relu), Dense(256, out_dim)) # Use Chain if you want to stack layers
#global model     = Chain(Dense(in_dim, 512, relu), Dense(512, out_dim)) # Use Chain if you want to stack layers
# パラメタ
global ps_model  = params(model)
# -----
# 損失関数
#global loss(x, y) = mse(model(x'), Matrix(y'))            # 二乗平均誤差
global loss(x, y)  = huber_loss(model(x'), Matrix(y'))    # フーバー誤差
# -----
# オプティマイザ
global opt         = ADAM()

# ----------------
# メモリを蓄積する
function remember_memory(iCnt_play, iTurn, memory, state_cur, action, reward, state_next, done)
    push!(memory, (iCnt_play, iTurn, state_cur, action, reward, state_next, done))
    len_memory = length(memory)
    if len_memory > MEMORY_CAPACITY
        memory = memory[2:end]
    end
    return memory
end

# ----------------
# (REPLAY EXPERIENCE)学習する
function replay_experience(iCnt_play, memory, batch_size)
    # --------------------------------------------------
    len_memory = length(memory)
    batch_size = min(batch_size, len_memory)
    minibatch  = rand(memory, batch_size)
    X, Y   = zeros(batch_size,in_dim), zeros(batch_size)
    # --------------------------------------------------
    for ibat in 1:batch_size
        iPlay, iTurn, state_cur, str_action, reward, state_next, done = minibatch[ibat]
        # CURRENT: < X >-MATRIX
        # ---------------- 
        # DIRS = [:left, :up, :right, :down]
        # ----------------
        if str_action == :left
            action = 1
        elseif str_action == :up
            action = 2
        elseif str_action == :right
            action = 3
        else 
            action = 4
        end
        # ---------------- 
        a_onehot = onehotbatch(action, 1:4)
        state_action_cur = vcat(state_cur, a_onehot)
        #println("state_action_cur: $state_action_cur")
        X[ibat,:]  = state_action_cur'
        # ----------------
        # NEXT: < Y >-ARRAY
        if done == true
            Y[ibat] = reward
        else
            mx_input = zeros(4,in_dim)  # 状態(state)と行動(action)マトリックス
            for imov in 1:4
                # [array]-action を結合します。
                a_onehot = onehotbatch(imov, 1:4)
                temp_s_a = vcat(state_next, a_onehot)
                #println("temp_s_a-choose action: ", temp_s_a)
                mx_input[imov,:] = temp_s_a'      # 状態(S)行動(A)マトリックス
            end
            #println("--- mx_input(replay_experience) ---")
            #println(mx_input)
            # ----------------
            # predict 'y'
            y_pred   = model(mx_input')
            target_f = reward + γ * maximum(y_pred)
            #println("target_f: $target_f")
            Y[ibat]  = target_f
        end
    end
    # ----------------
    # TRAINING
    # calculate loss -> gradient
    gs = gradient(() -> loss(X, Y), ps_model)
    # update weights
    Flux.Optimise.update!(opt, ps_model, gs)
    # ----------------
    val_loss = round(loss(X, Y); digits=5)
    #println("iPlay: $iCnt_play, loss: $val_loss")
    return val_loss
end

```

D先生： “今回の関数のラインアップには、構成が512x512x256となっているモノもあって、今までよりも複雑なものに対応できるようになっています。”

QEU:FOUNDER ： “今回の**「基底条件」**を見直してみただけです。今回計算したのはベースライン用で関数構造は512x512でます。”

D先生： “それでは、解説をつづけましょう。”

```julia
# =================================================
# EPISODE CONTROL functions
# =================================================
# エピソードを運用する
function get_episode!(iCnt_play, memory, rep_bonuscore)

    # リセット：基本変数を初期化する
    (grid, arr_acType, arr_action, arr_Qvalue, arr_reward, mx_states) = reset!()
    # -----
    # SOARTテクノメトリックスを計算する[RESET-CURRENT]
    # -----
    state_cur = calc_techmetrics(grid, 0)
    # -----
    flg_moved, flg_gameover, iTurn, total_score = true, false, 1, 0
    non_valid_count, valid_count = 0, 0
    while flg_gameover == false        # ゲームオーバーでループから外れる
        # 初期化
        Qvalue, acType  = -0.0001, "random"        
        # コーナーボーナス設定
        flg_corner, bonus_corners = false, 0
        arr_corners = [grid[1,1], grid[1,4], grid[4,1], grid[4,4]]
        if iTurn < 70
            if maximum(arr_corners) > 3 && maximum(arr_corners) - minimum(arr_corners) > 2
                flg_corner    = true
                bonus_corners = (maximum(arr_corners) - minimum(arr_corners))*2
            end
        else
            if maximum(arr_corners) > 4 && maximum(arr_corners) - minimum(arr_corners) > 3
                flg_corner    = true
                bonus_corners = (maximum(arr_corners) - minimum(arr_corners))*2
            end
        end
        # ----------------
        # 命令を選択し、ゲームを動かす
        if ε < rand() && flg_moved == true
            # DQNでコマを動かす
            (action, acType, Qvalue, grid, reward, flg_moved) = step_machine!(grid, state_cur, iCnt_play, Qvalue)
            # ゲーム盤が動かなければ報酬はマイナスになる
            #println("flg_moved: ", flg_moved)
        else
            # ランダムでコマを動かす
            (action, acType, Qvalue, grid, reward, flg_moved) = step_random!(grid)
        end
        #if iTurn%10 == 0
        #    println("Play: $iCnt_play, Turn: $iTurn, grid: $grid, acType: $acType, action: $action, reward: $reward, flg_moved: $flg_moved")
        #end
        # ----------------
        # SOARTテクノメトリックスを計算する[RUN-NEXT]
        # ----------------
        state_next = calc_techmetrics(grid, iTurn)
        #println("UPDATED - state_next: $state_next")
        # ----------------
         # 記録用リストを追記する
        push!(arr_acType, acType)
        push!(arr_action, action)
        push!(arr_Qvalue, Qvalue)
        push!(arr_reward, reward)
        if iTurn == 1
            mx_states = state_cur
        else
            mx_states = cat(mx_states, state_cur, dims=2)
        end
        # ----------------
        state_cur   = state_next
        total_score = total_score + reward
        # ----------------
        # 報酬修正(1) : FEED_BACK
        if flg_corner == true
            reward = round(reward + bonus_corners; digits=3)
        end
        #println("報酬量修正後 : ", reward)
        # ----------------
        # カウント && ゲームオーバーしているか？ 
        if flg_moved == false
            non_valid_count = non_valid_count + 1
            flg_gameover = is_gameover!(grid)
        else
            valid_count = valid_count + 1
        end 
        # ----------------
        # ゲームオーバーした場合、ブレイクする
        if flg_gameover == true
            #println("BREAK!! - Play: $iCnt_play, Turn: $iTurn, flg_gameover: $flg_gameover")
            # 報酬修正(2)  ボーナス
            if total_score > rep_bonuscore*3.0 && iCnt_play > 20
                reward = 1.0*valid_count
            elseif total_score > rep_bonuscore && iCnt_play > 20
                reward = 0.5*valid_count - non_valid_count
            else
                reward = -0.3*non_valid_count
            end
        else
            iTurn = iTurn + 1
        end
        # ----------------
        # Experience Replay配列に保管する
        memory = remember_memory(iCnt_play, iTurn, memory, state_cur, action, reward, state_next, flg_gameover)
    end
    # ---------------------------
    # ゲーム代表値のリスト
    rep_predQV = quantile(arr_Qvalue, [0.25, 0.5, 0.75, 1])
    
    return (iTurn, round(total_score; digits=2), memory, rep_predQV, valid_count, non_valid_count, mx_states')
end


# =================================================
# MAIN ROUTINE
# =================================================
# 記録用パラメタ類
memory        = []  # メモリのリスト
# ---------------------------
# 記録用パラメタ類
arr_iplay     = []  # count game play    プレイ番号
arr_maxturn   = []  # turn game play    ターン数
arr_maxscore  = []  # rl_score game play    報酬の総和
arr_loss      = []  # DQN-Experience Replay学習
arr_εs       = []  # ε-Greedy
# ---------------------------
# Q値の分析用
arr_maxQV     = []  # QV値の最大値
arr_q25QV     = []  # QV値の4分の1値
arr_q50QV     = []  # QV値の4分の2値
arr_q75QV     = []  # QV値の4分の3値
arr_nvalid    = []  # 無効命令率
# ---------------------------
# 初期化
val_loss      = 0.0
iCnt_play     = 1
rep_bonuscore = 500
# ---------------------------
# エピソードを実行する
while true   
    # ゲームを動かす
    (maxturn, maxscore, memory, rep_predQV, valid_count, non_valid_count, mx_states) = get_episode!(iCnt_play, memory, rep_bonuscore)
    # 無効命令率の計算
    ratio_nvalid = round(non_valid_count/maxturn; digits=3)
    # 簡単な出力
    #println("maxturn: $maxturn, maxscore: $maxscore, ratio_nvalid: $ratio_nvalid")

    # ----------
    # DQN-Experience Replay学習(重み付き)
    if valid_count >= 150
        for iLearn in 1:50
            val_loss = replay_experience(iCnt_play, memory, BATCH_SIZE)
        end
    elseif valid_count > 100 && valid_count < 150
        for iLearn in 1:20
            val_loss = replay_experience(iCnt_play, memory, BATCH_SIZE)
        #    println("Learn_turn:$iLearn, loss:$val_loss")
        end
    else
        val_loss = replay_experience(iCnt_play, memory, BATCH_SIZE)
    end

    # ----------
    # 学習結果の出力
    if iCnt_play%40 == 0
        println("iPlay: $iCnt_play, maxturn: $maxturn, maxscore: $maxscore, ε:$ε, loss:$val_loss, bonuscore:$rep_bonuscore, ratio_nvalid:$ratio_nvalid")
    end
    #println(mx_states)
    #println(size(mx_states))
    # ----------
    # 記録用パラメタ類(プレイベース)の追加
    push!(arr_iplay, iCnt_play)  # count game play    プレイ番号
    push!(arr_maxturn, maxturn)  # max_turn   ゲームのターン数
    push!(arr_maxscore, maxscore)  # rl_score game play    最終プレイスコア
    push!(arr_loss, val_loss)    # DQN-Experience Replay学習
    push!(arr_εs, ε)  # イプシロン
    # ----------
    # ゲーム代表値の保管
    push!(arr_maxQV, rep_predQV[4])  # QV値の最大値
    push!(arr_q75QV, rep_predQV[3])  # QV値の4分の3値
    push!(arr_q50QV, rep_predQV[2])  # QV値の4分の2値
    push!(arr_q25QV, rep_predQV[1])  # QV値の4分の1値
    push!(arr_nvalid, ratio_nvalid)  # 無効命令率
    # ---------------------------
    # ボーナス支給のしきい値を設定する
    #rep_bonuscore = quantile(arr_maxscore, [0.75])[1]
    # ----------
    # Change EPSILON
    if ε > e_min && iCnt_play < total_episodes        # total_episodes
        ε *= e_decay
        ε = round(ε; digits=5)
        iCnt_play = iCnt_play + 1
    else
        break
    end
end


# ---------
# Create Graph(2) : 移動平均による平滑化
using MarketTechnicals

# A-GROUP
pltA1 = plot(arr_εs, label="EPSILON", legend=:topright)
pltA2 = plot(arr_loss, label="loss", legend=:topleft)
        plot!(ema(arr_loss, 10, wilder = false), label="movave", legend=:topleft)
pltA3 = plot(arr_maxturn, label="maxturn", legend=:topleft)
        plot!(ema(arr_maxturn,10, wilder = false), label="movave", legend=:topleft)
pltA4 = plot(arr_maxscore, label="maxscore", legend=:topleft)
        plot!(ema(arr_maxscore,10, wilder = false), label="movave", legend=:topleft)
plot(pltA1, pltA2, pltA3, pltA4)

```

**（512x512-12i-beta）**

![imageJRL1-30-3](/2023-02-02-QEUR22_2048S10/imageJRL1-30-3.jpg)

```julia
# B-GROUP
pltB1 = plot(ema(arr_maxQV,10, wilder = false), label="max", legend=:topleft)
        plot!(ema(arr_q75QV,10, wilder = false), label="q75", legend=:topleft)
        plot!(ema(arr_q50QV,10, wilder = false), label="q50", legend=:topleft)
        plot!(ema(arr_q25QV,10, wilder = false), label="q25", legend=:topleft)
pltB2 = plot(ema(arr_nvalid,10, wilder = false), label="non_valid", legend=:topleft)
plot(pltB1, pltB2)
```

**（512x512-12i-beta）**

![imageJRL1-30-4](/2023-02-02-QEUR22_2048S10/imageJRL1-30-4.jpg)

D先生： “さすがに「最強のテクノメトリックス(SLIDERT)」ですね。でも、その割にスコアがあまり伸びないですね。ちょっと残念・・・。”

QEU:FOUNDER ： “畳み込みのように、**「フィードバック法が使えない」**ですから・・・。苦し紛れで、むりやり類似したロジックに加えたが、ぜんぜん力不足です。”

D先生： “そういえば、距離を計算するための「変動分解」において、なぜあえて回転（beta）をつかったんですか？畳み込みのときには、明るさ(delta)のメトリックスをベースに距離を計算したんですが・・・。”

QEU:FOUNDER ： “この図(↓)はゲーム内のbeta,deltaの推移を示しています。どう思いますか？”

**(betaの推移)**

![imageJRL1-30-5](/2023-02-02-QEUR22_2048S10/imageJRL1-30-5.jpg)

**(deltaの推移)**

![imageJRL1-30-6](/2023-02-02-QEUR22_2048S10/imageJRL1-30-6.jpg)

D先生： “どっちを使えば判別能力が上がるんだろうか・・・う～ん・・・。私でもやはりBetaを感度に使います。Deltaの大きさはiTurnに対して、あまりに依存しています。”

QEU:FOUNDER ： “それでは、次はD先生のお好きな普通のRT法(input:8+1)の強化学習をやっていきましょう。今回のムリヤリ**12件インプットの効果**は如何に・・・。”


## ～　まとめ　～

C部長 : “久々に、FOUNDERの好きなオッサンの登場デス！”

[![MOVIE1](http://img.youtube.com/vi/-5S4AInoT7Q/0.jpg)](http://www.youtube.com/watch?v=-5S4AInoT7Q "れいわ新選組公認豊見城市議会議員候補予定者久保田みどりと参議院大島九州男の朝の活動")

QEU:FOUNDER ： “いつ見ても、いい男だ。惚れ惚れするね・・・。**プリティさん**の名前を呼んでおり、久々の名前に感動しました。「さすがオッサンだな」って・・・。”

C部長 : “プリティさんって、あの人でしょ？れ某党のオッサンがその名前を口にするのは、かなり「微妙なセン」ですね。FOUNDERはプリティ支持？”

QEU:FOUNDER ： “そうだよ。「あの時」、おもわず激怒したからね。”

C部長 : “へえ！？”


