---
title: QEUR22_JRLINT5:　ReinforcementLearning.jlでDeepRLを・・・(その１)
date: 2023-01-06
tags: ["QEUシステム", "メトリックス", "Julia言語", "機械学習", "Flux", "ディープラーニング", "強化学習"]
excerpt: Julia言語を使った強化学習
---

## QEUR22_JRLINT5:　ReinforcementLearning.jlでDeepRLを・・・(その１)

## ～　Julia言語の「良しあし」・・・　～

QEU:FOUNDER （設定65歳） ： “この番組、**「高齢者によるイノベーション」**はまだまだ続きます。高齢者は偉いんだ、イノベーションもへっちゃらさ♪”

D先生（設定65歳） ： “えっと・・・、今回の「イノベーション」はReinforcementLearning.jlで**ディープラーニングを使う**？”

![imageJRL1-6-1](/2023-01-06-QEUR22_JRLINT5/imageJRL1-6-1.jpg)

QEU:FOUNDER ： “DRLをとにかくやりたいのであれば、このページ（↑）からお好きなテーマを選んでください。ページを開き、コードをJupyterなどに貼りつけて実行すればできますよ。”

![imageJRL1-6-2](/2023-01-06-QEUR22_JRLINT5/imageJRL1-6-2.jpg)

D先生 ： “昔やった**Cliff_Walking(↓)**をやってみたいです。あれは結構実用的だし・・・。”

![imageJRL1-6-3](/2023-01-06-QEUR22_JRLINT5/imageJRL1-6-3.jpg)

QEU:FOUNDER ： “自分でENV(環境)をつくる必要が出てきたようですね。**とても参考になる資料（↑）もある**のでやってみましょう。ドン！！”

```julia
# ----
# Cliff_Walking
# ----
using ReinforcementLearning
using Plots
using Flux
using Statistics

const NX = 4
const NY = 12
const Start = CartesianIndex(4, 1)
const Goal = CartesianIndex(4, 12)
const LRUD = [
	CartesianIndex(0, -1),  # left
	CartesianIndex(0, 1),   # right
	CartesianIndex(-1, 0),  # up
	CartesianIndex(1, 0),   # down
]
const LinearInds = LinearIndices((NX, NY))

# ------
function iscliff(p::CartesianIndex{2})
    x, y = Tuple(p)
    x == 4 && y > 1 && y < NY
end

# take a look at the wordmap
heatmap((!iscliff).(CartesianIndices((NX, NY))); yflip = true)
```

![imageJRL1-6-4](/2023-01-06-QEUR22_JRLINT5/imageJRL1-6-4.jpg)

```julia
# ------
Base.@kwdef mutable struct CliffWalkingEnv <: AbstractEnv
	position::CartesianIndex{2} = Start
end
function (env::CliffWalkingEnv)(a::Int)
	x, y = Tuple(env.position + LRUD[a])
	env.position = CartesianIndex(min(max(x, 1), NX), min(max(y, 1), NY))
end

# ------
RLBase.state(env::CliffWalkingEnv) = LinearInds[env.position]
RLBase.state_space(env::CliffWalkingEnv) = Base.OneTo(length(LinearInds))
RLBase.action_space(env::CliffWalkingEnv) = Base.OneTo(length(LRUD))
RLBase.reward(env::CliffWalkingEnv) = env.position == Goal ? 0.0 : (iscliff(env.position) ? -100.0 : -1.0)
RLBase.is_terminated(env::CliffWalkingEnv) = env.position == Goal || iscliff(env.position)
RLBase.reset!(env::CliffWalkingEnv) = env.position = Start
world = CliffWalkingEnv()
```

![imageJRL1-6-5](/2023-01-06-QEUR22_JRLINT5/imageJRL1-6-5.jpg)

D先生 ： “こんなキレイなコード・・・。FOUNDERのモノでじゃないでしょ・・・。”

![imageJRL1-6-6](/2023-01-06-QEUR22_JRLINT5/imageJRL1-6-6.jpg)

QEU:FOUNDER ： “もちろん。ワシは高齢者じゃ、文句あっか・・・。Web検索して、あるものを使う。**「車輪の再発明（reinventing the wheel）」**は良くないです。今回はworldという名前で環境をインスタンス化しましたが、そのときに「Traits(特徴)」の一覧が出力されないと使えないです。Julia言語って、厳密さを要求するから、ゼロからつくると時間がかかるよ。じゃあ、次にランダムポリシーで軽く動かしてみましょう。”

```julia
# ----
hook = TotalRewardPerEpisode()
run(RandomPolicy(action_space(world)), world, StopAfterEpisode(1_000), hook)
```

![imageJRL1-6-7](/2023-01-06-QEUR22_JRLINT5/imageJRL1-6-7.jpg)

D先生 ： “これで、ENV(環境)が完成していることが確認されました。これで、あとはエージェント(ポリシー)側をいろいろ変えればいいだけですね。”

QEU:FOUNDER ： “ちなみに、オフィシャル資料（↑）にはさらにCliffWalkingよりもシンプルな**「宝くじ(LotteryEnv)」の事例がある**ので、この事例と比較すれば構造がわかりやすいです。”

![imageJRL1-6-8](/2023-01-06-QEUR22_JRLINT5/imageJRL1-6-8.jpg)

D先生 ： “あとはディープラーニングを載せればいいだけなんだが、トラブルかなぁ・・・。”


## ～　まとめ　～

QEU:FOUNDER ： “**「例の東の横綱」の動画**をもう一度みていたんですが、こうやって一歩一歩コードを実行しながら動画を見直すと**超ハッカー様**が言っていることが見えてきます。”

[![MOVIE1](http://img.youtube.com/vi/ckIIxKM14Ow/0.jpg)](http://www.youtube.com/watch?v=ckIIxKM14Ow "ReinforcementLearning jl | Jun Tian | Julia User Group Munich - Share&Code")

C部長 : “動画拝見一発了解じゃ無理ですか？”

QEU:FOUNDER ： “「少なくとも高齢者には無理じゃ」！！でも、「オーディエンス（開発者じゃない第三者）」は好きなことを言うなとも思ったね。ReinforcementLearning.jlとPOMDPs.jlを統合すればいいんじゃないかとの提案があったよ。”

C部長 : “そんなこと、できるんですか？”

QEU:FOUNDER ： “できるかどうかは知らん・・・。でも、本人は「時間がない」って断っていましたよ。いわばNPO（非営利）なので、**タダでここまでやれただけでもすごい**ですがね。”

C部長 : “そこに、さらに(組織)統合までお願いとか・・・（笑）。”

QEU:FOUNDER ： “NPOの経営学によると、お金で制約されていないから、個人の趣向が優先し組織のコントロールが効かないんだよね。政治でいう、**リベラル勢力がまとまらないのと同じ**。すべての要求が通らないと動かないんです。”

