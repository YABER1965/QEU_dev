---
title: QEUR22_CLIFF5:　Cliff_Walkingで遊ぼう（その6：JuliaRLAGENT編-テクノメトリックス2）
date: 2023-01-18
tags: ["QEUシステム", "メトリックス", "Julia言語", "Cliff Walking", "Flux", "ディープラーニング", "強化学習"]
excerpt: Julia言語を使った強化学習
---

## QEUR22_CLIFF5:　Cliff_Walkingで遊ぼう（その6：JuliaRLAGENT編-テクノメトリックス2）

## ～　符号のチカラとは　～

D先生(設定年齢65歳)  ： “つぎは符号化です。**「テクノメトリックス(TM)付き強化学習」**のスキームの中で、符号化がどれだけの効用を持つのか・・・。この番組は**「高齢者によるイノベーション」**です。”

![imageJRL1-14-1](/2023-01-18-QEUR22_CLIFF5/imageJRL1-14-1.jpg)

QEU:FOUNDER(設定年齢65歳) ： “（符号の）効用ねえ・・・。”

D先生  ： “人生って、そういうモンです。ホラ・・・。”

![imageJRL1-14-2](/2023-01-18-QEUR22_CLIFF5/imageJRL1-14-2.jpg)

QEU:FOUNDER ： “参りました・・・。”

D先生 ： “でも、どうやって符号をつけるんですか？”

![imageJRL1-14-3](/2023-01-18-QEUR22_CLIFF5/imageJRL1-14-3.jpg)

QEU:FOUNDER ： “今回の場合には、**プラスマイナスの意味は幾何的に明らか**だから2つの基準点を設けて、それを比較だけでいいでしょうね。基準経路の外側領域をプラスとし、内側領域をマイナスを定義します。”

D先生 ： “なるほど・・・。でも、このような計算ロジックってかなり処理時間がかかりそうですね。”

QEU:FOUNDER ： “だから、テクノメトリックス付き強化学習にはJulia言語を使うことが有利なんです。例えば、minRT法はmin関数を使っていますが、min関数というのは使っている言語によるスピード差が出やすいんです。”

![imageJRL1-14-4](/2023-01-18-QEUR22_CLIFF5/imageJRL1-14-4.jpg)

D先生 ： “min/max関数が機能するにはデータのソート(sort)が前提ですからね。min関数を使わない方法はないかなぁ・・・。”

QEU:FOUNDER ： “それはご自分で考えてください（笑）。それでは、プログラムと実行結果を紹介します。ドン・・・。”

```julia
# ---
# TRIAL【5】_Cliff_Walking_DQN_TechnoMetricsRT_(P/M)SIGN
# ---
using ReinforcementLearning
using Flux
using Flux: mse, onehotbatch
using Random
using Statistics
using Plots
using LinearAlgebra

# ---------------------------
# グローバル宣言
global num_state
global num_actions 
global in_dim 
global out_dim
global base_mx
# minRT距離の基準経路
base_mx = Float32[4.0 1.0; 3.0 1.0; 2.0 1.0; 2.0 2.0; 2.0 3.0
        ; 2.0 4.0; 2.0 5.0; 2.0 6.0; 2.0 7.0; 2.0 8.0
        ; 2.0 9.0; 2.0 10.0; 2.0 11.0; 2.0 12.0; 3.0 12.0; 4.0 12.0]
# minRT距離の符号判定用
base_plus  = [1,6.5]
base_minus = [3,6.5]

# ---------------------------
# Defining all the required parameters
global ε
global γ 
global e_decay  
global e_min   
global learning_rate
global total_episodes 

# ---------------------------
# Cliff_Walking_Environment
# ---------------------------
NX, NY = 4, 12
PStart = [4, 1]
PGoal  = [4, 12]
LRUD = [
    [0, -1],  # 1->left
    [0, 1],   # 2->right
    [-1, 0],  # 3->up
    [1, 0],   # 4->down
]

# ------
function iscliff(p)
    x, y = p[1], p[2]
    if x == 4 && y > 1 && y < NY
        return true
    else
        return false
    end
end

# take a look at the wordmap
aaaa = zeros(Float32, NX, NY)
for i in 1:NX
    for j in 1:NY
        position  = [i,j]
        aaaa[i,j] = iscliff(position)
    end
end

# ----
# HEATMAPを描く
heatmap(aaaa;yflip=true)

# ------
# 環境用のユーティリティ関数群
function isoutlayout(p)
    x, y = p[1], p[2]
    if x < 1 || x > NX || y < 1 || y > NY
        return true
    else
        return false
    end
end

function calc_onehot(p)
    onehot = zeros(Float32, NX*NY)
    x, y = p[1], p[2]
    number::Int64  = x + (y-1)*NX
    onehot[number] = 1.0
    return onehot
end

function calc_techMX(p)
    # minRTメトリックスの計算
    arr_techMX  = []
    for i in 1:16
        temp_techMX = sqrt(mse(p,base_mx[i,:]))
        push!(arr_techMX, temp_techMX)
    end
	techMX = round(minimum(arr_techMX);digits=4)
	# ---
	# 符号化
	Dis_plus  = mse(p, base_plus)
	Dis_minus = mse(p, base_minus)
	if Dis_minus < Dis_plus
		techMX = -1 * techMX
    end
    return techMX
end

function calc_techRTABS(p)
    x, y = p[1], p[2]
    # minRTメトリックスの計算
    techMX = calc_techMX(p)
    # START距離
    DStart = round(sqrt(mse(p, PStart));digits=4)
    # END距離
    DGoal  = round(sqrt(mse(p, PGoal));digits=4)
    # ベクトル化
    Vec_TM = [x, y, techMX, DStart, DGoal]
    return Vec_TM
end

function calc_number(p)
    x, y = p[1], p[2]
    number::Int64  = x + (y-1)*NX
    return number
end

function rev_onehot(p)
    arrseq = 1:NX*NY
    number = dot(arrseq, p)
    return number
end

function calc_rewards(p)
    reward::Float32 = 0.0
    if env.flg_layout == true
        reward = -1.0
    elseif env.flg_cliff == true
        reward = -50.0
    elseif p[1] == PGoal[1] && p[2] == PGoal[2]
        reward = 2000
    else
        reward = -0.2
    end
    return reward
end

function calc_isterm(p,cnt)
    if p[1] == PGoal[1] && p[2] == PGoal[2]
        return true
    elseif cnt > 50
        return true
    else
        return false
    end
end

# ------
# 環境の構築
Base.@kwdef mutable struct CliffWalkingEnv <: AbstractEnv
    position = PStart
    count::Int = 0
    flg_layout = false
    flg_cliff  = false
end

function (env::CliffWalkingEnv)(a::Int)
    env.flg_layout = false
    env.flg_cliff  = false
    temp = LRUD[a]
    x, y = env.position[1] + temp[1], env.position[2] + temp[2]
    if isoutlayout([x,y]) == true
        env.position = env.position
        env.flg_layout = true
    elseif iscliff([x,y]) == true
        env.count     = env.count + 1
        env.position  = PStart
        env.flg_cliff = true
    else
        env.position = [x, y]
    end
end

function env_reset()
    env.position = PStart
    env.count = 0
    env.flg_layout = false
    env.flg_cliff  = false
end

# ------
RLBase.state(env::CliffWalkingEnv) = calc_techRTABS(env.position)
RLBase.state_space(env::CliffWalkingEnv) = Base.OneTo(length(calc_techRTABS(env.position)))
RLBase.action_space(env::CliffWalkingEnv) = Base.OneTo(length(LRUD))
RLBase.reward(env::CliffWalkingEnv) = calc_rewards(env.position)
RLBase.is_terminated(env::CliffWalkingEnv) = calc_isterm(env.position, env.count)
RLBase.reset!(env::CliffWalkingEnv) = env_reset()
env = CliffWalkingEnv()

# ----------------
# 命令を選択する
function choose_action(state_cur, iCnt_play)

    # ----------------------------------------
    # 移動命令
    # LRUD = [
    #    [0.0, -1.0],  # 1->left
    #    [0.0, 1.0],   # 2->right
    #    [-1.0, 0.0],  # 3->up
    #    [1.0, 0.0],   # 4->down
    # ----------------
    Qvalue = -0.0001
    #println("BIGIN:choose_action")
    # ----------------
    # 最適命令の選択
    if ε < rand()
        # DQNによる選択
        acType   = "machine"
        mx_input = zeros(4,in_dim)
        for action in 1:4
            # [array]-action を生成します。
            a_onehot = onehotbatch(action, 1:4)
            temp_s_a = cat(state_cur, a_onehot , dims=1)'
            #println("temp_s_a-choose action: ", temp_s_a)
            mx_input[action,:] = temp_s_a     # 状態(S)行動(A)マトリックス
        end
        #println("----- mx_input(choose_action) -----")
        #println(mx_input)
        # ----------------------------------------
        # predict 'y'
        y_pred  = model(mx_input')
        Qvalue  = maximum(y_pred)
        a_order = argmax(y_pred)[2]
    else
        # 乱数による選択
        acType = "random"
        a_order = rand(1:4)
    end

    return a_order, acType, round(Qvalue; digits=5)
end

# -----
#    Args:
#        ε: The degree of exploration
#        γ: The discount factor
#        num_state: The number of states
#        num_actions: The number of actions
#        action_space: To call the random action
# -----
num_state     = 5
num_actions   = 4
in_dim  = num_state + num_actions
out_dim = 1

# =================================================
# Calculation class(2) : DQN_Solver:
# =================================================
# crate instance for input
global model       = Chain(Dense(in_dim, 256, relu), Dense(256, out_dim)) # Use Chain if you want to stack layers
# パラメタ
global ps_model    = params(model)
# -----
# 損失関数
global loss(x, y)  = mse(model(x'), Matrix(y'))
# -----
# オプティマイザ
global opt         = ADAM()

# ----------------
# メモリを蓄積する
function remember_memory(memory, state_cur, action, reward, state_next, done)
    push!(memory, (state_cur, action, reward, state_next, done))
    if length(memory) > MEMORY_CAPACITY
        memory = memory[2:end]
    end
    return memory
end

# ----------------
# (REPLAY EXPERIENCE)学習する
function replay_experience(iCnt_play, memory, batch_size)
    #println("BIGIN:replay_experience")
    # --------------------------------------------------
    batch_size = min(batch_size, length(memory))
    minibatch  = rand(memory, batch_size)
    X, Y   = zeros(batch_size,in_dim), zeros(batch_size)
    # --------------------------------------------------
    for ibat in 1:batch_size
        state_cur, action, reward_cur, state_next, done = minibatch[ibat]
        # CURRENT: < X >-MATRIX
        a_onehot = onehotbatch(action, 1:4)
        state_action_cur = cat(state_cur, a_onehot, dims=1)'
        #println("state_action_cur: $state_action_cur")
        X[ibat,:]  = state_action_cur
        # ----------------
        # NEXT: < Y >-ARRAY
        if done == true
            Y[ibat] = reward_cur
        else
            mx_input = zeros(4,in_dim)  # 状態(state)と行動(action)マトリックス
            for imov in 1:4
                # [array]-action を結合します。
                a_onehot = onehotbatch(imov, 1:4)
                temp_s_a = cat(state_next, a_onehot , dims=1)'
                #print("temp_s_a-choose action: ", temp_s_a)
                mx_input[imov,:] = temp_s_a      # 状態(S)行動(A)マトリックス
            end
            #println("--- mx_input(replay_experience) ---")
            #println(mx_input)
            # --------------------------------------------------
            # predict 'y'
            y_pred   = model(mx_input')
            target_f = reward_cur + γ * maximum(y_pred)
            #println("target_f: $target_f")
            Y[ibat]  = target_f
        end
    end
    # --------------------------------------------------
    # TRAINING
    # calculate loss -> gradient
    gs = gradient(() -> loss(X, Y), ps_model)
    # update weights
    Flux.Optimise.update!(opt, ps_model, gs)
    # -----
    val_loss = round(loss(X, Y); digits=5)
    #println("iPlay: $iCnt_play, loss: $val_loss")

    return val_loss

end

# =================================================
# ENVIRONMENT class
# =================================================
# リセット
function env_reset!(env)
    reset!(env)
    state_cur = state(env)
    position_cur = ( env.position[1], env.position[2] )
    #println("state_cur: $state_cur, position_cur: $position_cur")
    return state_cur, position_cur
end

# 運用中
function env_step!(env, action)
    env(action)
    state_next = state(env)
    position_next = ( env.position[1], env.position[2] )
    reward_cur = reward(env)
    done       = is_terminated(env)
    #println("state_next: $state_next, position_next: $position_next, reward: $reward_cur, done: $done")
    return state_next, position_next, reward_cur, done
end

# =================================================
# Calculation class
# =================================================
# エピソードを運用する
function get_episode(iCnt_play, memory)

    # ---------------------------
    # 機械学習用のパラメタ群
    Qvalue, reward_cur, done = -0.0001, 0, false
    maxturn, maxscore = 0, 0
    # ---------------------------
    # 記録用パラメタ類(ターンベース)
    arr_iturn  = []  # ターン・カウンタリスト
    orders_row = []  # 指示リスト(row)
    orders_col = []  # 指示リスト(col)
    arr_orders = []  # 指示リスト(命令)
    arr_acType = []  # 指示のタイプ
    arr_scores = []  # ゲームスコアリスト
    arr_dones  = []  # ゲームオーバーフラグ
    arr_predQV = []  # Q値のリスト
	arr_RTdist = []  # RTテクノメトリックスのリスト
    # ---------------------------
    # ゲームをリセットする
    state_cur, position_cur = env_reset!(env)
    # ---------------------------
    # ゲームをプレイする
    iCnt_turn = 0
    while true
        # 命令(a_order)を作成する
        a_order, acType, Qvalue = choose_action(state_cur, iCnt_play)
        # ---------------------------
        # ゲームをもう一歩進める
        state_next, position_next, reward_cur, done = env_step!(env, a_order)
		# ベクトル化：  Vec_TM = [x, y, techMX, DStart, DGoal]
		
        #if iCnt_play % 5 < 0.01 && iCnt_turn % 20 < 0.01 || done == true
        #    println("iPlay: $iCnt_play, iTurn: $iCnt_turn, position_cur: $position_cur, action: $a_order, done: $done, reward: $reward_cur")
        #end
        # ---------------------------
        # 記録用リストを追記する
        push!(arr_iturn, iCnt_turn)  # ターン・カウンタリスト
        push!(orders_row, position_cur[1])  # 指示リスト(ROW)
        push!(orders_col, position_cur[2])  # 指示リスト(COL)
        push!(arr_orders, a_order)  # 指示リスト(命令)
        push!(arr_acType, acType)  # 指示のタイプ
        push!(arr_scores, reward_cur)  # ゲームスコアリスト
        push!(arr_dones, done)  # ゲームオーバーフラグ
        push!(arr_predQV, Qvalue)  # Q値のリスト
		push!(arr_RTdist, state_cur[3])  # RTテクノメトリックスのリスト
        # ----------------
        # Experience Replay配列に保管する
        memory = remember_memory(memory, state_cur, a_order, reward_cur, state_next, done)
        # ----------------
        # 実行継続するか分岐
        if done == true
            # Game over
            break  # (game over rip)
        else
            # count of game turn
            position_cur  = position_next
            state_cur     = state_next
            iCnt_turn     = iCnt_turn + 1
        end
    end
    # ---------------------------
    # ゲーム代表値のリスト
	rep_predQV = quantile(arr_predQV, [0.25, 0.75, 1])
	rep_RTdist = sum(arr_RTdist)
    # ---------------------------
    # プレイデータの引継ぎ
    maxturn, maxscore = iCnt_turn, sum(arr_scores)

    return maxturn, round(maxscore; digits=2), memory, rep_predQV, rep_RTdist

end

# =================================================
# MAIN ROUTINE
# =================================================
# Defining all the required parameters
ε     = 0.80
γ     = 0.95
e_decay       = 0.9995
e_min         = 0.01
learning_rate  = 0.005
total_episodes = 5000

# ---------------------------
# ハイパーパラメタ
memory         = []
BATCH_SIZE     = 128  # サンプルサイズ
MEMORY_CAPACITY = 128 * 8  # メモリ容量

# ---------------------------
# 記録用パラメタ類(プレイベース)
arr_iplay     = []  # count game play    プレイ番号
arr_maxturn   = []  # turn game play    ターン数
arr_maxscore  = []  # rl_score game play    報酬の総和
arr_loss      = []  # DQN-Experience Replay学習
arr_εs        = []  # ε-Greedy
# ---------------------------
# Q値の分析用(プレイベース)
arr_maxQV     = []  # QV値の最大値
arr_q25QV     = []  # QV値の4分の1値
arr_q75QV     = []  # QV値の4分の3値
arr_sumRT	  = []  # minRT距離の合計
# ---------------------------
# エピソードを実行する
for iCnt_play in 1:total_episodes   # total_episodes
    # ゲームする
    maxturn, maxscore, memory, rep_predQV, rep_RTdist = get_episode(iCnt_play, memory)
    # ----------
    # DQN-Experience Replay学習
    val_loss = replay_experience(iCnt_play, memory, BATCH_SIZE)
    # 結果の出力
    if iCnt_play % 20 < 0.01
        println("iCnt_play: $iCnt_play, maxturn:$maxturn, maxscore:$maxscore, ε:$ε, loss:$val_loss")
    end
    # ----------
    # 記録用パラメタ類(プレイベース)の追加
    push!(arr_iplay, iCnt_play)  # count game play    プレイ番号
    push!(arr_maxturn, maxturn)  # max_turn   ゲームのターン数
    push!(arr_maxscore, maxscore)  # rl_score game play    最終プレイスコア
    push!(arr_loss, val_loss)    # DQN-Experience Replay学習
    push!(arr_εs, ε)  # イプシロン
    # ----------
    # ゲーム代表値の保管
    push!(arr_maxQV, rep_predQV[3])  # QV値の最大値
    push!(arr_q75QV, rep_predQV[2])  # QV値の4分の3値
    push!(arr_q25QV, rep_predQV[1])  # QV値の4分の1値
	push!(arr_sumRT, rep_RTdist)  # minRT距離の合計
    # ----------
    # Change EPSILON
    if ε > e_min
        ε *= e_decay
        ε = round(ε; digits=5)
    end
end

# ----------
# Create Graph
#using Plots

# A-GROUP
pltA1 = plot(arr_εs, label="EPSILON")
pltA2 = plot(arr_loss, label="loss")
pltA3 = plot(arr_maxturn, label="maxturn")
pltA4 = plot(arr_maxscore, label="maxscore")
plot(pltA1, pltA2, pltA3, pltA4)
```

**(今回：符号化あり)**

![imageJRL1-14-5](/2023-01-18-QEUR22_CLIFF5/imageJRL1-14-5.jpg)

**(前回：符号化なし)**

![imageJRL1-14-6](/2023-01-18-QEUR22_CLIFF5/imageJRL1-14-6.jpg)

D先生 ： “やはり符号のパワーはすごいですね。MAXSCOREのグラフの変化に驚きました。一気に良くなりますね。”

QEU:FOUNDER ： “機械が「テクノメトリックスのプラス値がよいか」、それとも「マイナス値の場合が有利か」を調査し、かならずマイナス値の場合がゲームに良い結果が出ると判断したんだろうね。”

```julia
# B-GROUP
pltB1 = plot(arr_maxQV, label="max")
		plot!(arr_q75QV, label="q75")
		plot!(arr_q25QV, label="q25")
pltB2 = plot(arr_sumRT, label="sumRT")
plot(pltB1, pltB2)

```

**(今回：符号化あり)**

![imageJRL1-14-7](/2023-01-18-QEUR22_CLIFF5/imageJRL1-14-7.jpg)

**(前回：符号化なし)**

![imageJRL1-14-8](/2023-01-18-QEUR22_CLIFF5/imageJRL1-14-8.jpg)

D先生 ： “やっぱり、Q値グラフでも、もっと顕著な差異が出てきています。”

QEU:FOUNDER ： “今回の符号化テストは成功しましたね。客観的に確認できました。”

D先生 ： “次回のさらなるトライアルは、あるんですか？”

![imageJRL1-14-9](/2023-01-18-QEUR22_CLIFF5/imageJRL1-14-9.jpg)

QEU:FOUNDER ： “次はいよいよ「フィードバック制御」に行きます！！これこそが、「テクノメトリックス(TM)付き強化学習」の真骨頂です。”



## ～　まとめ　～

C部長 : “なんか、この大先生は悲観的なことを言っていますね。皆さん、経済のことを話すとこうなっちゃうのかね・・・。”

[![MOVIE1](http://img.youtube.com/vi/-rOD4Js1Qbc/0.jpg)](http://www.youtube.com/watch?v=-rOD4Js1Qbc "「新しい戦前」が来る~やってきた破局の先【金子勝の言いたい放題】20230105")

QEU:FOUNDER ： “経済をよくするには、やはり新陳代謝が必要なんじゃないですか？古いもの（陳）は多いが、新しいものは少ない。”

![imageJRL1-14-10](/2023-01-18-QEUR22_CLIFF5/imageJRL1-14-10.jpg)

QEU:FOUNDER ： “**ユニコーン企業**というんだっけ・・・。”

C部長 : “そうそう・・・。われらがスーパー大先生が海外に見学に行くんでしたっけ。”

![imageJRL1-14-11](/2023-01-18-QEUR22_CLIFF5/imageJRL1-14-11.jpg)

QEU:FOUNDER ： “いや、人を海外に派遣するらしいよ。このニュース（↑）によると・・・。でも、変だよね。なぜ若手起業家なんだろう。あと、行先はA国じゃないとダメなんでしょうかね。”

C部長 : “A国じゃ、だめですか？”

![imageJRL1-14-12](/2023-01-18-QEUR22_CLIFF5/imageJRL1-14-12.jpg)

QEU:FOUNDER ： “ユニコーン企業輩出国としては、**C国もかなりすごい**んだけど・・・”

C部長 : “ゲッ！J国代表は「メル〇リ」だけか・・・。こりゃ**インドネシアにも負けてます**わ・・・（笑）。ただし、2017年データだけどね・・・。”

QEU:FOUNDER ： “やっぱり、C国の経営者に来ていただくのが最も良いですね。そのためには、金利上昇こそが最大の景気対策！！”

