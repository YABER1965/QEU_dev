---
title: QEUR22_CLIFF0:　Cliff_Walkingで遊ぼう（その１）
date: 2023-01-12
tags: ["QEUシステム", "メトリックス", "Python言語", "Cliff Walking", "Pytorch", "ディープラーニング", "強化学習"]
excerpt: Julia言語を使った強化学習
---

## QEUR22_CLIFF0:　Cliff_Walkingで遊ぼう（その１）

## ～　再スタートへ！！　～

### ・・・　前回のつづきです　・・・

QEU:FOUNDER(設定年齢65歳) ： “これはゲームの報酬の履歴を見ているのであって、ディープラーニング関数の損失量推移をみているわけではないです。それでも、小生もちょっとヘンだとは思います。”

![imageJRL1-9-1](/2023-01-12-QEUR22_CLIFF0/imageJRL1-9-1.jpg)

D先生(設定年齢65歳)  ： “でも、我々が前回にCliff_Walkingで行ったやり方は正しいことは分かりましたね。もちろん、バグの有無は分からないですが・・・。次は、どうしますか？”

QEU:FOUNDER ： “ちょっと、考え中・・・。この番組は、**「高齢者によるイノベーション」**です。よし！最初からやってみるか！！”

D先生 ： “えっ！？オッサン（FOUNDER）の言っている意味が分かりません。”

QEU:FOUNDER ： “いきなり、Pythonプログラムを晒すよ！ドン！！実行結果も追加しています。”

```python
# ----------------
# CLIFF WALKING SIMULATIONの強化学習システム
# step1 : 簡単な強化学習モデルです
# step1 : dqn_cw_pytorch(handmade_env).py
# ----------------
# 数値計算のﾗｲﾌﾞﾗﾘ読み込み
import math
import numpy as np
import copy, random, time
import pandas as pd
from collections import deque, Counter
from scipy.special import softmax
from IPython.display import clear_output
# ----------------
import torch
import torch.nn.functional
import torch.utils.data
# ----------------
import matplotlib.pyplot as plt
# %matplotlib inline
# =================================================
# difinition of Environment function(Cliff_Walking)
# =================================================
NX, NY = 4, 12
PStart = [3.0, 0.0]
PGoal  = [3.0, 11.0]
LRUD = [
	[0.0, -1.0],  # 0->left
	[0.0, 1.0],   # 1->right
	[-1.0, 0.0],  # 2->up
	[1.0, 0.0],   # 3->down
]

# ------
# Cliffに落ちたかの判定
def iscliff(p):
    x, y = p[0], p[1]
    if x == 3 and (y > 0 and y < NY-1):
        return True
    else:
        return False

# ------
# ゲームゾーンから外れたかの判定
def isoutlayout(p):
    x, y = p[0], p[1]
    if x < 0 or x > 3 or y < 0 or y > 11:
        return True
    else:
        return False

# ------
# test_no1 -> It works!
p = [1.0, 1.0]
print("iscliff: ",iscliff(p))

# ------
# ENVIRONMENT：リセット
def cliff_reset():
    done, reward = False, 0    # initialize
    position = PStart
    Sx, Sy   = position[0], position[1]
    # ---------------------------
    # スタートとゴールからの距離を計算する
    EuDist_S = round(np.sqrt((Sx - PStart[0]) ** 2 + (Sy - PStart[1]) ** 2),4)
    EuDist_G = round(np.sqrt((Sx - PGoal[0]) ** 2 + (Sy - PGoal[1]) ** 2),4)
    EuDist_T = EuDist_S + EuDist_G
    arr_dists = [EuDist_S, EuDist_G, EuDist_T]
    # ---------------------------
    # Stateを計算する
    state = np.array([Sx, Sy, EuDist_S, EuDist_G])
    
    return state, position, reward, done, arr_dists

# ------
# test_no2 -> It works!
state, position, reward, done, arr_dists = cliff_reset()
print("position:{}, position:{}, reward:{}, done:{}, arr_dists:{}".format(state, position, reward, done, arr_dists))

# ------
# ENVIRONMENT：コマの動きの処理
def cliff_env(position, action):
    done, reward = False, 0    # initialize
    Sx, Sy = position[0], position[1]
    Ax, Ay = LRUD[action][0], LRUD[action][1]
    # ---
    Sx_next = Sx + Ax
    Sy_next = Sy + Ay
    position_next = [Sx_next, Sy_next]
    # ---
    if position_next[0] == PGoal[0] and position_next[1] == PGoal[1]:
        done = True
        reward = 50
    elif iscliff(position_next) == True:
        position_next = PStart
        reward = -100
    elif isoutlayout(position_next) == True:
        position_next = position
        reward = -10      
    else:
        reward = -0.1
    # ---------------------------
    # スタートとゴールからの距離を計算する
    Sx_next = position_next[0]
    Sy_next = position_next[1]
    EuDist_S = round(np.sqrt((Sx_next - PStart[0]) ** 2 + (Sy_next - PStart[1]) ** 2),4)
    EuDist_G = round(np.sqrt((Sx_next - PGoal[0]) ** 2 + (Sy_next - PGoal[1]) ** 2),4)
    EuDist_T = EuDist_S + EuDist_G
    arr_dists = [EuDist_S, EuDist_G, EuDist_T]
    # ---------------------------
    # Stateを計算する
    state_next = np.array([Sx_next, Sy_next, EuDist_S, EuDist_G])
    
    return state_next, position_next, reward, done, arr_dists
    
# ------
# test_no3 -> It works!
position = [1.0, 1.0]
state_next, position_next, reward, done, arr_dists = cliff_env(position, 0)
print("position:{}, position:{}, reward:{}, done:{}, arr_dists:{}".format(state_next, position_next, reward, done, arr_dists))

# =================================================
# ENV-test用
# =================================================
# ---
# [0.0, -1.0],  # 0->left
# [0.0, 1.0],   # 1->right
# [-1.0, 0.0],  # 2->up
# [1.0, 0.0],   # 3->down
# ---
state, position, reward, done, arr_dists = cliff_reset()
print("--- RESET ---")
print("position:{}, position:{}, reward:{}, done:{}, arr_dists:{}".format(state, position, reward, done, arr_dists))

arr_action = [2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3]
for i, action in enumerate(arr_action):
    state, position, reward, done, arr_dists = cliff_env(position, action)
    print("--- MOVE:{}-{} ---".format(i, action))
    print("position:{}, position:{}, reward:{}, done:{}, arr_dists:{}".format(state, position, reward, done, arr_dists))
 
# =================================================
# RANDOM.CHOICE - アンチョコ用
# =================================================
# Calc-Distance
def EuGoal_dist(position, action):

    Sx, Sy = position[0], position[1]
    Ax, Ay = LRUD[action][0], LRUD[action][1]
    # ---
    Sx_next = Sx + Ax
    Sy_next = Sy + Ay
    
    EuDist_G = (Sx_next - PGoal[0]) ** 2 + (Sy_next - PGoal[1]) ** 2

    return EuDist_G

# ------
# random_probability
def dist_prob(position):
    # ---
	# [0.0, -1.0],  # 0->left
	# [0.0, 1.0],   # 1->right
	# [-1.0, 0.0],  # 2->up
	# [1.0, 0.0],   # 3->down
    # ---
    if position == PStart:
        arr_prob = [0.1, 0.1, 0.7, 0.1]
    else:
        arr_dist, arr_prob = [], [0.2, 0.2, 0.2, 0.2]
        for action in [0,1,2,3]:
            EuDist_G = EuGoal_dist(position, action)
            arr_dist.append(EuDist_G)

        idx_min = np.argmin(arr_dist)
        arr_prob[idx_min] = 0.4

    return arr_prob 

# =================================================
# Deep Learning Model class
# =================================================
# PyTorchのDLの定義
class Net(torch.nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = torch.nn.Linear(dim_input, 128)
        self.fc1.weight.data.normal_(0, 0.1)
        #self.fc2 = torch.nn.Linear(128, 128)
        #self.fc2.weight.data.normal_(0, 0.1)
        self.fc2 = torch.nn.Linear(128, dim_output)
        self.fc2.weight.data.normal_(0, 0.1)

    def forward(self, x):
        x = torch.nn.functional.relu(self.fc1(x))
        #x = torch.nn.functional.relu(self.fc2(x))
        x = self.fc2(x)
        return x

# =================================================
# Calculation class(2) : DQN_Solver:
# =================================================
# Solving the minesweeper in Deep Q-learning
class DQN_Solver:

    def __init__(self):

        # --------------------------------------------------
        # ハイパーパラメタ
        self.memory     = deque(maxlen=MEMORY_CAPACITY)
        self.gamma      = GAMMA
        self.epsilon    = EPSILON
        self.e_decay    = EPDECAY
        self.e_min      = EPMIN
        self.learning_rate = LRATE
        # --------------------------------------------------
        # crate instance for input
        self.eval_net, self.target_net = Net(), Net()  # 利用Net创建两个神经网络: 评估网络和目标网络
        self.learn_step_counter = 0
        # --------------------------------------------------
        # Save and load the model via state_dict
        # self.eval_net.load_state_dict(torch.load(file_input_model))
        # --------------------------------------------------
        # set validaiton mode
        self.eval_net.eval()
        # --------------------------------------------------
        # set training parameters
        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=self.learning_rate)
        self.criterion = torch.nn.MSELoss()

    # ----------------
    # PyTorchモデルの保存
    # def save_models(self):
    # torch.save(self.eval_net.state_dict(), file_output_model)

    # ----------------
    # メモリを蓄積する
    def remember_memory(self, state, action, reward, state_next, done):
        self.memory.append((state, action, reward, state_next, done))

    # ----------------
    # 命令を選択する
    def choose_action(self, state, iCnt_play):

        # ----------------------------------------
        # 移動命令
        # LRUD = [
        #    [0.0, -1.0],  # 0->left
        #    [0.0, 1.0],   # 1->right
        #    [-1.0, 0.0],  # 2->up
        #    [1.0, 0.0],   # 3->down
        # ----------------
        Qvalue = -0.0001
        # ----------------
        # 最適命令の選択
        if self.epsilon <= random.random():
            # DQNによる選択
            acType = "machine"
            for iCnt, action in enumerate([0,1,2,3]):
                # [array]-action を生成します。
                if action == 0:
                    temp_s_a = np.hstack([state, [1, 0, 0, 0]])
                elif action == 1:
                    temp_s_a = np.hstack([state, [0, 1, 0, 0]])
                elif action == 2:
                    temp_s_a = np.hstack([state, [0, 0, 1, 0]])
                elif action == 3:
                    temp_s_a = np.hstack([state, [0, 0, 0, 1]])
                # ----
                if iCnt == 0:
                    mx_input = np.array([temp_s_a])  # 状態(S)行動(A)マトリックス
                else:
                    mx_input = np.append(mx_input, np.array([temp_s_a]), axis=0)  # 状態(S)行動(A)マトリックス
            # print("----- mx_input -----")
            # print(mx_input)
            # --------------------------------------------------
            # generate new 'x'
            x_input_tensor = torch.from_numpy(mx_input).float()
            # predict 'y'
            with torch.no_grad():
                y_pred_tensor = self.eval_net(x_input_tensor)
            # convert tensor to numpy
            y_pred  = y_pred_tensor.data.numpy().flatten()
            Qvalue  = np.max(y_pred)
            a_order = np.argmax(y_pred)
        else:
            # 乱数による選択
            acType = "random"
            if iCnt_play < 1500:
                # アンチョコ用
                prob_choice = dist_prob([state[0], state[1]])
                a_order = np.random.choice([0,1,2,3], p=prob_choice)
            else:
                # 通常用
                a_order = np.random.choice([0,1,2,3])

        return a_order, acType, Qvalue

    # ----------------
    # (REPLAY EXPERIENCE)学習する
    def replay_experience(self, batch_size):

        # 目标网络参数更新
        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:  # 一开始触发，然后每100步触发
            self.target_net.load_state_dict(self.eval_net.state_dict())  # 将评估网络的参数赋给目标网络
        self.learn_step_counter += 1
        # --------------------------------------------------
        batch_size = min(batch_size, len(self.memory))
        minibatch = random.sample(self.memory, batch_size)
        X, Y = np.array([]), np.array([])
        # --------------------------------------------------
        for ibat in range(batch_size):
            state, action, reward, state_next, done = minibatch[ibat]
            # [array]-action を結合します。
            if action == 0:
                state_action_curr = np.hstack([state, [1, 0, 0, 0]])
            elif action == 1:
                state_action_curr = np.hstack([state, [0, 1, 0, 0]])
            elif action == 2:
                state_action_curr = np.hstack([state, [0, 0, 1, 0]])
            elif action == 3:
                state_action_curr = np.hstack([state, [0, 0, 0, 1]])
            # print("state_action_curr",state_action_curr)
            # ----------------
            if done:
                target_f = reward
            else:
                mx_input = []  # 状態(state)と行動(action)マトリックス
                for iCnt, imov in enumerate([0,1,2,3]):
                    # [array]-action を結合します。
                    if imov == 0:
                        temp_s_a = np.hstack([state_next, [1, 0, 0, 0]])
                    elif imov == 1:
                        temp_s_a = np.hstack([state_next, [0, 1, 0, 0]])
                    elif imov == 2:
                        temp_s_a = np.hstack([state_next, [0, 0, 1, 0]])
                    elif imov == 3:
                        temp_s_a = np.hstack([state_next, [0, 0, 0, 1]])
                    # ----
                    if iCnt == 0:
                        mx_input = [temp_s_a]  # 状態(S)行動(A)マトリックス
                    else:
                        mx_input = np.concatenate([mx_input, [temp_s_a]], axis=0)  # 状態(S)行動(A)マトリックス
                # ----------------
                mx_input = np.array(mx_input)
                # print("--- mx_input ---")
                # print(mx_input)
                # --------------------------------------------------
                # generate new 'x'
                x_input_tensor = torch.from_numpy(mx_input).float()
                # predict 'y'
                with torch.no_grad():
                    y_pred_tensor = self.target_net(x_input_tensor)
                # convert tensor to numpy
                y_pred = y_pred_tensor.data.numpy().flatten()
                np_n_r_max = np.amax(y_pred)
                target_f = reward + self.gamma * np_n_r_max
            # ----------------
            if ibat == 0:
                X = np.array([state_action_curr])  # 状態(S)行動(A)マトリックス
            else:
                X = np.append(X, np.array([state_action_curr]), axis=0)  # 状態(S)行動(A)マトリックス
            Y = np.append(Y, target_f)
        # --------------------------------------------------
        # TRAINING
        # convert numpy array to tensor
        state_action_next = torch.from_numpy(X).float()
        q_target = torch.from_numpy(Y.reshape(-1, 1)).float()
        # print("state_action_next:",state_action_next)
        # print("q_target:",q_target)
        # --- building model ---
        q_eval = self.eval_net(state_action_next)
        # calculate loss
        loss = self.criterion(q_eval, q_target)
        # update weights
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        # Show progress
        # print('learn done -- [epsilon: {0}, loss: {1}]'.format(self.epsilon, loss))
        if self.epsilon > self.e_min:
            self.epsilon *= self.e_decay

        return round(self.epsilon, 5), round(loss.data.item(), 5)

# =================================================
# Calculation class(3) : Agent
# =================================================
class Agent():
    # -----
    def __init__(self):

        # --------------------------------------------------
        # イプシロンの設定
        self.epsilon    = EPSILON  # ε-Greedy
        # ---------------------------
        # 記録用パラメタ類(プレイベース)
        self.arr_iplay      = []  # count game play    プレイ番号
        self.arr_maxturn    = []  # turn game play    ターン数
        self.arr_maxscore   = []  # rl_score game play    報酬の総和
        self.arr_victory    = []  # victory    勝利したか
        self.arr_loss       = []  # DQN-Experience Replay学習
        self.arr_epsilon    = []  # ε-Greedy
        # ---------------------------
        # Q値の分析用(プレイベース)
        self.arr_numQV = []  # QV値のN数
        self.arr_maxQV = []  # QV値の最大値
        self.arr_q25QV = []  # QV値の4分の1値
        self.arr_q75QV = []  # QV値の4分の3値
        # ---------------------------
        # ユークリッド距離の分析用(プレイベース)
        self.arr_maxED = []  # ユークリッド距離
        # print("starting simulation")
        # --------------------------------------------------
        # エピソードを実行する
        for iCnt_play in range(num_episodes):  # num_episodes
            # ゲームする
            maxturn, maxscore, flag_goal, maxEDist = self.get_episode(iCnt_play)
            # ----------
            # DQN-Experience Replay学習
            val_epsilon, val_loss = dql_solver.replay_experience(BATCH_SIZE)
            # 結果の出力
            print("iCnt_play:{0}, maxturn:{1}, maxscore:{2}, flag_goal:{3}, epsilon:{4}, loss:{5}".format(iCnt_play,
                                                    maxturn, maxscore, flag_goal, val_epsilon, val_loss))
            # ----------
            # 記録用パラメタ類(プレイベース)の追加
            self.arr_iplay.append(iCnt_play)  # count game play    プレイ番号
            self.arr_maxturn.append(maxturn)  # max_turn   ゲームのターン数
            self.arr_maxscore.append(maxscore)  # rl_score game play    最終プレイスコア
            self.arr_victory.append(flag_goal)  # victory    勝利したか
            self.arr_loss.append(val_loss)    # DQN-Experience Replay学習
            self.arr_epsilon.append(val_epsilon)  # イプシロン
            # ----------
            # Q値の保管(プレイベース)
            self.arr_numQV.append(maxturn)  # QV値のN数
            self.arr_maxQV.append(np.max(self.arr_predQV))  # QV値の最大値
            self.arr_q25QV.append(np.percentile(self.arr_predQV, q=25))  # QV値の4分の1値
            self.arr_q75QV.append(np.percentile(self.arr_predQV, q=75))  # QV値の4分の3値
            # ユークリッド距離の分析用(プレイベース)
            self.arr_maxED.append(maxEDist)  # ユークリッド距離
            # ----------
            # しばらくすれば表示が消えます
            if iCnt_play % 50 == 0:
                time.sleep(SLEEP_TIME)
                clear_output(wait=True)

        # --------------------------------------------------
        # 学習履歴を出力する
        fig = plt.figure(figsize=(14, 6))
        ax1 = fig.add_subplot(1, 2, 1)
        ax1.set_title('learning transition : epsilon')
        ax1.plot(self.arr_iplay, self.arr_epsilon, label="epsilon", color="blue")
        ax1.set_xlabel('#episode')
        ax1.set_ylabel('epsilon')
        ax1.legend(loc='best')
        # -----
        ax2 = fig.add_subplot(1, 2, 2)
        ax2.set_title('learning transition : Learning loss')
        ax2.set_xlabel('#episode')
        ax2.set_ylabel('Loss amount')
        ax2.plot(self.arr_iplay, self.arr_loss, label="loss", color="blue")
        ax2.legend(loc='best')
        plt.show()

        # --------------------------------------------------
        # グラフを表示する
        self.show_graph()

```

**(実行結果その１)**

![imageJRL1-9-2](/2023-01-12-QEUR22_CLIFF0/imageJRL1-9-2.jpg)

```python

    # --------------------------------------------------
    # エピソードを運用する
    def get_episode(self, iCnt_play):

        # ---------------------------
        # 機械学習用のパラメタ群
        Qvalue, reward, done, flag_goal = -0.0001, 0, False, "NA"
        maxturn, maxscore, maxEDist = 0, 0, 0
        # ---------------------------
        # 記録用パラメタ類(ターンベース)
        self.arr_iturn  = []  # ターン・カウンタリスト
        self.orders_row = []  # 指示リスト(row)
        self.orders_col = []  # 指示リスト(col)
        self.arr_orders = []  # 指示リスト(命令)
        self.arr_acType = []  # 指示のタイプ
        self.arr_scores = []  # ゲームスコアリスト
        self.arr_dones  = []  # ゲームオーバーフラグ
        self.arr_EuDist = []  # ユーグリッド距離のリスト
        self.arr_predQV = []  # Q値のリスト
        # ---------------------------
        # ゲームをリセットする
        state, position, reward, done, arr_dists = cliff_reset()

        # ---------------------------
        # ゲームをプレイする
        iCnt_turn = 0
        while True:
            # 命令(a_order)と状態(state)を作成する
            a_order, acType, Qvalue = dql_solver.choose_action(state, iCnt_play)
            # ---------------------------
            # ゲームをもう一歩進める
            state_next, position_next, reward, done, arr_dists = cliff_env(position, a_order)
            # ゲームが完了しているのか
            if done   == True:
                flag_goal = "goal"
            #print("iCnt_play:{}, iCnt_turn:{}, position:{}, state:{}, flag_goal:{}, reward:{}, arr_dists:{}".format(iCnt_play, iCnt_turn, position, state, flag_goal, reward, arr_dists))

            # ---------------------------
            # 記録用リストを追記する
            self.arr_iturn.append(iCnt_turn)  # ターン・カウンタリスト
            self.orders_row.append(position[0])  # 指示リスト(ROW)
            self.orders_col.append(position[1])  # 指示リスト(COL)
            self.arr_orders.append(a_order)  # 指示リスト(命令)
            self.arr_acType.append(acType)  # 指示のタイプ
            self.arr_scores.append(reward)  # ゲームスコアリスト
            self.arr_dones.append(done)  # ゲームオーバーフラグ
            self.arr_EuDist.append(arr_dists[2])  # ユーグリッド距離のリスト
            self.arr_predQV.append(Qvalue)  # Q値のリスト
            # ----------------
            # Experience Replay配列に保管する
            dql_solver.remember_memory(state, a_order, reward, state_next, done)
            # ----------------
            # 実行継続するか分岐
            if done == True:
                # Game over
                break  # (game over rip)
            elif iCnt_play < 2000 and iCnt_turn > 30000:
                flag_goal = "too_long"
                # Game over
                break  # (game over rip)
            elif iCnt_play >= 2000 and iCnt_turn > 2000:
                flag_goal = "too_long"
                # Game over
                break  # (game over rip)
            else:
                # ----------------
                # count of game turn
                position    = position_next
                state       = state_next
                iCnt_turn   = iCnt_turn + 1

        # --------------------------------------------------
        # プレイデータの引継ぎ
        maxturn, maxscore, maxEDist = iCnt_turn, np.sum(self.arr_scores), np.amax(self.arr_EuDist)

        return maxturn, round(maxscore,2), flag_goal, maxEDist

    # ----------------
    # 学習結果のグラフ化
    def show_graph(self):

        fig2 = plt.figure(figsize=(14, 12))
        # -----
        ax1 = fig2.add_subplot(2, 2, 1)
        ax1.set_title('DQN parameter transition: QValue')
        ax1.set_xlabel('#episode')
        ax1.set_ylabel('QValue')
        ax1.plot(self.arr_iplay, self.arr_maxQV, label="max_QV", color="blue")
        ax1.plot(self.arr_iplay, self.arr_q25QV, label="q25_QV", color="red")
        ax1.plot(self.arr_iplay, self.arr_q75QV, label="q75_QV", color="green")
        ax1.grid(True)
        ax1.legend(loc='best')
        # -----
        # 移動平均を付ける
        y_rolling = pd.Series(self.arr_maxED).rolling(window=12, center=True).mean()
        ax2 = fig2.add_subplot(2, 2, 2)
        ax2.set_title('learning transition : maxEDist')
        ax2.set_xlabel('#episode')
        ax2.set_ylabel('maxEDist')
        ax2.grid(True)
        ax2.plot(self.arr_iplay, self.arr_maxED, label="original", color="blue")
        ax2.plot(self.arr_iplay, y_rolling, label="moving", color="red")
        ax2.legend(loc='best')
        # -----
        # 移動平均を付ける
        y_rolling = pd.Series(self.arr_maxturn).rolling(window=12, center=True).mean()
        ax3 = fig2.add_subplot(2, 2, 3)
        ax3.set_title('learning transition : maxturn')
        ax3.set_xlabel('#episode')
        ax3.set_ylabel('maxturn')
        ax3.grid(True)
        ax3.plot(self.arr_iplay, self.arr_maxturn, label="original", color="blue")
        ax3.plot(self.arr_iplay, y_rolling, label="moving", color="red")
        ax3.legend(loc='best')
        # -----
        # 移動平均を付ける
        y_rolling = pd.Series(self.arr_maxscore).rolling(window=12, center=True).mean()
        ax4 = fig2.add_subplot(2, 2, 4)
        ax4.set_title('learning transition : maxscore')
        ax4.set_xlabel('#episode')
        ax4.set_ylabel('maxscore')
        ax4.grid(True)
        ax4.plot(self.arr_iplay, self.arr_maxscore, label="original", color="blue")
        ax4.plot(self.arr_iplay, y_rolling, label="moving", color="red")
        ax4.legend(loc='best')
        # -----
        fig2.tight_layout()
        # fig.savefig("./AAC_img.png")
        plt.show()
```


**(実行結果その２)**

![imageJRL1-9-3](/2023-01-12-QEUR22_CLIFF0/imageJRL1-9-3.jpg)

```python

# =================================================
# main function
# =================================================
if __name__ == "__main__":

    # ---------------------------
    # パラメタ設定の初期化
    dim_input  = 4 + 4  # action_onehot
    dim_output = 1
    # ---------------------------
    # ハイパーパラメタ
    BATCH_SIZE = 128  # サンプルサイズ
    LRATE = 0.001  # 学習率
    EPSILON = 0.99  # greedy policy
    EPDECAY = 0.999
    EPMIN = 0.01
    GAMMA = 0.95  # reward discount
    TARGET_REPLACE_ITER = 100  # 目标网络更新频率
    MEMORY_CAPACITY = 128 * 8  # メモリ容量
    # ---------------------------
    SLEEP_TIME = 0.01
    num_episodes = 2000  # 繰り返し回数

    # ---------------------------
    # フォルダ名の指定
    # foldername = "./ML_csvout/"  # My project folder

    # ---------------------------
    # dql_solverのインスタンス化
    dql_solver = DQN_Solver()

    # ---------------------------
    # 出力用:Pytorchモデルのファイル名
    # comment_output_model = "initial"
    # code_output_model = "model_cliffwalk_DQNER_{0}.pt".format(comment_output_model)  # モデルの指定
    # file_output_model = foldername + code_output_model  # ファイルパス名の生成

    # ---------------------------
    # DQN-ERで学習する
    Agent()

```

D先生 ： “ああ・・・、懐かしいですねえ・・・。環境（ゲーム）部分については自作に切り替えましたか・・・。”

QEU:FOUNDER ： “Julia版の環境を尊重した設計です・・・(笑)。でもね、Julia版のゲームでは全く学習が収束しないですよ。だから、相当変えています。あと、Stateのインプット項目の情報量が不十分なせいか、収束したりしなかったりと不安定です。その意味でも、いろいろ「手品」を組み込んでいます。”

D先生 ： “わざわざPythonにまで戻って、これからどうするんですか？”

QEU:FOUNDER ： “これをベースに、徐々にJulia版に切り替えていきます。そうすれば、比較が楽だし、ReinforcementLearning.jlのありがたみがわかるんじゃないかなぁ・・・。”

D先生 ： “やっぱり、Python版のグラフ群はきれいだ・・・。”

QEU:FOUNDER  ： “キレイなだけじゃないよ。何が起こっているのかがわかりやすいんですよ。特に、QValueのグラフを見れば、このロジックに学習能力があるのかがわかります。”

D先生 ： “なるほど！！学習損失の変動や、スコアの変動を見てもよくわからないですからね。”

QEU:FOUNDER ： “**「Hook」**っていうんだっけ、学習の状態を抽出する機能って・・・。Hookにもっといろいろな機能があれば便利なんですけどね。”

D先生 ： “できれば、そこまでやってみたいですね。できれば・・・。”



## ～　まとめ　～

## ・・・　前回のつづきです　・・・

QEU:FOUNDER ： “C国のB2Cプラットフォームは製造側の創造力を大きく伸ばした反面、国内量販店、小売店が全国的に壊滅しました。結局は「良しあしの問題」なのかもしれない。ただし、小生はC国はものづくり大国でありつづけると思うよ。”

[![MOVIE1](http://img.youtube.com/vi/aXzDXf-KJeY/0.jpg)](http://www.youtube.com/watch?v=aXzDXf-KJeY "淘寶開箱📦～ 9件好物沒後悔買了它們！😁👻28-1-2022")

C部長 : “C国は、いまだに周りからたたかれていますが・・・。”

![imageJRL1-9-4](/2023-01-12-QEUR22_CLIFF0/imageJRL1-9-4.jpg)

QEU:FOUNDER ： “C国は、その持ち前の創造力で、その圧力を跳ね返すんです。昔のJ国みたいに・・・。今までの議論をかんがみて、動画を見ると・・・失笑・・・・。”

[![MOVIE2](http://img.youtube.com/vi/5fhNArCQAUA/0.jpg)](http://www.youtube.com/watch?v=5fhNArCQAUA "【経済大展望2023】今年は賃金上昇の転換点／GAFAのトップ人材が日本で働く／日本はG7で最も実質成長率が高い／低賃金が技術革新を遅らせる／英語に頼らないインバウンド対応")

QEU:FOUNDER ： “ちなみに、いままでの議論にはちゃんと背景(↓)があるんですよ。この本って、1993年だったっけ・・・。”

![imageJRL1-9-5](/2023-01-12-QEUR22_CLIFF0/imageJRL1-9-5.jpg)

C部長 : “いわゆる、**「未来学者」**でしたっけ。彼は・・・。”

QEU:FOUNDER ： “知らん・・・。この本、小生のうろ覚えでは、**知識社会におけるサプライチェーンでは、上流（製造）―下流（小売）のパワーシフトが起こる**ことを述べています。下流（小売）は従来は製造側のモノをそのまま並べ、ただ売ることを脱し、自分がPOSデータなどで蓄えた情報を武器に上流にモノ申すようになる。最後は、「自分でモノを作る」ようになると・・・。”

C部長 : “DONKIは「情熱価格」、AEONは・・・。”

QEU:FOUNDER ： “でもね、現実は未来学者を超えたんですよね。**消費者が直接に上流とくっついた！！**”

C部長 : “そうすると？”

QEU:FOUNDER ： “エコシステム（生態系）が大きく変わりました。**生態系が変わると、そこに住む生物も変わっちゃうよね**。C国では進化が生まれ、製造が強くなりました。一方、J国では進化は進まず、いまだに下流が強い状況が維持されています。”

C部長 : “それが安いJ国というわけですね・・・。”

QEU:FOUNDER ： “お国がらというわけ・・・。それがいいかどうかは知らん・・・。”

