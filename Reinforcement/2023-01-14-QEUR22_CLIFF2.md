---
title: QEUR22_CLIFF2:　Cliff_Walkingで遊ぼう（その3：JuliaRLAGENT編-TABLE）
date: 2023-01-14
tags: ["QEUシステム", "メトリックス", "Julia言語", "Cliff Walking", "Flux", "ディープラーニング", "強化学習"]
excerpt: Julia言語を使った強化学習
---

## QEUR22_CLIFF2:　Cliff_Walkingで遊ぼう（その3：JuliaRLAGENT編-TABLE）

## ～　QTABLEは基本中の基本！！　～

QEU:FOUNDER(設定年齢65歳) ： “この番組は、**「高齢者によるイノベーション」**です。いやいや、2023年はさらにイクかもねえ・・・。”

![imageJRL1-11-1](/2023-01-14-QEUR22_CLIFF2/imageJRL1-11-1.jpg)

D先生(設定年齢65歳)  ： “オッサンの手柄ですね。こんなモンを社会の「良識」として、30年間なにもしなければ、当たり前に衰退するって・・・。”

### おっさん（＠QCサークル講話）；「従業員の皆さんにはテレビを見てください。皆が同じように考えてください。」

### オッサン（＠車中、N社検査不正について）：　「“検査不正”っていうのはなァ、（組織外に不正を）漏らしたヤツが悪いんだよ・・・」

### オッサン（海外工場のあいさつにて）：「私の使命はこの会社で終身雇用制を実現することにある・・・。」

QEU:FOUNDER ： “なんのなんの、プロレス好きのオッサンをなめてはいけない。これは、**「ワシは、わざとロープ際で攻められている」**のだ・・・。あとで、**「延髄斬り」**がでるよ！！”

D先生 ： “必殺技が来る・・・(笑)。”

QEU:FOUNDER ： “たぶん・・・。さて、地道な我々としては、強化学習も、「いきなりDQN」にいくのは大変なのでテープル解法をつかってみましょう。それでは、環境(ENV)-Agent側プログラムを晒すよ！”

```julia
# ---
# TRIAL【2】_Cliff_Walking_TABLE
# ---
using ReinforcementLearning
using Flux
using Flux.Losses
using Random

# ----
# Cliff_Walking_Environment
# ----
NX, NY = 4, 12
PStart = [4, 1]
PGoal  = [4, 12]
LRUD = [
	[0, -1],  # 1->left
	[0, 1],   # 2->right
	[-1, 0],  # 3->up
	[1, 0],   # 4->down
]

# ------
function iscliff(p)
    x, y = p[1], p[2]
    if x == 4 && y > 1 && y < NY
		return true
	else
		return false
	end
end

# ------
# 環境用のユーティリティ関数群
using LinearAlgebra
function isoutlayout(p)
    x, y = p[1], p[2]
    if x < 1 || x > NX || y < 1 || y > NY
        return true
    else
        return false
	end
end

function calc_onehot(p)
	onehot = zeros(Float32, NX*NY)
    x, y = p[1], p[2]
    number::Int64  = x + (y-1)*NX
	onehot[number] = 1.0
	return onehot
end

function calc_number(p)
    x, y = p[1], p[2]
    number::Int64  = x + (y-1)*NX
	return number
end

function rev_onehot(p)
	arrseq = 1:NX*NY
	number = dot(arrseq, p)
	return number
end

function calc_rewards(p)
	reward::Float32 = 0.0
	if env.flg_layout == true
		reward = -10.0
	elseif env.flg_cliff == true
		reward = -100.0
	elseif p[1] == PGoal[1] && p[2] == PGoal[2]
		reward = 50
	else
		reward = -0.1
	end
	return reward
end

function calc_isterm(p,cnt)
    if p[1] == PGoal[1] && p[2] == PGoal[2]
        return true
	elseif cnt > 20
		return true
    else
        return false
	end
end

# ------
# 環境の構築
Base.@kwdef mutable struct CliffWalkingEnv <: AbstractEnv
	position = PStart
	count::Int = 0
	flg_layout = false
	flg_cliff  = false
end

function (env::CliffWalkingEnv)(a::Int)
	env.flg_layout = false
	env.flg_cliff  = false
	temp = LRUD[a]
	x, y = env.position[1] + temp[1], env.position[2] + temp[2]
	if isoutlayout([x,y]) == true
		env.position = env.position
		env.flg_layout = true
	elseif iscliff([x,y]) == true
		env.count     = env.count + 1
		env.position  = PStart
		env.flg_cliff = true
	else
		env.position = [x, y]
	end
end

function env_reset()
    env.position = PStart
    env.count = 0
	env.flg_layout = false
	env.flg_cliff  = false
end

# ------
RLBase.state(env::CliffWalkingEnv) = calc_number(env.position)
RLBase.state_space(env::CliffWalkingEnv) = Base.OneTo(length(calc_number(env.position)))
RLBase.action_space(env::CliffWalkingEnv) = Base.OneTo(length(LRUD))
RLBase.reward(env::CliffWalkingEnv) = calc_rewards(env.position)
RLBase.is_terminated(env::CliffWalkingEnv) = calc_isterm(env.position, env.count)
RLBase.reset!(env::CliffWalkingEnv) = env_reset()
env = CliffWalkingEnv()

```

![imageJRL1-11-2](/2023-01-14-QEUR22_CLIFF2/imageJRL1-11-2.jpg)

D先生 ： “今回、環境が構築されると**Traitの情報が少し変わっています**。State-Spaceの値は以前は48だったでしょう？”

QEU:FOUNDER ： “テーブル解法に合わせて、プログラムを変えました。”

D先生 ： “どう変えたんですか？”

QEU:FOUNDER ： “次のテストを見ればわかります。ドン・・・。”

```julia
# -------
# SIMULATION
# 動きを確認する
# -------
#	[0, -1],  # 1->left
#	[0, 1],   # 2->right
#	[-1, 0],  # 3->up
#	[1, 0],   # 4->down
# -------
# 決まったアクションの場合
arr_action = [3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4]
n_episode = 3

for epi in 1:n_episode
	reset!(env)
	count = 1
	for action in arr_action
		env(action)
		println("EPISODE: ", epi, ", CNT: ", count, ", ACT: ", action, ", ENV: ",state(env)," REWARD: ",reward(env)," TERM: ",is_terminated(env))
		if is_terminated(env) == true
			break
		end
		count = count + 1
	end
end

```

![imageJRL1-11-3](/2023-01-14-QEUR22_CLIFF2/imageJRL1-11-3.jpg)

D先生 ： “・・・ああ、なるほど・・・。***１～48番のアドレス***に変換されたんですね。”

QEU:FOUNDER ： “こうすると、テーブル解法が簡単になるんです。それでは、Agent側の開発を続けましょう。”

```julia
# -----
# SarsaAgent
# The Agent that uses SARSA update to improve it's behaviour
# -----
#	Args:
#		ε: The degree of exploration
#		γ: The discount factor
#		num_state: The number of states
#		num_actions: The number of actions
#		action_space: To call the random action
# -----
num_state 	= 48
num_actions = 4
# -----
# Defining all the required parameters
ε = 0.9
decay = 0.995
α = 0.5
γ = 1
total_episodes = 300
max_steps = 10000

# -----
Q = zeros((num_state, num_actions))

function update(prev_state, next_state, reward, prev_action, next_action)
	# -----
	#Update the action value function using the SARSA update.
	#Q(S, A) = Q(S, A) + α(reward + (γ * Q(S_, A_) - Q(S, A))
	#Args:
	#	prev_state: The previous state
	#	next_state: The next state
	#	reward: The reward for taking the respective action
	#	prev_action: The previous action
	#	next_action: The next action
	#Returns:
	#	None
	# -----
	predict = Q[prev_state, prev_action]
	target = reward + γ * Q[next_state, next_action]
	Q[prev_state, prev_action] += α * (target - predict)
end 

# -----
# Agent-ACTION
function choose_action(state)
	action = 1
	if rand() < ε
		action = rand([1,2,3,4])
	else
		action = argmax(Q[state, :])
	end
	return action
end

# -----
#    The two parameters below is used to calculate
#    the reward by each algorithm
# -----
arr_reward = []
# Now we run all the episodes and calculate the reward obtained by
# each agent at the end of the episode
 
for epi in 1:total_episodes
	# Initialize the necessary parameters before
	# the start of the episode
	t, episodeReward = 0, 0
	reset!(env)
	state_cur = state(env)
	#println("RESET-STATE:", state_cur)
	# Choosing the current action
	action = choose_action(state_cur)
	# ----
	while true
		# Getting the next state, reward, and other parameters
		#state_next, reward, done, info,_ = env.step(action)
		env(action)
		state_next = state(env)
		reward_cur = reward(env)
		done = is_terminated(env)
		#println("EPI:", epi,", STEP:", t,", state_next:", state_next,", reward:", reward_cur,", done:", done)
 
		# Choosing the next action
		action_next = choose_action(state_next)
		 
		# Learning the Q-value
		update(state_cur, state_next, reward_cur, action, action_next)

		# Updating the respective vaLues
		state_cur = state_next
		action    = action_next	
		episodeReward += reward_cur

		# If at the end of learning process
		if done
			println("episode:",epi," ε:",ε," GAME WIN")
			break
		elseif t > max_steps
			println("episode:",epi," ε:",ε," time OVER")
			break
		end
		# count up
		t += 1
    end
	push!(arr_reward, episodeReward)
	ε = ε * decay
end
 
# Print the results
using Statistics
println("SARSA Average Sum of Reward:", mean(arr_reward))

```

![imageJRL1-11-4](/2023-01-14-QEUR22_CLIFF2/imageJRL1-11-4.jpg)

```julia
# Graph
using Plots
plot(arr_reward)

```

![imageJRL1-11-5](/2023-01-14-QEUR22_CLIFF2/imageJRL1-11-5.jpg)

D先生 ： “やっぱり、テーブル解法は学習収束が速いですね。**「動的計画法(Dynamic programming)」のパワー**ですね。”

QEU:FOUNDER ： “このつぎは、いよいよDQNになります。でも、強化学習の基本は同じなので、テーブル解法(Discrete)のプログラムを作っておくと楽になるよねえ。”

D先生 ： “DQN(Continuous)は複雑ですから・・・。”

QEU:FOUNDER ： “収束しない場合、なにが悪いのかがわからなくなっちゃうんですよね。”

D先生 ： “お次は・・・。”

QEU:FOUNDER  ： “お次はDQN48・・・。AKBではありません・・・。”


## ～　まとめ　～

QEU:FOUNDER ： “そして、「2022年の顔」は今年も突っ走るのであった・・・。”

[![MOVIE1](http://img.youtube.com/vi/m_WTtXcXyIw/0.jpg)](http://www.youtube.com/watch?v=m_WTtXcXyIw "【鈴木エイト氏と語る、手つかずの問題“自民党と統一教会の関係”】郷原信郎の「日本の権力を斬る！」")

C部長 : “がんばって欲しいですね。・・・。”

QEU:FOUNDER ： “もう、ひたすら感謝、感謝、感謝・・・の人・・・。”

C部長 : “本件は、今年が良い年になる十分条件ではないが、必要条件ではありますね。”

QEU:FOUNDER ： “C先生・・・。たまには、いいことをいう・・・。”

