---
title: QEUR22_2048S5:　番外編～ゲーム2048で遊ぼう（その5：Julia編-SLIDERT）
date: 2023-01-31
tags: ["QEUシステム", "メトリックス", "Julia言語", "2048", "SOART", "ディープラーニング", "強化学習"]
excerpt: julia言語とテクノメトリックスを使った強化学習
---

## QEUR22_2048S5:　番外編～ゲーム2048で遊ぼう（その5：Julia編-SLIDERT）

## ～　大成功の大失敗なのだ・・・　～

### ・・・　前回のつづきです　・・・

D先生(設定年齢65歳)  ： “趣味で一つやらせてください。**「スライドRT(SLIDERT)」**法による強化学習をやってもらえませんか？”

QEU:FOUNDER(設定年齢65歳) ： “えっ！？あれ？やるの・・・！？Python言語でさんざんやったでしょう。え～と・・・、途中ですが、この番組は**「高齢者によるイノベーション」**です。”

![imageJRL1-25-1](/2023-01-31-QEUR22_2048S5/imageJRL1-25-1.jpg)

QEU:FOUNDER ： “スライド前のゲーム盤の状態と後の状態を比較し、それをRTメトリックスで指標化するものです。じゃあ、SLIDERT法をやってみるよ。プログラムをドン！！”

```julia
# ------
# テクノ・メトリックスの可能性を試すシリーズ
# Julia - SOARTメトリックス@Game2048の環境設定
# ------
using DataFrames, CSV
using LinearAlgebra
using Distances
using Plots
using Statistics

# =================================================
# テクノメトリックスのベクトルを計算する
# =================================================
# テクノ・メトリックスを計算する関数
function calc_techmetrics(grid, iTurn)

    # メトリックス配列を初期化する
    arr_metrics   = []
    arr_beta      = []
    arr_distance  = []
	beta     　= 1.0
	mDistance = 0.0
    # ベースのゲーム盤の状態
    board_temp    = convert(Matrix{Float64}, grid)
    board_cur    = vec(board_temp)
    # ------
    # 新RTメトリックスの計算用(STEP1) 
    #xx = dot(board_cur,board_cur)
    xx = sum(board_cur)
    # ---------------- 
    # DIRS = [:left, :up, :right, :down]
    # ---------------- 
    for i in 1:4
        d1 = DIRS[i]
        (grid_dummy, ok, cart, two_or_four, reward) = simulate_move!(grid, d1)
        if ok == true
            # 移動後のゲーム盤の状態
            board_temp    = convert(Matrix{Float64}, grid)
            board_next    = vec(board_temp)
            # ------
            # 新RTメトリックスの計算用(STEP1)
            #xy = dot(board_next, board_next)
            xy = sum(board_next)
            beta = round(xy/xx; digits=5)
            # ------
            # テクノ・メトリックスの計算用(STEP2)
            #mDistance = round(cityblock(board_next, beta*board_cur); digits=5)
			mDistance = round(mse(board_next, beta*board_cur); digits=5)
        else
            beta     　= 1.0
            mDistance = 0.0
        end
        # ------
        # 配列への追加
        push!(arr_beta, beta)
        push!(arr_distance, mDistance)
        #println("beta: $beta, mDistance: $mDistance")
    end

    # クロス系畳み込みマトリックスを計算する
    arr_metrics = vcat(arr_beta, arr_distance)
	push!(arr_metrics, iTurn)
    #println("arr_metrics: $arr_metrics")

    return arr_metrics
end

# =================================================
# 2048ゲームを実行するための関数群
# =================================================
using StatsBase

# グローバル宣言
const DIRS = [:left, :up, :right, :down]
rand2_1()  = rand() < 0.1 ? 2 : 1

function init_game()
    grid = zeros(Int8,4,4)    
    grid[rand(1:4),rand(1:4)] = rand2_1()
    grid[rand(1:4),rand(1:4)] = rand2_1()
    return grid
end

# a function to simulate the move and return a reward
function move!(x, xinc, xstart, xend)
    reward = 0
    @inbounds for i = xstart:xinc:xend # for each row move the left most piece first #if move_row = 1 then i control the row
        if x[i] != 0 # if the position is occupied by a number move it
            # firstly look "behind" to see if there is a number that is the same
            # this is to deal better with situations like 2 2 4 4
            @inbounds for k = i+xinc:xinc:xend
                if x[k] != 0
                    if x[k] == x[i]
                        x[i] += 1
                        reward += 2^x[i]
                        x[k] = 0
                    end
                    break;
                end
            end
            # now place it in the first empty slot
            @inbounds for k = xstart:xinc:i
                if x[k] == 0
                    x[k] = x[i]
                    x[i] = 0
                end
            end
        end
    end
    (x, reward)
end

function move_left!(x)
    move!(x, 1, 1, 4)
end

function move_right!(x)
    move!(x, -1, 4, 1)
end

function move!(grid::Array{T,2}, direction) where T <: Integer
    reward::Int16 = zero(Int16)
    if direction == :left
        for j = 1:4
            (tmp, new_reward) = move_left!(@view grid[j,:])
            reward += new_reward
        end
    elseif direction == :right
        for j = 1:4
            (tmp, new_reward) = move_right!(@view grid[j,:])
            reward += new_reward
        end
    elseif direction == :up
        for j = 1:4
            (tmp, new_reward) = move_left!(@view grid[:,j])
            reward += new_reward
        end
    else
        for j = 1:4
            (tmp, new_reward) = move_right!(@view grid[:,j])
            reward += new_reward
        end
    end
    (grid, reward)
end

# -------
# SIMULATION - TILE MOVE
# -------
# assume no need to check for validate moves
function simulate_move!(grid, direction)
    tmp_grid = copy(grid)
    (grid, reward) = move!(grid, direction)
    if all(tmp_grid .== grid)
        return (grid, false, CartesianIndex{2}(-1,-1), -1, 0)
    else
        cart = rand(findall(grid .== 0)) # randomly choose one empty slot
        one_or_two = rand2_1()
        grid[cart] = one_or_two
        return (grid, true, cart, one_or_two, reward)
    end
end

# =================================================
# TRIAL【1】_Game2048_DQN_TechnoMetricsRT_with_FEEDBACK
# =================================================
#using ReinforcementLearning
using Flux
using Flux: mse, huber_loss, onehotbatch
using Random
#using Statistics
#using Plots
#using LinearAlgebra

# ---------------------------
# グローバル宣言
global num_state
global num_actions 
global in_dim 
global out_dim

# ---------------------------
# Defining all the required parameters
global memory
global ε
global γ 
global e_decay  
global e_min   
global learning_rate
global total_episodes 

# =================================================
#    Args:
#        ε: The degree of exploration
#        γ: The discount factor
#        num_state: The number of states
#        num_actions: The number of actions
#        action_space: To call the random action
# =================================================
num_state     = 8 + 1
num_actions   = 4
in_dim  = num_state + num_actions
out_dim = 1

# Defining all the required parameters
ε     = 0.95
γ     = 0.99
e_decay    = 0.9999
e_min      = 0.01
total_episodes = 20000

# ---------------------------
# ハイパーパラメタ
const BATCH_SIZE    = 128  # サンプルサイズ
const MEMORY_CAPACITY = 128 * 32  # メモリ容量

# =================================================
# AGENT functions
# =================================================
# リセット：基本変数を初期化する
function reset!()
    # ---------------------------
    # ゲームの初期化(2タイルが２か所)
    grid = init_game()
    # ---------------------------
    arr_acType    = []
    arr_action    = []
    arr_Qvalue    = []
    arr_reward    = []
    mx_states     = []
    return grid, arr_acType, arr_action, arr_Qvalue, arr_reward, mx_states
end

# ----------------
# 命令を選択し、ゲームを動かす(ランダム判定)
function step_random!(grid) 
    Qvalue = -0.0001
    acType = "random"
    # ---------------- 
    # DIRS = [:left, :up, :right, :down]
    # ---------------- 
    directions = sample(DIRS, 4 , replace = false)
    flg_moved = false     # falseとは「ゲーム盤が動かない」という意味
    for i in 1:3
        d1 = directions[i]
        (grid, ok, cart, two_or_four, reward) = simulate_move!(grid, d1)
        if ok
            flg_moved = true     # trueとは「動いた」という意味
            return (d1, acType, Qvalue, grid, reward, flg_moved)
        end
    end
    (grid, ok, cart, two_or_four, reward) = simulate_move!(grid, directions[4])
    return (directions[4], acType, Qvalue, grid, reward, flg_moved)
end

# ----------------
# 命令を選択し、ゲームを動かす(マシン判定)
function step_machine!(grid, state_cur, iCnt_play, Qvalue)
    # ---------------- 
    # DIRS = [:left, :up, :right, :down]
    # ----------------
    # 最適命令を選択する(DQN)
    flg_moved = true         # falseとは「ゲーム盤が動かない」という意味
    acType    = "machine"
    mx_input  = zeros(4,in_dim)
    #println(mx_input)
    for action in 1:4
        # [array]-action を生成します。
        a_onehot = onehotbatch(action, 1:4)
        temp_s_a = vcat(state_cur, a_onehot)
        #println("temp_s_a-choose action: ", temp_s_a)
        mx_input[action,:] = temp_s_a'     # 状態(S)行動(A)マトリックス
    end
    #println("----- mx_input(choose_action: machine) -----")
    #println(mx_input)
    # ----------------
    # predict 'y'
    y_pred  = model(mx_input')
    Qvalue  = maximum(y_pred)
    a_order = argmax(y_pred)[2]
    action  = DIRS[a_order]
    # ----------------
    # ゲーム盤を動かす
    (grid, flg_moved, cart, two_or_four, reward) = simulate_move!(grid, action)

    return action, acType, round(Qvalue; digits=5), grid, reward, flg_moved
end

# ----------------
# ゲームオーバーしているか？
function is_gameover!(grid) 
    # ---------------- 
    # DIRS = [:left, :up, :right, :down]
    # ---------------- 
    flg_gameover = true      # trueとは「ゲームオーバーである」という意味
    for i in 1:4
        d1 = DIRS[i]
        (grid, ok, cart, two_or_four, reward) = simulate_move!(grid, d1)
        if ok
            flg_gameover = false     # falseとは「ゲームオーバーではない」という意味
            return flg_gameover
        end
    end
    return flg_gameover
end

# =================================================
# DQN_Solver functions
# =================================================
# crate instance for input
global model     = Chain(Dense(in_dim, 256, relu), Dense(256, 256, relu), Dense(256, out_dim)) # Use Chain if you want to stack layers
# パラメタ
global ps_model  = params(model)
# -----
# 損失関数
#global loss(x, y) = mse(model(x'), Matrix(y'))            # 二乗平均誤差
global loss(x, y)  = huber_loss(model(x'), Matrix(y'))    # フーバー誤差
# -----
# オプティマイザ
global opt         = ADAM()

# ----------------
# メモリを蓄積する
function remember_memory(iCnt_play, iTurn, memory, state_cur, action, reward, state_next, done)
    push!(memory, (iCnt_play, iTurn, state_cur, action, reward, state_next, done))
    len_memory = length(memory)
    if len_memory > MEMORY_CAPACITY
        memory = memory[2:end]
    end
    return memory
end

# ----------------
# (REPLAY EXPERIENCE)学習する
function replay_experience(iCnt_play, memory, batch_size)
    # --------------------------------------------------
    len_memory = length(memory)
    batch_size = min(batch_size, len_memory)
    minibatch  = rand(memory, batch_size)
    X, Y   = zeros(batch_size,in_dim), zeros(batch_size)
    # --------------------------------------------------
    for ibat in 1:batch_size
        iPlay, iTurn, state_cur, str_action, reward, state_next, done = minibatch[ibat]
        # CURRENT: < X >-MATRIX
        # ---------------- 
        # DIRS = [:left, :up, :right, :down]
        # ----------------
        if str_action == :left
            action = 1
        elseif str_action == :up
            action = 2
        elseif str_action == :right
            action = 3
        else 
            action = 4
        end
        # ---------------- 
        a_onehot = onehotbatch(action, 1:4)
        state_action_cur = vcat(state_cur, a_onehot)
        #println("state_action_cur: $state_action_cur")
        X[ibat,:]  = state_action_cur'
        # ----------------
        # NEXT: < Y >-ARRAY
        if done == true
            Y[ibat] = reward
        else
            mx_input = zeros(4,in_dim)  # 状態(state)と行動(action)マトリックス
            for imov in 1:4
                # [array]-action を結合します。
                a_onehot = onehotbatch(imov, 1:4)
                temp_s_a = vcat(state_next, a_onehot)
                #println("temp_s_a-choose action: ", temp_s_a)
                mx_input[imov,:] = temp_s_a'      # 状態(S)行動(A)マトリックス
            end
            #println("--- mx_input(replay_experience) ---")
            #println(mx_input)
            # ----------------
            # predict 'y'
            y_pred   = model(mx_input')
            target_f = reward + γ * maximum(y_pred)
            #println("target_f: $target_f")
            Y[ibat]  = target_f
        end
    end
    # ----------------
    # TRAINING
    # calculate loss -> gradient
    gs = gradient(() -> loss(X, Y), ps_model)
    # update weights
    Flux.Optimise.update!(opt, ps_model, gs)
    # ----------------
    val_loss = round(loss(X, Y); digits=5)
    #println("iPlay: $iCnt_play, loss: $val_loss")
    return val_loss
end

# =================================================
# EPISODE CONTROL functions
# =================================================
# エピソードを運用する
function get_episode!(iCnt_play, memory, rep_bonuscore)

    # リセット：基本変数を初期化する
    (grid, arr_acType, arr_action, arr_Qvalue, arr_reward, mx_states) = reset!()
    # -----
    # SOARTテクノメトリックスを計算する[RESET-CURRENT]
    # -----
    board = convert(Matrix{Float64}, grid)
    state_cur = calc_techmetrics(grid, 0)
    # -----
    flg_moved, flg_gameover, iTurn, total_score = true, false, 1, 0
    non_valid_count, valid_count = 0, 0
    while flg_gameover == false        # ゲームオーバーでループから外れる
        # 初期化
        Qvalue, acType  = -0.0001, "random"        
        # ----------------
        # 命令を選択し、ゲームを動かす
        if ε < rand() && flg_moved == true
            # DQNでコマを動かす
            (action, acType, Qvalue, grid, reward, flg_moved) = step_machine!(grid, state_cur, iCnt_play, Qvalue)
            # ゲーム盤が動かなければ報酬はマイナスになる
            #println("flg_moved: ", flg_moved)
        else
            # ランダムでコマを動かす
            (action, acType, Qvalue, grid, reward, flg_moved) = step_random!(grid)
        end
        #if iTurn%10 == 0
        #    println("Play: $iCnt_play, Turn: $iTurn, grid: $grid, acType: $acType, action: $action, re-ward: $reward, flg_moved: $flg_moved")
        #end
        # ----------------
        # SOARTテクノメトリックスを計算する[RUN-NEXT]
        # ----------------
        board_next = convert(Matrix{Float64}, grid)
        state_next = calc_techmetrics(grid, iTurn)
        #println("UPDATED - state_next: $state_next")
        # ----------------
         # 記録用リストを追記する
        push!(arr_acType, acType)
        push!(arr_action, action)
        push!(arr_Qvalue, Qvalue)
        push!(arr_reward, reward)
        if iTurn == 1
            mx_states = state_cur
        else
            mx_states = cat(mx_states, state_cur, dims=2)
        end
        # ----------------
        state_cur   = state_next
        total_score = total_score + reward
        # ----------------
        # カウント && ゲームオーバーしているか？ 
        if flg_moved == false
            non_valid_count = non_valid_count + 1
            flg_gameover = is_gameover!(grid)
        else
            valid_count = valid_count + 1
        end 
        # ----------------
        # ゲームオーバーした場合、ブレイクする
        if flg_gameover == true
            #println("BREAK!! - Play: $iCnt_play, Turn: $iTurn, flg_gameover: $flg_gameover")
            # 報酬修正(2)  ボーナス
            if total_score > rep_bonuscore*1.5 && iCnt_play > 20
                reward = 0.5*valid_count
            elseif total_score > rep_bonuscore && iCnt_play > 20
                reward = 0.3*valid_count
            else
                reward = -1.0*non_valid_count
            end
        else
            iTurn = iTurn + 1
        end
        # ----------------
        # Experience Replay配列に保管する
        memory = remember_memory(iCnt_play, iTurn, memory, state_cur, action, reward, state_next, flg_gameover)
    end
    # ---------------------------
    # ゲーム代表値のリスト
    rep_predQV = quantile(arr_Qvalue, [0.25, 0.5, 0.75, 1])
    
    return (iTurn, round(total_score; digits=2), memory, rep_predQV, valid_count, non_valid_count, mx_states')
end

# =================================================
# MAIN ROUTINE
# =================================================
# 記録用パラメタ類
memory        = []  # メモリのリスト
# ---------------------------
# 記録用パラメタ類
arr_iplay     = []  # count game play    プレイ番号
arr_maxturn   = []  # turn game play    ターン数
arr_maxscore  = []  # rl_score game play    報酬の総和
arr_loss      = []  # DQN-Experience Replay学習
arr_εs       = []  # ε-Greedy
# ---------------------------
# Q値の分析用
arr_maxQV     = []  # QV値の最大値
arr_q25QV     = []  # QV値の4分の1値
arr_q50QV     = []  # QV値の4分の2値
arr_q75QV     = []  # QV値の4分の3値
arr_nvalid    = []  # 無効命令率
# ---------------------------
# 初期化
val_loss      = 0.0
iCnt_play     = 1
rep_bonuscore = 1000
# ---------------------------
# エピソードを実行する
while true   
    # ゲームを動かす
    (maxturn, maxscore, memory, rep_predQV, valid_count, non_valid_count, mx_states) = get_episode!(iCnt_play, memory, rep_bonuscore)
    # 無効命令率の計算
    ratio_nvalid = round(non_valid_count/maxturn; digits=3)
    # 簡単な出力
    #println("maxturn: $maxturn, maxscore: $maxscore, ratio_nvalid: $ratio_nvalid")

    # ----------
    # DQN-Experience Replay学習(重み付き)
    if valid_count >= 150
        for iLearn in 1:50
            val_loss = replay_experience(iCnt_play, memory, BATCH_SIZE)
        end
    elseif valid_count > 100 && valid_count < 150
        for iLearn in 1:20
            val_loss = replay_experience(iCnt_play, memory, BATCH_SIZE)
        #    println("Learn_turn:$iLearn, loss:$val_loss")
        end
    else
        val_loss = replay_experience(iCnt_play, memory, BATCH_SIZE)
    end

    # ----------
    # 学習結果の出力
    if iCnt_play%40 == 0
        println("iPlay: $iCnt_play, maxturn: $maxturn, maxscore: $maxscore, ε:$ε, loss:$val_loss, bo-nuscore:$rep_bonuscore, ratio_nvalid:$ratio_nvalid")
    end
    #println(mx_states)
    #println(size(mx_states))
    # ----------
    # 記録用パラメタ類(プレイベース)の追加
    push!(arr_iplay, iCnt_play)  # count game play    プレイ番号
    push!(arr_maxturn, maxturn)  # max_turn   ゲームのターン数
    push!(arr_maxscore, maxscore)  # rl_score game play    最終プレイスコア
    push!(arr_loss, val_loss)    # DQN-Experience Replay学習
    push!(arr_εs, ε)  # イプシロン
    # ----------
    # ゲーム代表値の保管
    push!(arr_maxQV, rep_predQV[4])  # QV値の最大値
    push!(arr_q75QV, rep_predQV[3])  # QV値の4分の3値
    push!(arr_q50QV, rep_predQV[2])  # QV値の4分の2値
    push!(arr_q25QV, rep_predQV[1])  # QV値の4分の1値
    push!(arr_nvalid, ratio_nvalid)  # 無効命令率
    # ---------------------------
    # ボーナス支給のしきい値を設定する
    #rep_bonuscore = quantile(arr_maxscore, [0.75])[1]
    # ----------
    # Change EPSILON
    if ε > e_min && iCnt_play < total_episodes        # total_episodes
        ε *= e_decay
        ε = round(ε; digits=5)
        iCnt_play = iCnt_play + 1
    else
        break
    end
end

# ---------
# Create Graph(2) : 移動平均による平滑化
using MarketTechnicals

# A-GROUP
pltA1 = plot(arr_εs, label="EPSILON", legend=:topright)
pltA2 = plot(arr_loss, label="loss", legend=:topleft)
        plot!(ema(arr_loss, 10, wilder = false), label="movave", legend=:topleft)
pltA3 = plot(arr_maxturn, label="maxturn", legend=:topleft)
        plot!(ema(arr_maxturn,10, wilder = false), label="movave", legend=:topleft)
pltA4 = plot(arr_maxscore, label="maxscore", legend=:topleft)
        plot!(ema(arr_maxscore,10, wilder = false), label="movave", legend=:topleft)
plot(pltA1, pltA2, pltA3, pltA4)

```

**(MAXPOOL)**

[A]

![imageJRL1-25-2](/2023-01-31-QEUR22_2048S5/imageJRL1-25-2.jpg)

[B]

![imageJRL1-25-3](/2023-01-31-QEUR22_2048S5/imageJRL1-25-3.jpg)

**(SLIDERT)**

![imageJRL1-25-4](/2023-01-31-QEUR22_2048S5/imageJRL1-25-4.jpg)

```julia
# B-GROUP
pltB1 = plot(ema(arr_maxQV,10, wilder = false), label="max", legend=:topleft)
        plot!(ema(arr_q75QV,10, wilder = false), label="q75", legend=:topleft)
        plot!(ema(arr_q50QV,10, wilder = false), label="q50", legend=:topleft)
        plot!(ema(arr_q25QV,10, wilder = false), label="q25", legend=:topleft)
pltB2 = plot(ema(arr_nvalid,10, wilder = false), label="non_valid", legend=:topleft)
plot(pltB1, pltB2)
```

**(MAXPOOL)**

![imageJRL1-25-5](/2023-01-31-QEUR22_2048S5/imageJRL1-25-5.jpg)

**(SLIDERT)**

![imageJRL1-25-6](/2023-01-31-QEUR22_2048S5/imageJRL1-25-6.jpg)

D先生  ： “なんですか！？SLIDERTメトリックスって、ぜんぜんイケてるじゃないですか！！！確かに、SLIDERTはテクノメトリックスとしては**「最強」**であることは認めるが・・・。”

QEU:FOUNDER： “すまん・・・。”

D先生  ： “えっ！？いきなり誤って・・・。どうしました？”

QEU:FOUNDER ： “このプロジェクトを始めたときは、いろいろ試しても全然ダメ（学習できない）でした。しかし、状態(STATE)の中に**「ゲームのターン数(iTurn)を入れる」**と、マシンがいきなり学習をし始めたんです。”

**(感度のゲーム内推移)**

![imageJRL1-25-7](/2023-01-31-QEUR22_2048S5/imageJRL1-25-7.jpg)

**(SN比のゲーム内推移)**

![imageJRL1-25-8](/2023-01-31-QEUR22_2048S5/imageJRL1-25-8.jpg)

D先生  ： “つまり、メトリックスのゲーム内の変動がターン数(iTurn)にあまりに依存していると・・・。”

QEU:FOUNDER ： “ディープラーニングの立場から考えてみて・・・。こんなデータ（↑）をエピソード毎に詰め込まれて最適命令を計算してといわれても、なにがなんだかわからなくなるよ（学習分散が大きい）・・・。でも、もしもメトリックスがゲームのターン数に依存していることを承知していると、ディープラーニング様も納得しやすい。”

D先生  ： “「ディープラーニング様」と人間になりましたか・・・（笑）。どうします？全面的に（計算を）やり直し？”

![imageJRL1-25-9](/2023-01-31-QEUR22_2048S5/imageJRL1-25-9.jpg)

QEU:FOUNDER ： “そう・・・、やりなおし。それをしないと、総合SN比による**ロバストネス評価ができない**でしょ？”


### ～　まとめ　～

QEU:FOUNDER ： “今回も、宮〇先生の決定版の動画(↓)をネタにしよう。”

[![MOVIE1](http://img.youtube.com/vi/FCc8cojW3ZU/0.jpg)](http://www.youtube.com/watch?v=FCc8cojW3ZU "田内学×宮台真司：人を幸せにする経済とは")

C部長 : “本当に、経済のことがわかりますよね。FOUNDERはMMT推しでしょ？”

![imageJRL1-25-10](/2023-01-31-QEUR22_2048S5/imageJRL1-25-10.jpg)

QEU:FOUNDER ： “小生は今でもMMT推しではあるけど、**（MMTは）J国では無理ですよ。（J国には）生産力（稼ぐ力）が全然足らない**・・・。むかし、MMTに傾倒したときには、「（稼ぐ力が）ある」と思っていたんだけど・・・。”

C部長 : “（稼ぐ力は）実はなかった・・・（笑）。”

### おっさん（＠QCサークル講話）；「従業員の皆さんにはテレビを見てください。皆が同じように考えてください。」

### オッサン（＠車中、N社検査不正について）：　「“検査不正”っていうのはなァ、（組織外に不正を）漏らしたヤツが悪いんだよ・・・」

### オッサン（海外工場のあいさつにて）：「私の使命はこの会社で終身雇用制を実現することにある・・・。」

QEU:FOUNDER ： “おいおい、笑いごとじゃないってば・・・。よくもまあ、**ここまでグチャグチャにした**もんだ。”

C部長 : “「～にした」って表現、誰かが**恣意的にやった**ようじゃないですか。”

[![MOVIE2](http://img.youtube.com/vi/3Fiy9Te7r9Y/0.jpg)](http://www.youtube.com/watch?v=3Fiy9Te7r9Y "Tesla股價現神奇反彈！為何？一架車淨利潤抵豐田八架、更超越Benz！介紹Tesla賺大錢秘訣！是否無人能挑戰其地位？")

QEU:FOUNDER ： “本来は多様性のあった経済構造を自動車だけの一本足打法にして、それでコケるんだから恣意的といわれてもしょうがないね。そういえば、A国のT社の1台当たりの儲けは、J国のT社の8倍らしいね。”

