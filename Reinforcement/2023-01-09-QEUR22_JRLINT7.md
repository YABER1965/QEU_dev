---
title: QEUR22_JRLINT7:　閑話休題～GridWorlds.jlをReinforcementLearning.jl上で使う
date: 2023-01-09
tags: ["QEUシステム", "メトリックス", "Julia言語", "機械学習", "Flux", "ディープラーニング", "強化学習"]
excerpt: Julia言語を使った強化学習
---

## QEUR22_JRLINT7:　閑話休題～GridWorlds.jlをReinforcementLearning.jl上で使う

## ～　結局、わかんないなぁ・・・　～

### ・・・　前回のつづきです　・・・

D先生 ： “えっ！？（Cliff_Walkingの開発）を続けてやらないんですか？あと、この強化学習事例(↓)では、たった2行の命令で実行できるのですか？”

![imageJRL1-8-1](/2023-01-09-QEUR22_JRLINT7/imageJRL1-8-1.jpg)

QEU:FOUNDER ： “まあ、その件は後で・・・。さて、RL.jl用のメイズ(迷路)系のツール・パッケージがあるので、まずはそれを使ってみようと思います。世の中、回り道が近道になることもあります。”

<iframe width="560" height="315" src="https://www.youtube.com/embed/nL7f68DCQwg" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyro-scope; picture-in-picture; web-share" allowfullscreen></iframe>

D先生 ： “そんなもんですかねえ・・・。”

QEU:FOUNDER ： “まあ、小生に任せてくださいね・・・(笑)。テーマが2つありますね。どれからやろうかな・・・。GridWorldsの紹介をやりましょう。これ（↓）です。REPLというJulia用のターミナル上で動作する環境です。例のハッカーさんも一枚かんでいます。”

![imageJRL1-8-2](/2023-01-09-QEUR22_JRLINT7/imageJRL1-8-2.jpg)

QEU:FOUNDER ： “まず最初に、GridWorldsの手引き(REDME)の事例の通りに命令を操作すると動きがわかりやすいです。だから、小生としては、ちょっと考え方を変えて、強化学習上でどのように使われるのかを見てみましょう。このようにパッケージの使用事例はつまらないがブログで誰かが紹介することは必要です。以前、Rachel Thomasさんが、ブログを書くことの有用性について強調しているように、素人の良いところは開発者よりも良い着眼点でマニュアルを書くことができることなんです。”

![imageJRL1-8-3](/2023-01-09-QEUR22_JRLINT7/imageJRL1-8-3.jpg)

QEU:FOUNDER ： “これはJupyter上で動かしたものです。GridWorlds.jlでは、「state-action-rewards-done」をどのように扱っているのかがよくわかります。”

```julia
# GridWorlds for Reinforcement Learning
# use GridWorlds.jl
import GridWorlds as GW

# Each environment `Env` lives in its own module `EnvModule`
# For example, the `SingleRoomUndirected` environment lives inside the `SingleRoomUndirectedModule` module
env = GW.SingleRoomUndirectedModule.SingleRoomUndirected()

# use the RLBase API
import ReinforcementLearningBase as RLBase

# wrap a game instance from this package to create an RLBase compatible environment
rlbase_env = GW.RLBaseEnv(env)

# perform RLBase operations on the wrapped environment
RLBase.reset!(rlbase_env)

##### STATE, REWARD AND DONE BY PC(reset) ####
# display <state>
state = RLBase.state(rlbase_env)
```

![imageJRL1-8-4](/2023-01-09-QEUR22_JRLINT7/imageJRL1-8-4.jpg)

```julia
# get names of objects in this environment
GW.get_object_names(env)

# display <reward>
reward = RLBase.reward(rlbase_env)
# display <done>
done = RLBase.is_terminated(rlbase_env)
println("REWARD:",reward, ", DONE:",done)

##### TAKE AN ACTION by USER(1st) ######
# get names of actions that can be performed in this environment
GW.get_action_names(env)
```

![imageJRL1-8-5](/2023-01-09-QEUR22_JRLINT7/imageJRL1-8-5.jpg)

QEU:FOUNDER ： “Actionには特に注意したほう方がいいです。強化学習でENVにACTIONをかける前にACTIONの種類とINDEXを理解しておく必要があります。この例では、INDEXは「UP-DOWN-LEFT-RIGHT」の順番ですね。このINDEXを入力して、ACTIONを入力するんです。”

```julia
# take an <action>
rlbase_env(1) # move up

##### STATE, REWARD AND DONE BY PC(turn1) ####
# display <state>
state = RLBase.state(rlbase_env)
```

![imageJRL1-8-6](/2023-01-09-QEUR22_JRLINT7/imageJRL1-8-6.jpg)

```julia
# display <reward>
reward = RLBase.reward(rlbase_env)
# display <done>
done = RLBase.is_terminated(rlbase_env)
println("REWARD:",reward, ", DONE:",done)
```

D先生 ： “この場合のSTATEは「AGENT-WALL-GOAL」のレイヤ構造になっているんですね。なるほど・・・。ここまではわかりました。”

QEU:FOUNDER ： “じゃあ、次のテーマにいくよ・・・。ReinforcementLearning.jlでは、たった2行の命令文で強化学習を実行できるかをやってみましょう。”

![imageJRL1-8-7](/2023-01-09-QEUR22_JRLINT7/imageJRL1-8-7.jpg)

D先生 ： “あれ？失敗しちゃった・・・。”

QEU:FOUNDER  ： “あれ？おかしいな？おや？、Experimentsを読み込めていないんですよね。本来は、ReinforcementLearning.jlにはいくつかの事例群(↑)が内蔵されており、簡単なタイトル入力だけで実行できるはずなんだが・・・。まあ、この件は大した話じゃないので、これ以上突っ込みません。ここで重要なことをいいかす。このExperiment文というのはとても作業効率化のツールなんです。D先生、例えば先生が現場でREPLを使って強化学習を使うと考えてください。”

![imageJRL1-8-8](/2023-01-09-QEUR22_JRLINT7/imageJRL1-8-8.jpg)

D先生 ： “なるほど！！Experiment関数の引数と(ex)タイトル名が対応しているのか！！この機能を活用すれば、参照できる場所にたくさんの関数を置いておいて、ユーザーが実行したい関数をREPL上で簡単に呼び出すことができますね。”

QEU:FOUNDER ： “このアイデアって、とても現場的・・・。いいでしょう？最後に、GridWorld.jlを使った強化学習の事例を見てみましょう。これもExperimentsの中に紹介されています。”

```julia
using ReinforcementLearning
using GridWorlds
using StableRNGs
using Flux
using Flux.Losses

function RL.Experiment(
    ::Val{:JuliaRL},
    ::Val{:BasicDQN},
    ::Val{:SingleRoomUndirected},
    ::Nothing;
    seed=123,
)
    rng = StableRNG(seed)

    env = GridWorlds.SingleRoomUndirectedModule.SingleRoomUndirected(rng=rng)
    env = GridWorlds.RLBaseEnv(env)
    env = RLEnvs.StateTransformedEnv(env; state_mapping=x -> vec(Float32.(x)))
    env = RewardTransformedEnv(env; reward_mapping = x -> x - convert(typeof(x), 0.01))
    env = MaxTimeoutEnv(env, 240)

    ns, na = length(state(env)), length(action_space(env))
    agent = Agent(
        policy=QBasedPolicy(
            learner=BasicDQNLearner(
                approximator=NeuralNetworkApproximator(
                    model=Chain(
                        Dense(ns, 128, relu; init=glorot_uniform(rng)),
                        Dense(128, 128, relu; init=glorot_uniform(rng)),
                        Dense(128, na; init=glorot_uniform(rng)),
                    ) |> cpu,
                    optimizer=ADAM(),
                ),
                batch_size=32,
                min_replay_history=100,
                loss_func=huber_loss,
                rng=rng,
            ),
            explorer=EpsilonGreedyExplorer(
                kind=:exp,
                ϵ_stable=0.01,
                decay_steps=500,
                rng=rng,
            ),
        ),
        trajectory=CircularArraySARTTrajectory(
            capacity=1000,
            state=Vector{Float32} => (ns,),
        ),
    )

    stop_condition = StopAfterStep(10_000, is_show_progress=!haskey(ENV, "CI"))
    hook = TotalRewardPerEpisode()
    Experiment(agent, env, stop_condition, hook, "")
end


# -----
using Plots
ex = E`JuliaRL_BasicDQN_SingleRoomUndirected`
run(ex)
plot(ex.hook.rewards)
```

![imageJRL1-8-9](/2023-01-09-QEUR22_JRLINT7/imageJRL1-8-9.jpg)

QEU:FOUNDER ： “めでたく、GridWorldsでもディープ強化学習ができましたよ。”

```julia
    env = GridWorlds.SingleRoomUndirectedModule.SingleRoomUndirected(rng=rng)
    env = GridWorlds.RLBaseEnv(env)
    env = RLEnvs.StateTransformedEnv(env; state_mapping=x -> vec(Float32.(x)))
    env = RewardTransformedEnv(env; reward_mapping = x -> x - convert(typeof(x), 0.01))
    env = MaxTimeoutEnv(env, 240)
```

D先生 ： “あら～、RL.jlにはワンホット(onehot)に変換できる機能があるんだ。さすが大ハッカー様・・・。でも、学習曲線はこんな感じななんですね。へんなの・・・。”

QEU:FOUNDER ： “これはゲーム後の報酬の履歴を見ているのであって、ディープラーニング関数の損失量推移をみているわけではないです。それでも、小生もちょっとヘンだとは思います。”

D先生 ： “でも、我々が前回にCliff_Walkingで行ったやり方は正しいことは分かりましたね。もちろん、バグの有無は分からないですが・・・。次は、どうしますか？”

QEU:FOUNDER ： “ちょっと、考え中・・・。”



## ～　まとめ　～


QEU:FOUNDER ： “この動画（↓）は結構おもしろいですよ。モノづくり、とかインバウンドとかの失笑ワードは散見されるが・・・。”

<iframe width="560" height="315" src="https://www.youtube.com/embed/5fhNArCQAUA" ti-tle="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; en-crypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

C部長 : “モノづくり、とかインバウンドとかダメですが？”

QEU:FOUNDER ： “残念ながら2013年時点でインバウンドは大爆笑ものでしょうね。もしうまく行ったらスマン（笑）。あと、モノづくりって、まだ夢見てるの！？J国がものづくり大国であったのはずっと前でしょう？その証拠をドン！！。”

<iframe width="560" height="315" src="https://www.youtube.com/embed/aNArM2EwhWE" ti-tle="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; en-crypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

C部長 : “うわぁ。おもしろい品がたくさん！！”

QEU:FOUNDER ： “すごいだろう！すごい**アイデアが詰まった商品**がたくさん、た～くさん、**「とあるところに埋まって」**いる・・・。”

C部長 : “とあるところ？どこに？”

![imageJRL1-8-10](/2023-01-09-QEUR22_JRLINT7/imageJRL1-8-10.jpg)

QEU:FOUNDER ： “ココ（↑）です。**C国を代表する「B to Cプラットフォーム」**の中に・・・。C国では**製造工場と消費者が直接つながる**ことによって、商品の創造力が大きく伸びたわけです。おそらく、C国で生まれたアイデア商品の多くはJ国の100円ショップで売られているんだと思うよ。”

C部長 : “100円ショップって、もうちょっとASEANシフトをすると思ったのですが、まだ製造移転はしていないですね。ひょっとして、アイデア商品類はC国製が多いのでできないのか・・・。”

QEU:FOUNDER ： “100円ショップ業界の詳細は分からないので、これはあくまで私見ということで・・・。このように、Y先生のいう「創発価値」、その製造装置(platform)を世界で初めて手に入れたのがC国・・・。J国はサプライチェーンにDONKIとかTOPVALUEとかが介在しているので、品質の良いモノは作れるが、創造力を爆発させる機能はどうかなと・・・。”

C部長 : “J国は、あくまで**「すりあわせ、ファインチューニング」の国**ですから・・・。”

QEU:FOUNDER ： “そのかわり、C国は、このプラットフォームのために**量販店、小売店が全国的に壊滅**しました。結局は「良しあしの問題」なのかもしれない。ただし、小生はC国はものづくり大国でありつづけると思うよ。”

C部長 : “C国は、いまだに周りからたたかれていますが・・・。”

![imageJRL1-8-11](/2023-01-09-QEUR22_JRLINT7/imageJRL1-8-11.jpg)

QEU:FOUNDER ： “C国は、その持ち前の創造力で、その圧力を跳ね返すんです。昔のJ国みたいに・・・。今までの議論をかんがみて、動画を見ると・・・失笑・・・・。”

