---
title: QEUR22_withT2S5:　簡単なカタストロフ（その1-A）
date: 2023-03-11
tags: ["QEUシステム", "メトリックス", "Python言語", "T法(2)", "REWARD", "強化学習"]
excerpt: T法(2)をテクノメトリックスとして使った強化学習
---

## QEUR22_withT2S5:　簡単なカタストロフ（その1-A）

## ～　これは「カタストロフ」じゃないよ！　～

QEU:FOUNDER ： “それでは、**強化学習で「カタストロフをシミュレート」しましょう**。・・・というのは無しね・・・(笑)。今回は、この若き大先生（↓）の講義を使って、**ノン・カタストロフ**をやりましょう。”

[![MOVIE1](http://img.youtube.com/vi/bD6V3rcr_54/0.jpg)](http://www.youtube.com/watch?v=bD6V3rcr_54 "Building a Custom Environment for Deep Reinforcement Learning with OpenAI Gym and Python")

D先生 ： “これは**「空調のバイメタル制御」みたいなもん**じゃないですか・・・。”

QEU:FOUNDER ： “今回、この事例を敢えてする理由は２つ・・・。一つは、**ベースライン**としてノンカタストロフについて理解しておきたい。あとは、OpenAI-Gymにおいて、**カスタム環境を構築する**こと・・・。”

D先生 ： “そもそもノン・カタストロフとは？”

QEU:FOUNDER ： “安定した状態が１つしかないから、**「これはノンカタストロフと考えました」**でえす。”

![imageJRL2-6-1](/2023-03-11-QEUR22_withT2S5/imageJRL2-6-1.jpg)

D先生 ： “Gymの環境には**2つの安定状態がある**場合が多いですからね。”

QEU:FOUNDER ： “まあ今回はウォーミング・アップですので、なんでもいいんです。それではプログラムをドン！！”

```python
# -----
# Test Random Environment with OpenAI Gym
from gym import Env
from gym.spaces import Discrete, Box
import numpy as np
import random
import matplotlib.pyplot as plt

class ShowerEnv(Env):
    def __init__(self):
        # Actions we can take, down, stay, up
        self.action_space = Discrete(3)
        # Temperature array
        self.observation_space = Box(low=np.array([0]), high=np.array([100]))
        # Set start temp
        self.state = random.choice([25,30,40,45])
        # Set shower length
        self.shower_length = 80
        # Set confort counter
        self.icnt_confort = 0
        # Set array of state and tscore
        self.arr_state  = []
        self.arr_tscore = []
        # Set total score
        self.total_score = 0
        
    def step(self, action):
        # Apply action
        # 0 -1 = -1 temperature
        # 1 -1 = 0 
        # 2 -1 = 1 temperature 
        self.state += action -1 
        # Reduce shower length by 1 second
        self.shower_length -= 1 
        
        # Calculate reward
        if self.state >=37 and self.state <=39: 
            reward = 1.0 
            self.icnt_confort += 1
        else: 
            reward = -1.0
        
        # Check if shower is done
        if self.shower_length <= 0: 
            done = True
        elif self.icnt_confort > 30:
            done = True
            reward = self.shower_length
        else:
            done = False
            
        # Calc total score
        self.total_score += reward
        
        # Calc total score
        self.arr_state.append(self.state)
        self.arr_tscore.append(self.total_score)

        # Apply temperature noise
        self.state += random.choice([-1,0,1])
        
        # Set placeholder for info
        info = {}
        
        # Return step information
        return self.state, reward, done, info

    def render(self, mode):
        # Implement vizual
        # Set start temp
        if self.shower_length < 80 and self.shower_length%20 == 0:
            plt.plot(self.arr_state)
            plt.plot(self.arr_tscore)
            plt.show()
        print("shower_length:{} ,state:{} ,icnt_confort:{} ".format(self.shower_length,self.state,self.icnt_confort))
    
    def reset(self):
        # Reset shower temperature
        self.state = random.choice([25,30,40,45])
        # Reset shower time
        self.shower_length = 80 
        # Reset confort counter
        self.icnt_confort = 0
        # Reset array of state and tscore
        self.arr_state  = []
        self.arr_tscore = []
        # Reset total score
        self.total_score = 0
        
        return self.state
    
env = ShowerEnv()
#env.observation_space.sample()

# -----
# Ramdom test
episodes = 10
for episode in range(1, episodes+1):
    state = env.reset()
    done = False
    score = 0 
    
    while not done:
        #env.render("human")
        action = env.action_space.sample()
        n_state, reward, done, info = env.step(action)
        score+=reward
    print('Episode:{} Score:{}'.format(episode, score))
```

![imageJRL2-6-2](/2023-03-11-QEUR22_withT2S5/imageJRL2-6-2.jpg)

QEU:FOUNDER ： “今回の場合、**実験環境を大先生のロジックから少しだけ変えております**。大先生の環境は、わずかな温度の揺らぎを目標範囲にコントロールすることを目的としています。我々の場合には、**初期の温度を目標制御範囲から大きく外しています**。そして、速く目標範囲に安定させるとボーナスが多くもらえます。”

```python
# -----
# Ramdom test
episodes = 1
for episode in range(1, episodes+1):
    state = env.reset()
    done = False
    score = 0 
    
    while not done:
        env.render("human")
        action = env.action_space.sample()
        n_state, reward, done, info = env.step(action)
        score+=reward
    print('Episode:{} Score:{}'.format(episode, score))

```

![imageJRL2-6-3](/2023-03-11-QEUR22_withT2S5/imageJRL2-6-3.jpg)

D先生 ： “あとは、今回の場合は**レンダリング機能でグラフを見る**ことができますね。これは便利だ・・・。これによると、学習をしていない状態では全然ダメダメです。”

QEU:FOUNDER ： “それでは、いよいよ学習させます。”

```python
# -----
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' 
# -----
# Create a Deep Learning Model with Keras
import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.optimizers import Adam

states = env.observation_space.shape
actions = env.action_space.n
print(actions)
```

![imageJRL2-6-4](/2023-03-11-QEUR22_withT2S5/imageJRL2-6-4.jpg)

```python
# -----
# 3. Build Agent with Keras-RL
from rl.agents import DQNAgent
from rl.policy import BoltzmannQPolicy
from rl.memory import SequentialMemory

def build_model(states, actions):
    model = Sequential()    
    model.add(Dense(64, activation='relu', input_shape=states))
    model.add(Dense(64, activation='relu'))
    model.add(Dense(actions, activation='linear'))
    return model

model = build_model(states, actions)
model.summary()

# -----
def build_agent(model, actions):
    policy = BoltzmannQPolicy()
    memory = SequentialMemory(limit=5000, window_length=1)
    dqn = DQNAgent(model=model, memory=memory, policy=policy, 
                  nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)
    return dqn

dqn = build_agent(model, actions)
dqn.compile(Adam(learning_rate=1e-3), metrics=['mae'])
dqn.fit(env, nb_steps=20000, visualize=False, verbose=1)

```

![imageJRL2-6-5](/2023-03-11-QEUR22_withT2S5/imageJRL2-6-5.jpg)

QEU:FOUNDER ： “ここで注意を一つ・・・。小生、**Keras-rl**について全く知らないです。”

D先生 ： “それで、よくリリースしましたねえ。それにしても、こんな簡単なケースでも5万回も学習するのか・・・。”

QEU:FOUNDER ： “たまたま動いたんで、（説明なしで）いいでしょう・・・。基本、大先生と同じロジックなんで・・・。もし、わからなかったら自分でググるか、大先生に聞いてください。それでは、強化学習の結果をみてみましょう。”

```python
# -----
scores = dqn.test(env, nb_episodes=100, visualize=False)
print(np.mean(scores.history['episode_reward']))

```

![imageJRL2-6-6](/2023-03-11-QEUR22_withT2S5/imageJRL2-6-6.jpg)

```python
# -----
scores = dqn.test(env, nb_episodes=1, visualize=True)

```

![imageJRL2-6-7](/2023-03-11-QEUR22_withT2S5/imageJRL2-6-7.jpg)

QEU:FOUNDER ： “まあ、結果としてはうまくいっているでしょ？”

D先生 ： “まあね・・・。”

QEU:FOUNDER ： “実は環境をカスタム化しないと、今後のカタストロフ版の実験はできないんです。”

D先生  ： “「我々の場合は」でしょ？**T法(2)を使う**から・・・。”

QEU:FOUNDER ： “そういうこと・・・。良い機会なので、これをいいましょ。”

### [＞寄付のお願い(donate)＜](https://www.paypal.com/paypalme/QEUglobal?v=1&utm_source=unp&utm_medium=email&utm_campaign=RT000481&utm_unptid=29844400-7613-11ec-ac72-3cfdfef0498d&ppid=RT000481&cnac=HK&rsta=en_GB%28en-HK%29&cust=5QPFDMW9B2T7Q&unptid=29844400-7613-11ec-ac72-3cfdfef0498d&calc=f860991d89600&unp_tpcid=ppme-social-business-profile-creat-ed&page=main%3Aemail%3ART000481&pgrp=main%3Aemail&e=cl&mchn=em&s=ci&mail=sys&appVersion=1.71.0&xt=104038)

QEU:FOUNDER ： “どうぞ、よろしく。”


## ～　まとめ　～

C部長 : “（ひそひそ話）知ってます？これ（↓）・・・。”

[![MOVIE2](http://img.youtube.com/vi/12ddOpt7Hio/0.jpg)](http://www.youtube.com/watch?v=12ddOpt7Hio "How The Japanese Economic Miracle Led to Lost Decades.")

QEU:FOUNDER ： “ああ・・・、すでに見たよ・・・。まあ、小生としては新鮮なモノはないですよ、**「よくある」件**です。例えば、こんな感じ（↓）の問題が原因の一つでしょう？”

[![MOVIE3](/2023-03-11-QEUR22_withT2S5/imageJRL2-6-8.jpg)](https://imaple8.co/play/266404-3-1.html "Please Click to view the movie")

C部長 : “おおっ、これは・・・！”

### オッサン（＠車中、N社検査不正について）：　「“検査不正”っていうのはなァ、（組織外に不正を）漏らしたヤツが悪いんだよ・・・」

QEU:FOUNDER ： “**「見た目はよいが、実は中は・・・」って、よくある話**じゃない？”

C部長 : “それ、ウチ（会社）のことを言ってます？”
