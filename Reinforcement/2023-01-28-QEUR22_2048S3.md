---
title: QEUR22_2048S3:　ゲーム2048で遊ぼう（その4：Julia編-newRT vs SOART）
date: 2023-01-28
tags: ["QEUシステム", "メトリックス", "Julia言語", "2048", "SOART", "ディープラーニング", "強化学習"]
excerpt: julia言語とテクノメトリックスを使った強化学習
---

## QEUR22_2048S3:　ゲーム2048で遊ぼう（その4：Julia編-newRT vs SOART）

## ～　大失敗!!　しかし・・・。　～

QEU:FOUNDER(設定年齢65歳) ： “いよいよ、SOARTによる強化学習だ！！夢はchatGPT・・・!!　これこそ**「高齢者によるイノベーション」**！！”

[![MOVIE1](http://img.youtube.com/vi/kDQNAqNPJYc/0.jpg)](http://www.youtube.com/watch?v=kDQNAqNPJYc "ChatGPT最大對手出現！google Sparrow將登場！號稱學習能力更佳！藝術創作功能將取代人腦創作造成結構性失業！中美AI對話內容讓人驚訝！")

D先生(設定年齢65歳)  ： “それは本番組のタイトルですね。しかし、A国だけじゃなく、C国にもchatGPTに相当するモノがあるんですね。最近、彼らが「チャット」したらしいし・・・。”

![imageJRL1-23-1](/2023-01-28-QEUR22_2048S3/imageJRL1-23-1.jpg)

QEU:FOUNDER： “ソフトウェアでしょ？C国はA国と実力はあまりかわらないって・・・。例えば、見てごらん？Twxxx社のビフォー＆アフターを・・・。”

![imageJRL1-23-2](/2023-01-28-QEUR22_2048S3/imageJRL1-23-2.jpg)


D先生  ： “あ～あ・・・。こうなっちゃいましたか・・・(笑)。すいませ～ん、質問があります。**SOART法って、なんでした**っけ・・・(笑)？”

![imageJRL1-23-3](/2023-01-28-QEUR22_2048S3/imageJRL1-23-3.jpg)

QEU:FOUNDER ： “おっ、そう来たか・・・。タグチ先生の提唱したRT法って、ユーグリッド距離による判別法にちょっと**「スパイスを付け加えた」**だけのものです。ちなみに、我々はnewRT法としてユーグリッド距離のかわりにマンハッタン距離を使うけどね。判別精度を上げるために・・・。そして、RT法の「スパイス」って、ユーグリッド距離を測る前に、2つの「画像（ベクトル）」を最適に回転させることなんです。これが、タグチ先生のいう「感度（β）」です。でもね・・・、少なくとも画像の場合には、回転度ってどれだけ有用な判別尺度なのか？”

![imageJRL1-23-4](/2023-01-28-QEUR22_2048S3/imageJRL1-23-4.jpg)

QEU:FOUNDER ： “画像を比較するための要素には、他にも**明るさもある**し、大きさもある。例えば、ユーグリッド距離で比較すると、画像の明るさが変わると距離にモロに影響が出ます。”

D先生  ： “SOART法の場合には、明るさ補正も目指したわけですね。”

![imageJRL1-23-5](/2023-01-28-QEUR22_2048S3/imageJRL1-23-5.jpg)

QEU:FOUNDER ： “明るさだけじゃないよ。SOART法は**マルチ（多段階）法がベース**になっています。まずは明るさを補正してメトリックスを出力し、そのメトリックスを回転補正をして判別距離を計算します。さらに、SOART法では種々のパターンの畳み込み部品（↑）を使用します。”

D先生  ： “あれ？**畳み込み部品の値が変わりました**ね。”

QEU:FOUNDER ： “角部の値を大きくして、部品により畳み込み値の差が出やすくしました。これ以上のSOART法の説明は、以前ブログを書いたので読んでください。なによりも、プログラムを読むのが最もよいです。それではプログラムをドン！！いっとくけど、この結果は「大失敗」だよ！！”

```julia
# ------
# テクノ・メトリックスの可能性を試すシリーズ
# Julia - SOARTメトリックス@Game2048の環境設定
# ------
using DataFrames, CSV
using LinearAlgebra
using Distances
using Plots
using Statistics
# ----
# Read the file using CSV.File and convert it to DataFrame
# ベンド系部品
# -- bend1 --
PATH_bend1 = "./conv_parts/bend1_cnv.csv"
df_bend1 = DataFrame(CSV.File(PATH_bend1; header=false))
conv_bend1 = Matrix(df_bend1)
conv_bend1 = convert(Matrix{Float64},conv_bend1)
println("--- conv_bend1 ---")
println(conv_bend1)
# -- bend2 --
PATH_bend2 = "./conv_parts/bend2_cnv.csv"
df_bend2 = DataFrame(CSV.File(PATH_bend2; header=false))
conv_bend2 = Matrix(df_bend2)
conv_bend2 = convert(Matrix{Float64},conv_bend2)
println("--- conv_bend2 ---")
println(conv_bend2)
# -- bend3 --
PATH_bend3 = "./conv_parts/bend3_cnv.csv"
df_bend3 = DataFrame(CSV.File(PATH_bend3; header=false))
conv_bend3 = Matrix(df_bend3)
conv_bend3 = convert(Matrix{Float64},conv_bend3)
println("--- conv_bend3 ---")
println(conv_bend3)
# -- bend4 --
PATH_bend4 = "./conv_parts/bend4_cnv.csv"
df_bend4 = DataFrame(CSV.File(PATH_bend4; header=false))
conv_bend4 = Matrix(df_bend4)
conv_bend4 = convert(Matrix{Float64},conv_bend4)
println("--- conv_bend4 ---")
println(conv_bend4)
# ライン系部品
# -- line1 --
PATH_line1 = "./conv_parts/line1_cnv.csv"
df_line1 = DataFrame(CSV.File(PATH_line1; header=false))
conv_line1 = Matrix(df_line1)
conv_line1 = convert(Matrix{Float64},conv_line1)
println("--- conv_line1 ---")
println(conv_line1)
# -- line2 --
PATH_line2 = "./conv_parts/line2_cnv.csv"
df_line2 = DataFrame(CSV.File(PATH_line2; header=false))
conv_line2 = Matrix(df_line2)
conv_line2 = convert(Matrix{Float64},conv_line2)
println("--- conv_line2 ---")
println(conv_line2)
# クロス系部品
# -- cross --
PATH_cross = "./conv_parts/datum_cnv.csv"
df_cross = DataFrame(CSV.File(PATH_cross; header=false))
conv_cross = Matrix(df_cross)
conv_cross = convert(Matrix{Float64},conv_cross)
println("--- conv_cross ---")
println(conv_cross)
# ------
# 畳み込み部品をまとめる
mx_cvparts = []
push!(mx_cvparts, conv_bend1)
push!(mx_cvparts, conv_bend2)
push!(mx_cvparts, conv_bend3)
push!(mx_cvparts, conv_bend4)
push!(mx_cvparts, conv_line1)
push!(mx_cvparts, conv_line2)
# ------
println(mx_cvparts)
println(size(mx_cvparts))

# =================================================
# テクノメトリックスのベクトルを計算する
# =================================================
# SOARTパラメータの設定
num_rtm   = 6        # bend x4, line x2
conv_dim  = 3
rtm_dim   = 2
start_cutxs = [1,2]     # rtm_dim
start_cutys = [1,2]     # rtm_dim

# -----
# 畳み込みマトリックスを計算する関数
function convmx_arrays(arr_conv_parts, mx_base)
    # ------
    # 畳み込み行列の初期化
    mx_meas = zeros((rtm_dim, rtm_dim))
    # ------
    # 各ブロックのメトリックスを計算する
    for i in start_cutys        # start_cutys
        for j in start_cutxs    # start_cutxs  
            #println(mx_base[start_cutxs[i]:start_cutxs[i]+conv_dim-1, start_cutys[j]:start_cutys[j]+conv_dim-1])
            arr_mx_base = vec(mx_base[start_cutxs[i]:start_cutxs[i]+conv_dim-1, start_cutys[j]:start_cutys[j]+conv_dim-1])
            dot_value   = dot(arr_conv_parts, arr_mx_base)
            mx_meas[i,j] = dot_value
        end
    end
    #println("mx_meas: ", mx_meas)
    return mx_meas
end

# -----
# テクノ・メトリックスを計算する関数
function calc_techmetrics(conv_cross, mx_cvparts, mx_base)

    # クロス系畳み込みマトリックスを計算する
    mx_cvbase = convmx_arrays(vec(conv_cross), mx_base)
    #println("mx_cvbase: ",mx_cvbase)
    # ------
    # 新RTメトリックスの計算用(STEP1)
    xx = dot(vec(mx_cvbase),vec(mx_cvbase))
    #xx = sum(vec(mx_cvbase))
    # ------
    if xx > 0.00001
        # ------
        arr_beta     = []
        arr_distance = []
        for k in 1:num_rtm
            # ------
            # 計測用畳み込みマトリックスを計算する
            mx_cvmeas = convmx_arrays(vec(mx_cvparts[k]), mx_base)
            # ------
            # 新RTメトリックスの計算用(STEP1)
            xy = dot(vec(mx_cvbase),vec(mx_cvmeas))
            #xy = sum(vec(mx_cvmeas))
            beta = round(xy/xx; digits=5)
            push!(arr_beta, beta)
            #println("beta: ",beta)
            # ------
            # テクノ・メトリックスの計算用(STEP2)
            mDistance = round(cityblock(vec(mx_cvmeas), vec(beta*mx_cvbase)); digits=5)
            push!(arr_distance, mDistance)
            #println("mDistance: ", mDistance)
        end
        arr_beta      = convert(Array{Float64},arr_beta)
        arr_distance  = convert(Array{Float64},arr_distance)
    else
        arr_beta      = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
        arr_distance  = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
        #println("xx_value less than 0")
    end
    return arr_beta, arr_distance
end

# =================================================
# 2048ゲームを実行するための関数群
# =================================================
using StatsBase

# グローバル宣言
const DIRS = [:left, :up, :right, :down]
rand2_1()  = rand() < 0.1 ? 2 : 1

function init_game()
    grid = zeros(Int8,4,4)    
    grid[rand(1:4),rand(1:4)] = rand2_1()
    grid[rand(1:4),rand(1:4)] = rand2_1()
    return grid
end

# a function to simulate the move and return a reward
function move!(x, xinc, xstart, xend)
    reward = 0
    @inbounds for i = xstart:xinc:xend # for each row move the left most piece first #if move_row = 1 then i control the row
        if x[i] != 0 # if the position is occupied by a number move it
            # firstly look "behind" to see if there is a number that is the same
            # this is to deal better with situations like 2 2 4 4
            @inbounds for k = i+xinc:xinc:xend
                if x[k] != 0
                    if x[k] == x[i]
                        x[i] += 1
                        reward += 2^x[i]
                        x[k] = 0
                    end
                    break;
                end
            end
            # now place it in the first empty slot
            @inbounds for k = xstart:xinc:i
                if x[k] == 0
                    x[k] = x[i]
                    x[i] = 0
                end
            end
        end
    end
    (x, reward)
end

function move_left!(x)
    move!(x, 1, 1, 4)
end

function move_right!(x)
    move!(x, -1, 4, 1)
end

function move!(grid::Array{T,2}, direction) where T <: Integer
    reward::Int16 = zero(Int16)
    if direction == :left
        for j = 1:4
            (tmp, new_reward) = move_left!(@view grid[j,:])
            reward += new_reward
        end
    elseif direction == :right
        for j = 1:4
            (tmp, new_reward) = move_right!(@view grid[j,:])
            reward += new_reward
        end
    elseif direction == :up
        for j = 1:4
            (tmp, new_reward) = move_left!(@view grid[:,j])
            reward += new_reward
        end
    else
        for j = 1:4
            (tmp, new_reward) = move_right!(@view grid[:,j])
            reward += new_reward
        end
    end
    (grid, reward)
end

# -------
# SIMULATION - TILE MOVE
# -------
# assume no need to check for validate moves
function simulate_move!(grid, direction)
    tmp_grid = copy(grid)
    (grid, reward) = move!(grid, direction)
    if all(tmp_grid .== grid)
        return (grid, false, CartesianIndex{2}(-1,-1), -1, 0)
    else
        cart = rand(findall(grid .== 0)) # randomly choose one empty slot
        one_or_two = rand2_1()
        grid[cart] = one_or_two
        return (grid, true, cart, one_or_two, reward)
    end
end

# =================================================
# TRIAL【2】_Game2048_DQN_TechnoMetricsRT_with_FEEDBACK
# =================================================
using Flux
using Flux: mse, huber_loss, onehotbatch
using Random

# ---------------------------
# グローバル宣言
global num_state
global num_actions 
global in_dim 
global out_dim

# ---------------------------
# Defining all the required parameters
global memory
global ε
global γ 
global e_decay  
global e_min   
global learning_rate
global total_episodes 

# =================================================
#    Args:
#        ε: The degree of exploration
#        γ: The discount factor
#        num_state: The number of states
#        num_actions: The number of actions
#        action_space: To call the random action
# =================================================
num_state     = 12
num_actions   = 4
in_dim  = num_state + num_actions
out_dim = 1

# Defining all the required parameters
ε     = 0.95
γ     = 0.99
e_decay    = 0.9999
e_min      = 0.01
total_episodes = 20000

# ---------------------------
# ハイパーパラメタ
const BATCH_SIZE    = 128  # サンプルサイズ
const MEMORY_CAPACITY = 128 * 32  # メモリ容量

# =================================================
# AGENT functions
# =================================================
# リセット：基本変数を初期化する
function reset!()
    # ---------------------------
    # ゲームの初期化(2タイルが２か所)
    grid = init_game()
    # ---------------------------
    arr_acType    = []
    arr_action    = []
    arr_Qvalue    = []
    arr_reward    = []
    mx_states     = []
    return grid, arr_acType, arr_action, arr_Qvalue, arr_reward, mx_states
end

# ----------------
# 命令を選択し、ゲームを動かす(ランダム判定)
function step_random!(grid) 
    Qvalue = -0.0001
    acType = "random"
    directions = sample(DIRS, 4 , replace = false)
    flg_moved = false     # falseとは「ゲーム盤が動かない」という意味
    for i in 1:3
        d1 = directions[i]
        (grid, ok, cart, two_or_four, reward) = simulate_move!(grid, d1)
        if ok
            flg_moved = true     # trueとは「動いた」という意味
            return (d1, acType, Qvalue, grid, reward, flg_moved)
        end
    end
    (grid, ok, cart, two_or_four, reward) = simulate_move!(grid, directions[4])
    return (directions[4], acType, Qvalue, grid, reward, flg_moved)
end

# ----------------
# 命令を選択し、ゲームを動かす(マシン判定)
function step_machine!(grid, state_cur, iCnt_play, Qvalue)
    # 最適命令を選択する(DQN)
    flg_moved = true         # falseとは「ゲーム盤が動かない」という意味
    acType    = "machine"
    mx_input  = zeros(4,in_dim)
    #println(mx_input)
    for action in 1:4
        # [array]-action を生成します。
        a_onehot = onehotbatch(action, 1:4)
        temp_s_a = vcat(state_cur, a_onehot)
        #println("temp_s_a-choose action: ", temp_s_a)
        mx_input[action,:] = temp_s_a'     # 状態(S)行動(A)マトリックス
    end
    #println("----- mx_input(choose_action: machine) -----")
    #println(mx_input)
    # ----------------
    # predict 'y'
    y_pred  = model(mx_input')
    Qvalue  = maximum(y_pred)
    a_order = argmax(y_pred)[2]
    action  = DIRS[a_order]
    # ----------------
    # ゲーム盤を動かす
    (grid, flg_moved, cart, two_or_four, reward) = simulate_move!(grid, action)

    return action, acType, round(Qvalue; digits=5), grid, reward, flg_moved
end

# ----------------
# ゲームオーバーしているか？
function is_gameover!(grid) 
    flg_gameover = true      # trueとは「ゲームオーバーである」という意味
    for i in 1:4
        d1 = DIRS[i]
        (grid, ok, cart, two_or_four, reward) = simulate_move!(grid, d1)
        if ok
            flg_gameover = false     # falseとは「ゲームオーバーではない」という意味
            return flg_gameover
        end
    end
    return flg_gameover
end

# =================================================
# DQN_Solver functions
# =================================================
# crate instance for input
global model       = Chain(Dense(in_dim, 256, relu), Dense(256, 256, relu), Dense(256, out_dim)) # Use Chain if you want to stack layers
# パラメタ
global ps_model    = params(model)
# -----
# 損失関数
#global loss(x, y) = mse(model(x'), Matrix(y'))            # 二乗平均誤差
global loss(x, y)  = huber_loss(model(x'), Matrix(y'))    # フーバー誤差
# -----
# オプティマイザ
global opt         = ADAM()

# ----------------
# メモリを蓄積する
function remember_memory(iCnt_play, iTurn, memory, state_cur, action, reward, state_next, done)
    push!(memory, (iCnt_play, iTurn, state_cur, action, reward, state_next, done))
    len_memory = length(memory)
    if len_memory > MEMORY_CAPACITY
        memory = memory[2:end]
    end
    return memory
end

# ----------------
# (REPLAY EXPERIENCE)学習する
function replay_experience(iCnt_play, memory, batch_size)
    # --------------------------------------------------
    len_memory = length(memory)
    batch_size = min(batch_size, len_memory)
    minibatch  = rand(memory, batch_size)
    X, Y   = zeros(batch_size,in_dim), zeros(batch_size)
    # --------------------------------------------------
    for ibat in 1:batch_size
        iPlay, iTurn, state_cur, str_action, reward, state_next, done = minibatch[ibat]
        # CURRENT: < X >-MATRIX
        if str_action == :left
            action = 1
        elseif str_action == :up
            action = 2
        elseif str_action == :right
            action = 3
        else 
            action = 4
        end
        # ---------------- 
        a_onehot = onehotbatch(action, 1:4)
        state_action_cur = vcat(state_cur, a_onehot)
        #println("state_action_cur: $state_action_cur")
        X[ibat,:]  = state_action_cur'
        # ----------------
        # NEXT: < Y >-ARRAY
        if done == true
            Y[ibat] = reward
        else
            mx_input = zeros(4,in_dim)  # 状態(state)と行動(action)マトリックス
            for imov in 1:4
                # [array]-action を結合します。
                a_onehot = onehotbatch(imov, 1:4)
                temp_s_a = vcat(state_next, a_onehot)
                #println("temp_s_a-choose action: ", temp_s_a)
                mx_input[imov,:] = temp_s_a'      # 状態(S)行動(A)マトリックス
            end
            #println("--- mx_input(replay_experience) ---")
            #println(mx_input)
            # ----------------
            # predict 'y'
            y_pred   = model(mx_input')
            target_f = reward + γ * maximum(y_pred)
            #println("target_f: $target_f")
            Y[ibat]  = target_f
        end
    end
    # ----------------
    # TRAINING
    # calculate loss -> gradient
    gs = gradient(() -> loss(X, Y), ps_model)
    # update weights
    Flux.Optimise.update!(opt, ps_model, gs)
    # ----------------
    val_loss = round(loss(X, Y); digits=5)
    #println("iPlay: $iCnt_play, loss: $val_loss")
    return val_loss
end

# =================================================
# EPISODE CONTROL functions
# =================================================
# エピソードを運用する
function get_episode!(iCnt_play, memory, rep_bonuscore)

    # リセット：基本変数を初期化する
    (grid, arr_acType, arr_action, arr_Qvalue, arr_reward, mx_states) = reset!()
    # -----
    # SOARTテクノメトリックスを計算する[RESET-CURRENT]
    # -----
    board = convert(Matrix{Float64}, grid)
    arr_beta, arr_distance = calc_techmetrics(conv_cross, mx_cvparts, board)
    state_cur = vcat(arr_beta, arr_distance)
    # -----
    flg_moved, flg_gameover, iTurn, total_score = true, false, 1, 0
    non_valid_count, valid_count = 0, 0
    while flg_gameover == false        # ゲームオーバーでループから外れる
        # 初期化
        Qvalue, acType  = -0.0001, "random"        
        # ----------------
        # 命令を選択し、ゲームを動かす
        if ε < rand() && flg_moved == true
            # DQNでコマを動かす
            (action, acType, Qvalue, grid, reward, flg_moved) = step_machine!(grid, state_cur, iCnt_play, Qvalue)
        else
            # ランダムでコマを動かす
            (action, acType, Qvalue, grid, reward, flg_moved) = step_random!(grid)
        end
        #if iTurn%10 == 0
        #    println("Play: $iCnt_play, Turn: $iTurn, grid: $grid, acType: $acType, action: $action, reward: $reward, flg_moved: $flg_moved")
        #end
        # ----------------
        # SOARTテクノメトリックスを計算する[RUN-NEXT]
        # ----------------
        board_next = convert(Matrix{Float64}, grid)
        arr_beta, arr_distance = calc_techmetrics(conv_cross, mx_cvparts, board_next)
        state_next = vcat(arr_beta, arr_distance)
        #println("UPDATED - state_next: $state_next")
        # ----------------
         # 記録用リストを追記する
        push!(arr_acType, acType)
        push!(arr_action, action)
        push!(arr_Qvalue, Qvalue)
        push!(arr_reward, reward)
        if iTurn == 1
            mx_states = state_cur
        else
            mx_states = cat(mx_states, state_cur, dims=2)
        end
        # ----------------
        state_cur   = state_next
        total_score = total_score + reward
        # ----------------
        # 報酬修正(1) : FEED_BACK
        reward = round(reward + 0.5*(maximum(state_cur[7:10]) - minimum(state_cur[7:10])); digits=3)
        # ----------------
        # カウント && ゲームオーバーしているか？ 
        if flg_moved == false
            non_valid_count = non_valid_count + 1
            flg_gameover = is_gameover!(grid)
        else
            valid_count = valid_count + 1
        end 
        # ----------------
        # ゲームオーバーした場合、ブレイクする
        if flg_gameover == true
            #println("BREAK!! - Play: $iCnt_play, Turn: $iTurn, flg_gameover: $flg_gameover")
            # 報酬修正(2)  ボーナス
            if total_score > rep_bonuscore*1.5 && iCnt_play > 20
                reward = 0.5*valid_count
            elseif total_score > rep_bonuscore && iCnt_play > 20
                reward = 0.3*valid_count
            else
                reward = -1.0*non_valid_count
            end
        else
            iTurn = iTurn + 1
        end
        # ----------------
        # Experience Replay配列に保管する
        memory = remember_memory(iCnt_play, iTurn, memory, state_cur, action, reward, state_next, flg_gameover)
    end
    # ---------------------------
    # ゲーム代表値のリスト
    rep_predQV = quantile(arr_Qvalue, [0.25, 0.75, 1])
    
    return (iTurn, round(total_score; digits=2), memory, rep_predQV, valid_count, non_valid_count, mx_states')
end

# =================================================
# MAIN ROUTINE
# =================================================
# 記録用パラメタ類
memory        = []  # メモリのリスト
# ---------------------------
# 記録用パラメタ類
arr_iplay     = []  # count game play    プレイ番号
arr_maxturn   = []  # turn game play    ターン数
arr_maxscore  = []  # rl_score game play    報酬の総和
arr_loss      = []  # DQN-Experience Replay学習
arr_εs       = []  # ε-Greedy
# ---------------------------
# Q値の分析用
arr_maxQV     = []  # QV値の最大値
arr_q25QV     = []  # QV値の4分の1値
arr_q75QV     = []  # QV値の4分の3値
arr_nvalid    = []  # 無効命令率
# ---------------------------
# 初期化
val_loss      = 0.0
iCnt_play     = 1
rep_bonuscore = 1000
# ---------------------------
# エピソードを実行する
while true   
    # ゲームを動かす
    (maxturn, maxscore, memory, rep_predQV, valid_count, non_valid_count, mx_states) = get_episode!(iCnt_play, memory, rep_bonuscore)
    # 無効命令率の計算
    ratio_nvalid = round(non_valid_count/maxturn; digits=3)
    # 簡単な出力
    #println("maxturn: $maxturn, maxscore: $maxscore, ratio_nvalid: $ratio_nvalid")

    # ----------
    # DQN-Experience Replay学習(重み付き)
    if valid_count >= 150
        for iLearn in 1:50
            val_loss = replay_experience(iCnt_play, memory, BATCH_SIZE)
        end
    elseif valid_count > 100 && valid_count < 150
        for iLearn in 1:20
            val_loss = replay_experience(iCnt_play, memory, BATCH_SIZE)
        #    println("Learn_turn:$iLearn, loss:$val_loss")
        end
    else
        val_loss = replay_experience(iCnt_play, memory, BATCH_SIZE)
    end

    # ----------
    # 学習結果の出力
    if iCnt_play%40 == 0
        println("iPlay: $iCnt_play, maxturn: $maxturn, maxscore: $maxscore, ε:$ε, loss:$val_loss, bonuscore:$rep_bonuscore, ratio_nvalid:$ratio_nvalid")
    end
    # ----------
    # 記録用パラメタ類(プレイベース)の追加
    push!(arr_iplay, iCnt_play)  # count game play    プレイ番号
    push!(arr_maxturn, maxturn)  # max_turn   ゲームのターン数
    push!(arr_maxscore, maxscore)  # rl_score game play    最終プレイスコア
    push!(arr_loss, val_loss)    # DQN-Experience Replay学習
    push!(arr_εs, ε)  # イプシロン
    # ----------
    # ゲーム代表値の保管
    push!(arr_maxQV, rep_predQV[3])  # QV値の最大値
    push!(arr_q75QV, rep_predQV[2])  # QV値の4分の3値
    push!(arr_q25QV, rep_predQV[1])  # QV値の4分の1値
    push!(arr_nvalid, ratio_nvalid)  # 無効命令率
    # ---------------------------
    # ボーナス支給のしきい値を設定する
    #rep_bonuscore = quantile(arr_maxscore, [0.75])[1]
    # ----------
    # Change EPSILON
    if ε > e_min && iCnt_play < total_episodes        # total_episodes
        ε *= e_decay
        ε = round(ε; digits=5)
        iCnt_play = iCnt_play + 1
    else
        break
    end
end
```

**(newRT)**

![imageJRL1-23-6](/2023-01-28-QEUR22_2048S3/imageJRL1-23-6.jpg)

**(SOART)**

![imageJRL1-23-7](/2023-01-28-QEUR22_2048S3/imageJRL1-23-7.jpg)

D先生(設定年齢65歳)  ： “へえ・・・。newRT法とSOART法の強化学習の結果を比較をしたんですね。でも、SOART法ってマルチ法でしょ？”

```julia
    # ------
    # 新RTメトリックスの計算用(STEP1)
    xx = dot(vec(mx_cvbase),vec(mx_cvbase))
    #xx = sum(vec(mx_cvbase))
    # ------
    if xx > 0.00001
        # ------
        arr_beta     = []
        arr_distance = []
        for k in 1:num_rtm
            # ------
            # 計測用畳み込みマトリックスを計算する
            mx_cvmeas = convmx_arrays(vec(mx_cvparts[k]), mx_base)
            # ------
            # 新RTメトリックスの計算用(STEP1)
            xy = dot(vec(mx_cvbase),vec(mx_cvmeas))
            #xy = sum(vec(mx_cvmeas))
            beta = round(xy/xx; digits=5)
            push!(arr_beta, beta)
            #println("beta: ",beta)
            # ------
```

QEU:FOUNDER： “前述のSOART法の説明を思い出してください。**newRTはβとして回転を計算し、SOART法は明るさを計算して補正しています**。その部分は、「＃」でコメントアウトして選択できるようになっています。”

D先生  ： “なるほどね。”

```julia
# ---------
# Create Graph(2) : 移動平均による平滑化
using MarketTechnicals

# A-GROUP
pltA1 = plot(arr_εs, label="EPSILON", legend=:topright)
pltA2 = plot(arr_loss, label="loss", legend=:topleft)
        plot!(ema(arr_loss, 10, wilder = false), label="movave", legend=:topleft)
pltA3 = plot(arr_maxturn, label="maxturn", legend=:topleft)
        plot!(ema(arr_maxturn,10, wilder = false), label="movave", legend=:topleft)
pltA4 = plot(arr_maxscore, label="maxscore", legend=:topleft)
        plot!(ema(arr_maxscore,10, wilder = false), label="movave", legend=:topleft)
plot(pltA1, pltA2, pltA3, pltA4)
```

**(newRT)**

[A]

![imageJRL1-23-8](/2023-01-28-QEUR22_2048S3/imageJRL1-23-8.jpg)

[B]

![imageJRL1-23-9](/2023-01-28-QEUR22_2048S3/imageJRL1-23-9.jpg)

**(SOART)**

[A]

![imageJRL1-23-10](/2023-01-28-QEUR22_2048S3/imageJRL1-23-10.jpg)

[B]

![imageJRL1-23-11](/2023-01-28-QEUR22_2048S3/imageJRL1-23-11.jpg)

D先生  ： “あらあら・・・。おっしゃる通り、大失敗ですね。”

QEU:FOUNDER ： “2048って、メイズ（迷路）と違って解のバリエーションがきわめて多いからね。20000回程度の学習じゃ、全然学習がダメなんだろうね。ちなみに4x4の生のボード情報で学習しても、あんまり差がなかった・・・。”

D先生  ： “でも・・・。両者（newRT vs SOART）には少しだけパフォーマンス差があります。SOARTの場合の方が、**すこしだけスコアが伸びている**ような気がします。”

QEU:FOUNDER ： “そうかな・・・？”

```julia
# B-GROUP
pltB1 = plot(ema(arr_maxQV,10, wilder = false), label="max", legend=:topleft)
        plot!(ema(arr_q75QV,10, wilder = false), label="q75", legend=:topleft)
        plot!(ema(arr_q25QV,10, wilder = false), label="q25", legend=:topleft)
pltB2 = plot(ema(arr_nvalid,10, wilder = false), label="non_valid", legend=:topleft)
plot(pltB1, pltB2)
```

**(newRT)**

![imageJRL1-23-12](/2023-01-28-QEUR22_2048S3/imageJRL1-23-12.jpg)

**(SOART)**

![imageJRL1-23-13](/2023-01-28-QEUR22_2048S3/imageJRL1-23-13.jpg)

QEU:FOUNDER ： “おう・・・、D先生の直感は正しかったね。Q値で比較した場合には、SOART法の方が**高い安定値**がでています。Q値はとても便利な指標だ・・・。”

D先生  ： “また、無効な命令を発行した比率「non_valid」もSOARTメトリックスの方が低いですね。あ～あ、タグチ感度（回転）ってパターン認識にはあまり使えないのかな？”

QEU:FOUNDER ： “その良し悪しは**「時と場合によりけり」**ですよ。単純な多重変量の比較においてはパワーを持っています。我々が以前、「大阪は異常か？」とか「日本は異常か？」というテーマで分析してみたでしょう？”

D先生  ： “でも、タグチ先生はRT法は画像のパターン分析として提案したんでしょ？”

QEU:FOUNDER ： “彼（タグチ先生）は「提案」担当だから・・・。**それを「検証」し、「発展」させなかった人たちの怠慢**でしょ？なにしろ30年、な～んにも進歩しなかったんだから・・・。”

D先生  ： “**そういう意味で「日本経済を象徴している技術分野」ですね。**”

QEU:FOUNDER ： “今回は強化学習としては失敗ですが、みなさんも是非やってみてください。Julia言語は速いです。それもPython言語に慣れた人から見て、もう感動モノ・・・。”


## ～　まとめ　～

QEU:FOUNDER ： “あ～あ、とうとうやっちゃった・・・。”

[![MOVIE1](http://img.youtube.com/vi/g300ehEWrwE/0.jpg)](http://www.youtube.com/watch?v=g300ehEWrwE "晶片光刻機禁售中國對三大廠有多少影響？特別對ASML的生意有何影響？究竟控制九成65nm以下的Canon、Nikon、ASML勝敗如何，其光核機如何分類？")

C部長 : “J国って、まだC国にまだ工場もってるんだっけ・・・。”

QEU:FOUNDER ： “J国は、**「もう（工場s in Cは）いらない」**と思っているんじゃない？まっ、「そういう考え方もある」しねえ・・・。・・・でも、「考え方が存在する」のであって、「かしこい」とはいっていない（笑）。”

![imageJRL1-23-14](/2023-01-28-QEUR22_2048S3/imageJRL1-23-14.jpg)

C部長 : “そもそも**「対立が存在するのか」もあやしい**のに・・・。それにしても、このお方（↑）は、いいたいことをずけずけいうひとですね。”

![imageJRL1-23-15](/2023-01-28-QEUR22_2048S3/imageJRL1-23-15.jpg)

QEU:FOUNDER ： “こういう（↑）人です。”

C部長 : “ゲッ・・・、あのお方の近所の生まれか・・・。”

QEU:FOUNDER ： “懐が深い地域といえます (笑)。”
