---
title: QEUR22_CLIFF3:　Cliff_Walkingで遊ぼう（その4：JuliaRLAGENT編-DQN）
date: 2023-01-16
tags: ["QEUシステム", "メトリックス", "Julia言語", "Cliff Walking", "Flux", "ディープラーニング", "強化学習"]
excerpt: Julia言語を使った強化学習
---

## QEUR22_CLIFF3:　Cliff_Walkingで遊ぼう（その4：JuliaRLAGENT編-DQN）

## ～　これは「意外とイノベーション」！！　～

QEU:FOUNDER(設定年齢65歳) ： “この番組は、やみくもに上から目線の**「高齢者によるイノベーション」**です。いやいや、C国はイケイケだねえ・・・。”

![imageJRL1-12-1](/2023-01-16-QEUR22_CLIFF3/imageJRL1-12-1.jpg)

D先生(設定年齢65歳)  ： “フン・・・、どうせ提灯記事だよ。”

### おっさん（＠QCサークル講話）；「従業員の皆さんにはテレビを見てください。皆が同じように考えてください。」

### オッサン（＠車中、N社検査不正について）：　「“検査不正”っていうのはなァ、（組織外に不正を）漏らしたヤツが悪いんだよ・・・」

### オッサン（海外工場のあいさつにて）：「私の使命はこの会社で終身雇用制を実現することにある・・・。」

QEU:FOUNDER ： “さすが殿下の宝刀、必殺！**「やみくもに上から目線」**・・・。”

D先生 ： “あとで都合が悪けりゃ、逃げればいい・・・(笑)。**「コ・・・、腰が・・・」**、**「ワシはかわいそうなんじゃ」**とか・・・。”

QEU:FOUNDER ： “あほらしい・・・。高齢者の中にも、**ちゃんと地道にイノベーションをする**人もいます。我々のように・・・。さて、今回はJulia(Flux)でDQNです。それでは、コードを晒します。”

```julia
# ---
# TRIAL【3】_Cliff_Walking_DQN_Onehot
# ---
using ReinforcementLearning
using Flux
using Flux: mse, onehotbatch
using Random
using Statistics
using Plots
using LinearAlgebra

# ---------------------------
# グローバル宣言
global num_state
global num_actions 
global in_dim 
global out_dim

# ---------------------------
# Defining all the required parameters
global ε
global γ 
global e_decay  
global e_min   
global learning_rate
global total_episodes 

# ---------------------------
# Cliff_Walking_Environment
# ---------------------------
NX, NY = 4, 12
PStart = [4, 1]
PGoal  = [4, 12]
LRUD = [
    [0, -1],  # 1->left
    [0, 1],   # 2->right
    [-1, 0],  # 3->up
    [1, 0],   # 4->down
]

# ------
function iscliff(p)
    x, y = p[1], p[2]
    if x == 4 && y > 1 && y < NY
        return true
    else
        return false
    end
end

# take a look at the wordmap
aaaa = zeros(Float32, NX, NY)
for i in 1:NX
    for j in 1:NY
        position  = [i,j]
        aaaa[i,j] = iscliff(position)
    end
end

# ----
# HEATMAPを描く
heatmap(aaaa;yflip=true)


# ------
# 環境用のユーティリティ関数群
function isoutlayout(p)
    x, y = p[1], p[2]
    if x < 1 || x > NX || y < 1 || y > NY
        return true
    else
        return false
    end
end

function calc_onehot(p)
    onehot = zeros(Float32, NX*NY)
    x, y = p[1], p[2]
    number::Int64  = x + (y-1)*NX
    onehot[number] = 1.0
    return onehot
end

function calc_number(p)
    x, y = p[1], p[2]
    number::Int64  = x + (y-1)*NX
    return number
end

function rev_onehot(p)
    arrseq = 1:NX*NY
    number = dot(arrseq, p)
    return number
end

function calc_rewards(p)
    reward::Float32 = 0.0
    if env.flg_layout == true
        reward = -1.0
    elseif env.flg_cliff == true
        reward = -50.0
    elseif p[1] == PGoal[1] && p[2] == PGoal[2]
        reward = 2000
    else
        reward = -0.2
    end
    return reward
end

function calc_isterm(p,cnt)
    if p[1] == PGoal[1] && p[2] == PGoal[2]
        return true
    elseif cnt > 50
        return true
    else
        return false
    end
end


# ------
# 環境の構築
Base.@kwdef mutable struct CliffWalkingEnv <: AbstractEnv
    position = PStart
    count::Int = 0
    flg_layout = false
    flg_cliff  = false
end

function (env::CliffWalkingEnv)(a::Int)
    env.flg_layout = false
    env.flg_cliff  = false
    temp = LRUD[a]
    x, y = env.position[1] + temp[1], env.position[2] + temp[2]
    if isoutlayout([x,y]) == true
        env.position = env.position
        env.flg_layout = true
    elseif iscliff([x,y]) == true
        env.count     = env.count + 1
        env.position  = PStart
        env.flg_cliff = true
    else
        env.position = [x, y]
    end
end

function env_reset()
    env.position = PStart
    env.count = 0
    env.flg_layout = false
    env.flg_cliff  = false
end

# ------
RLBase.state(env::CliffWalkingEnv) = calc_onehot(env.position)
RLBase.state_space(env::CliffWalkingEnv) = Base.OneTo(length(calc_onehot(env.position)))
RLBase.action_space(env::CliffWalkingEnv) = Base.OneTo(length(LRUD))
RLBase.reward(env::CliffWalkingEnv) = calc_rewards(env.position)
RLBase.is_terminated(env::CliffWalkingEnv) = calc_isterm(env.position, env.count)
RLBase.reset!(env::CliffWalkingEnv) = env_reset()
env = CliffWalkingEnv()


# ----------------
# 命令を選択する
function choose_action(state_cur, iCnt_play)

    # ----------------------------------------
    # 移動命令
    # LRUD = [
    #    [0.0, -1.0],  # 1->left
    #    [0.0, 1.0],   # 2->right
    #    [-1.0, 0.0],  # 3->up
    #    [1.0, 0.0],   # 4->down
    # ----------------
    Qvalue = -0.0001
    #println("BIGIN:choose_action")
    # ----------------
    # 最適命令の選択
    if ε < rand()
        # DQNによる選択
        acType   = "machine"
        mx_input = zeros(4,in_dim)
        for action in 1:4
            # [array]-action を生成します。
            a_onehot = onehotbatch(action, 1:4)
            temp_s_a = cat(state_cur, a_onehot , dims=1)'
            #println("temp_s_a-choose action: ", temp_s_a)
            mx_input[action,:] = temp_s_a     # 状態(S)行動(A)マトリックス
        end
        #println("----- mx_input(choose_action) -----")
        #println(mx_input)
        # ----------------------------------------
        # predict 'y'
        y_pred  = model(mx_input')
        Qvalue  = maximum(y_pred)
        a_order = argmax(y_pred)[2]
    else
        # 乱数による選択
        acType = "random"
        a_order = rand(1:4)
    end

    return a_order, acType, round(Qvalue; digits=5)
end


# =================================================
# Calculation class(2) : DQN_Solver:
# =================================================
# crate instance for input
global model       = Chain(Dense(in_dim, 256, relu), Dense(256, out_dim)) # Use Chain if you want to stack layers
# パラメタ
global ps_model    = params(model)
# -----
# 損失関数
global loss(x, y)  = mse(model(x'), Matrix(y'))
# -----
# オプティマイザ
global opt         = ADAM()

# ----------------
# メモリを蓄積する
function remember_memory(memory, state_cur, action, reward, state_next, done)
    push!(memory, (state_cur, action, reward, state_next, done))
    if length(memory) > MEMORY_CAPACITY
        memory = memory[2:end]
    end
    return memory
end

# ----------------
# (REPLAY EXPERIENCE)学習する
function replay_experience(iCnt_play, memory, batch_size)
    #println("BIGIN:replay_experience")
    # --------------------------------------------------
    batch_size = min(batch_size, length(memory))
    minibatch  = rand(memory, batch_size)
    X, Y   = zeros(batch_size,in_dim), zeros(batch_size)
    # --------------------------------------------------
    for ibat in 1:batch_size
        state_cur, action, reward_cur, state_next, done = minibatch[ibat]
        # CURRENT: < X >-MATRIX
        a_onehot = onehotbatch(action, 1:4)
        state_action_cur = cat(state_cur, a_onehot, dims=1)'
        #println("state_action_cur: $state_action_cur")
        X[ibat,:]  = state_action_cur
        # ----------------
        # NEXT: < Y >-ARRAY
        if done == true
            Y[ibat] = reward_cur
        else
            mx_input = zeros(4,in_dim)  # 状態(state)と行動(action)マトリックス
            for imov in 1:4
                # [array]-action を結合します。
                a_onehot = onehotbatch(imov, 1:4)
                temp_s_a = cat(state_next, a_onehot , dims=1)'
                #print("temp_s_a-choose action: ", temp_s_a)
                mx_input[imov,:] = temp_s_a      # 状態(S)行動(A)マトリックス
            end
            #println("--- mx_input(replay_experience) ---")
            #println(mx_input)
            # --------------------------------------------------
            # predict 'y'
            y_pred   = model(mx_input')
            target_f = reward_cur + γ * maximum(y_pred)
            #println("target_f: $target_f")
            Y[ibat]  = target_f
        end
    end
    # --------------------------------------------------
    # TRAINING
    # calculate loss -> gradient
    gs = gradient(() -> loss(X, Y), ps_model)
    # update weights
    Flux.Optimise.update!(opt, ps_model, gs)
    # -----
    val_loss = round(loss(X, Y); digits=5)
    #println("iPlay: $iCnt_play, loss: $val_loss")

    return val_loss
end


# =================================================
# ENVIRONMENT class
# =================================================
# リセット
function env_reset!(env)
    reset!(env)
    state_cur = state(env)
    position_cur = ( env.position[1], env.position[2] )
    #println("state_cur: $state_cur, position_cur: $position_cur")
    return state_cur, position_cur
end

# 運用中
function env_step!(env, action)
    env(action)
    state_next = state(env)
    position_next = ( env.position[1], env.position[2] )
    reward_cur = reward(env)
    done       = is_terminated(env)
    #println("state_next: $state_next, position_next: $position_next, reward: $reward_cur, done: $done")
    return state_next, position_next, reward_cur, done
end


# =================================================
# Calculation class
# =================================================
# エピソードを運用する
function get_episode(iCnt_play, memory)

    # ---------------------------
    # 機械学習用のパラメタ群
    Qvalue, reward_cur, done = -0.0001, 0, false
    maxturn, maxscore = 0, 0
    # ---------------------------
    # 記録用パラメタ類(ターンベース)
    arr_iturn  = []  # ターン・カウンタリスト
    orders_row = []  # 指示リスト(row)
    orders_col = []  # 指示リスト(col)
    arr_orders = []  # 指示リスト(命令)
    arr_acType = []  # 指示のタイプ
    arr_scores = []  # ゲームスコアリスト
    arr_dones  = []  # ゲームオーバーフラグ
    arr_predQV = []  # Q値のリスト
    # ---------------------------
    # ゲームをリセットする
    state_cur, position_cur = env_reset!(env)
    # ---------------------------
    # ゲームをプレイする
    iCnt_turn = 0
    while true
        # 命令(a_order)を作成する
        a_order, acType, Qvalue = choose_action(state_cur, iCnt_play)
        # ---------------------------
        # ゲームをもう一歩進める
        state_next, position_next, reward_cur, done = env_step!(env, a_order)
        #if iCnt_play % 5 < 0.01 && iCnt_turn % 20 < 0.01 || done == true
        #    println("iPlay: $iCnt_play, iTurn: $iCnt_turn, position_cur: $position_cur, action: $a_order, done: $done, reward: $reward_cur")
        #end
        # ---------------------------
        # 記録用リストを追記する
        push!(arr_iturn, iCnt_turn)  # ターン・カウンタリスト
        push!(orders_row, position_cur[1])  # 指示リスト(ROW)
        push!(orders_col, position_cur[2])  # 指示リスト(COL)
        push!(arr_orders, a_order)  # 指示リスト(命令)
        push!(arr_acType, acType)  # 指示のタイプ
        push!(arr_scores, reward_cur)  # ゲームスコアリスト
        push!(arr_dones, done)  # ゲームオーバーフラグ
        push!(arr_predQV, Qvalue)  # Q値のリスト
        # ----------------
        # Experience Replay配列に保管する
        memory = remember_memory(memory, state_cur, a_order, reward_cur, state_next, done)
        # ----------------
        # 実行継続するか分岐
        if done == true
            # Game over
            break  # (game over rip)
        else
            # count of game turn
            position_cur  = position_next
            state_cur     = state_next
            iCnt_turn     = iCnt_turn + 1
        end
    end
    # ---------------------------
    # プレイデータの引継ぎ
    maxturn, maxscore = iCnt_turn, sum(arr_scores)

    return maxturn, round(maxscore; digits=2), memory
end


# =================================================
# MAIN ROUTINE
# =================================================
# -----
#    Args:
#        ε: The degree of exploration
#        γ: The discount factor
#        num_state: The number of states
#        num_actions: The number of actions
#        action_space: To call the random action
# -----
num_state     = 48
num_actions   = 4
in_dim  = num_state + num_actions
out_dim = 1

# ---------------------------
# Defining all the required parameters
ε     = 0.70
γ     = 0.95
e_decay       = 0.9995
e_min         = 0.01
learning_rate  = 0.005
total_episodes = 5000

# ---------------------------
# ハイパーパラメタ
memory         = []
BATCH_SIZE     = 128  # サンプルサイズ
MEMORY_CAPACITY = 128 * 8  # メモリ容量

# ---------------------------
# 記録用パラメタ類(プレイベース)
arr_iplay     = []  # count game play    プレイ番号
arr_maxturn   = []  # turn game play    ターン数
arr_maxscore  = []  # rl_score game play    報酬の総和
arr_loss      = []  # DQN-Experience Replay学習
arr_εs        = []  # ε-Greedy
# ---------------------------
# Q値の分析用(プレイベース)
arr_maxQV     = []  # QV値の最大値
arr_q25QV     = []  # QV値の4分の1値
arr_q75QV     = []  # QV値の4分の3値
# ---------------------------
# エピソードを実行する
for iCnt_play in 1:total_episodes   # total_episodes
    # ゲームする
    maxturn, maxscore, memory = get_episode(iCnt_play, memory)
    # ----------
    # DQN-Experience Replay学習
    val_loss = replay_experience(iCnt_play, memory, BATCH_SIZE)
    # 結果の出力
	if iCnt_play % 20 < 0.01
		println("iCnt_play: $iCnt_play, maxturn:$maxturn, maxscore:$maxscore, ε:$ε, loss:$val_loss")
	end
    # ----------
    # 記録用パラメタ類(プレイベース)の追加
    push!(arr_iplay, iCnt_play)  # count game play    プレイ番号
    push!(arr_maxturn, maxturn)  # max_turn   ゲームのターン数
    push!(arr_maxscore, maxscore)  # rl_score game play    最終プレイスコア
    push!(arr_loss, val_loss)    # DQN-Experience Replay学習
    push!(arr_εs, ε)  # イプシロン
    # ----------
    # Q値の保管(プレイベース)
    #push!(arr_maxQV, max(arr_predQV))  # QV値の最大値
    #push!(arr_q25QV, np.percentile(arr_predQV, q=25))  # QV値の4分の1値
    #push!(arr_q75QV, np.percentile(arr_predQV, q=75))  # QV値の4分の3値
    # ----------
    # Change EPSILON
    if ε > e_min
        ε *= e_decay
		ε = round(ε; digits=5)
    end
end
```

![imageJRL1-12-2](/2023-01-16-QEUR22_CLIFF3/imageJRL1-12-2.jpg)

```julia
# ----------
# Graph
#using Plots

plt1 = plot(arr_εs, label="EPSILON")
plt2 = plot(arr_loss, label="loss")
plt3 = plot(arr_maxturn, label="maxturn")
plt4 = plot(arr_maxscore, label="maxscore")

plot(plt1, plt2, plt3, plt4)
```

![imageJRL1-12-3](/2023-01-16-QEUR22_CLIFF3/imageJRL1-12-3.jpg)

D先生 ： “・・ああ、なるほど・・・。前回おっしゃっていたDQN48とは、STATEを48件のワンホットにすることなんですね(笑)。結果として、学習はうまく行っていますが、結構シンプルなコードになりましたね。”

QEU:FOUNDER ： “Fluxを使うと構成がシンプルになるよね。感覚としてPytorchよりもイケています。あと、ReinforcementLearning.jl(RL.jl)のENV構築機能は、すごく使いやすいです。Julia言語で強化学習って、「アリ」だと思うけど・・・。”

D先生 ： “DQN程度であれば、RL.jlのAgent機能をわざわざ使う必要もないかもしれません。”

QEU:FOUNDER ： “・・・まあ、そこらへんはいまのところはわかりませんね・・・。”

D先生 ： “そういえば、今回の実行速度はPythonのときよりも速いですか？”

QEU:FOUNDER ： “速度はあんまり変わらないよ。Pythonそのものの計算速度は遅いが、Pytorchを使うとC言語が動いているので、計算速度が速いんですよ。”

D先生 ： “じゃあ、敢てJuliaをつかう意味がない・・・。”

QEU:FOUNDER  ： “普通の人にはそうかもね。でも、我々の場合には複雑な**「テクノメトリックス」を計算する**ので、Julia言語のありがたみがでてきます。。”

D先生 ： “おっと、そのことを忘れていました。”

QEU:FOUNDER ： “じゃあ、次はいよいよ**「例の(符号付き)minRT距離」**を使って強化学習をしてみましょう。”


## ～　まとめ　～

C部長 : “**「Julia言語で作った強化学習のプログラムを晒す」**、これは**「ひそかなイノベーション」**ですね。さすが、天下の高齢者様・・・。”

QEU:FOUNDER ： “小生は「上から目線」じゃないが・・・（笑）。でも、ホント、そうなんだよねえ・・・。Julia言語で**ベーシックな強化学習をした事例はWeb上でもほとんど見当たりません**。どうしてなんだろう・・・。”

C部長 : “Julia言語もマイナーだし、さらに強化学習も機械学習の中でもマイナーだし・・・。”

QEU:FOUNDER ： “多分、強化学習を勉強した人たちも、ほとんどが「(OpenAI)Gym止まり」なんだろうね。勉強したことで実用化まではいかないんでしょう・・・。”

![imageJRL1-12-4](/2023-01-16-QEUR22_CLIFF3/imageJRL1-12-4.jpg)

C部長 : “もったいないですね。あまりAgent側に気を入れすぎると、かえって実用化から遠のきます。通常の事例を解くにはDQNで十分・・・。結局、Python言語の方が環境構築が楽なのでしょうね。”

QEU:FOUNDER ： “やっぱり、エコシステムが重要ですね。Julia言語でも、**「小さな成功」を追求**させなければいけない・・・。”

