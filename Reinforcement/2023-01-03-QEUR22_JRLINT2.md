---
title: QEUR22_JRLINT2:　ReinforcementLearning.jlのこととか・・・
date: 2023-01-03
tags: ["QEUシステム", "メトリックス", "Julia言語", "機械学習", "Flux", "ディープラーニング", "強化学習"]
excerpt: Julia言語を使った強化学習
---

## QEUR22_JRLINT2:　ReinforcementLearning.jlのこととか・・・

## ～　まるで「KERAS」みたい・・・　～

QEU:FOUNDER （設定65歳） ： “さあて、この番組は**「高齢者によるイノベーション」**です。具体的に使ってみましょう。しつこく、この動画（↓）から始めます。”

[![MOVIE1](http://img.youtube.com/vi/ckIIxKM14Ow/0.jpg)](http://www.youtube.com/watch?v=ckIIxKM14Ow "ReinforcementLearning jl | Jun Tian | Julia User Group Munich - Share&Code")

QEU:FOUNDER ： “具体的には、この「横綱」(↓)のことです。”

**(東の横綱)**

![imageJRL1-3-1](/2023-01-03-QEUR22_JRLINT2/imageJRL1-3-1.jpg)

D先生 ： “ちなみに、前回の**「Asian RL Package」**という表現は自動翻訳のまちがいですが、おもしろいので使っています。このパッケージには、世界中の頭脳が参加しているのでAsianとWesternの差異なんかないですからね。”

QEU:FOUNDER ： “ちなみに、**「西の横綱」**はコレ(↓)・・・。”


**(西の横綱)**

![imageJRL1-3-2](/2023-01-03-QEUR22_JRLINT2/imageJRL1-3-2.jpg)

D先生 ： “FOUNDERのいわゆる**「西(western)」という意味**は？”

![imageJRL1-3-3](/2023-01-03-QEUR22_JRLINT2/imageJRL1-3-3.jpg)

QEU:FOUNDER ： “この大学(↑)が中心となって開発しているパッケージだからなんです。前回も言いましたけど、結局、我々にとってソレが使いやすいかどうかが良しあしの決めてです。Pythonのディープラーニングのソルーションが複数種類あるように、長期的には両者は共存していくんじゃないかと思います。”

D先生 ： “そして、「（我々にとって）使いやすい」とは？”

QEU:FOUNDER ： “結論？いやいや・・・。まずは、最初に「東側」の事例紹介をしましょうよ・・・。彼らのtutorialを見てみましょう。”

![imageJRL1-3-4](/2023-01-03-QEUR22_JRLINT2/imageJRL1-3-4.jpg)

D先生 ： “すごく簡単なゲームですね。”

QEU:FOUNDER ： “そうですね。我々も、これから始めましょう。POMDPs.jlも例題としてTigerという名前のゲームを使用しています。ちょっとゲームのルールは違いますけどね。・・・なにはともあれ、コードと実行結果をみてみましょう。”

```julia
################################################################################
# Chapter 6.2 Random Walk (Revised)
################################################################################
# ----
using ReinforcementLearning
using Flux
using Statistics
using Plots

# ----
env = RandomWalk1D(;rewards=0.0=>1.0)

# ----
NS, NA = length(state_space(env)), length(action_space(env))
```

![imageJRL1-3-5](/2023-01-03-QEUR22_JRLINT2/imageJRL1-3-5.jpg)

```julia
# ----
create_TD_agent(α) = Agent(
    policy=VBasedPolicy(
        learner = TDLearner(
            approximator=TabularApproximator(fill(0.5, NS), Descent(α)),
            method=:SRS,
            γ=1.0,
            n=0,
        ),
        mapping = (env, V) -> rand(1:NA)
    ),
    trajectory=VectorSARTTrajectory()
)


# ----
true_values = [i/6 for i in 1:5]

# ----
Base.@kwdef struct RecordRMS <: AbstractHook
    rms::Vector{Float64} = []
end

(f::RecordRMS)(::PostEpisodeStage, agent, env) = push!(
    f.rms,
    sqrt(mean((agent.policy.learner.approximator.table[2:end - 1] - true_values).^2))
)

# ----
function run_once(α;is_reduce=true, n_episode=50)
	agent = create_TD_agent(α)
	hook = RecordRMS()
	run(agent, env, StopAfterEpisode(n_episode; is_show_progress=false),hook)
	is_reduce ? mean(hook.rms) : hook.rms
end


# ----
fig_obj = plot(xlabel="Episode", ylabel="RMS")
plot!(fig_obj, mean(run_once(0.04; is_reduce=false, n_episode=300)[1:end-1] for _ in 1:100), la-bel="α=0.04")
plot!(fig_obj, mean(run_once(0.10; is_reduce=false, n_episode=300)[1:end-1] for _ in 1:100), la-bel="α=0.10")
fig_obj

```

![imageJRL1-3-6](/2023-01-03-QEUR22_JRLINT2/imageJRL1-3-6.jpg)

D先生 ： “Chapter6.2という場所に同じコードがあるんですね。でも、かなりコードが改造されています・・・。あと、アウトプットは、TD学習のスキームにおける学習率(α)による学習曲線の変化を見たものですね。・・・でも、グラフのメトリックス(Y軸)がRMS（2乗平均）になっています。これがちょっと特殊ですね・・・。”

QEU:FOUNDER ： “この部分の意図を理解するにはtutorialのコードを参照してください。。”


**（標準ベクトル）**

![imageJRL1-3-7](/2023-01-03-QEUR22_JRLINT2/imageJRL1-3-7.jpg)

**（学習にともなう変化）**

![imageJRL1-3-8](/2023-01-03-QEUR22_JRLINT2/imageJRL1-3-8.jpg)

QEU:FOUNDER ： “この簡単なゲームでは最適なV値がどうなるかが自明だったので、そこからの距離をメトリックスにしたわけですね。”

D先生 ： “それにしても、**ReinforcementLearning.jlを使えば簡単に強化学習を実現できます**ね。ディープラーニングにおけるKerasみたいに・・・。”

```julia
run(agent, env, StopAfterEpisode(n_episode; is_show_progress=false),hook)
```

QEU:FOUNDER ： “Wrapperだから・・・。RUN命令に定義されたAgentとEnvを組み込めば学習できるんです。”

![imageJRL1-3-9](/2023-01-03-QEUR22_JRLINT2/imageJRL1-3-9.jpg)

QEU:FOUNDER ： “Environment(環境、ゲーム)が簡単につくれたらなぁ・・・。”

D先生 ： “それが決めてですよね。”

![imageJRL1-3-10](/2023-01-03-QEUR22_JRLINT2/imageJRL1-3-10.jpg)

QEU:FOUNDER  ： “いまのところGymが提供しているenvを使っていて、どのようにプログラミングされているかがわからないんです。カスタマイズされた環境ができないと意味がないですから・・・。”

**（環境設計アウトプット-1）**

![imageJRL1-3-11](/2023-01-03-QEUR22_JRLINT2/imageJRL1-3-11.jpg)

**（環境設計アウトプット-2）**

![imageJRL1-3-12](/2023-01-03-QEUR22_JRLINT2/imageJRL1-3-12.jpg)

D先生 ： “ホントに・・・。いまのところブラックボックスなのですが、どうなっているんでしょうかねぇ・・・。”

QEU:FOUNDER ： “次はPOMDPs.jlパッケージを使ってみましょう。”


## ～　まとめ　～

QEU:FOUNDER ： “前回のつづきで、**「初夢を」**っと・・・。現場で使えればいいなぁ・・・。”

![imageJRL1-3-13](/2023-01-03-QEUR22_JRLINT2/imageJRL1-3-13.jpg)

C部長 : “環境をカスタマイズできればいけるでしょう。できれば、ラズパイでできればいいですね。”

![imageJRL1-3-14](/2023-01-03-QEUR22_JRLINT2/imageJRL1-3-14.jpg)

QEU:FOUNDER ： “最近のラズパイ本には強化学習だけでなく、CNN（畳み込みニューラルネット）も紹介されていますからね。”

C部長 : “CNNまで？”

QEU:FOUNDER ： “できるのかなぁ・・・。でもね、PythonよりもJulia言語でやればさらにハードルが低くなりますからね。”

C部長 : “この本を使って、実際にラズパイを使った検証をしたいですね。”

QEU:FOUNDER ： “Julia版と対比できればもっといい・・・。もうサプライアもKNS（カモネギ・システム）でヒイヒイ言っている場合じゃないって・・・。”

C部長 : “**カモ(K)ネギ(N)システム(S)**って？。”

![imageJRL1-3-15](/2023-01-03-QEUR22_JRLINT2/imageJRL1-3-15.jpg)

QEU:FOUNDER ： “Industry5.0段階になれば、**サプライアも身の丈を顧みず上に言われるままに導入したシステム**でヒイヒイ言っている場合じゃないって・・・。そういえば、N国ってすごいよね。産油国なのにEV指向なんだよね・・・。”

