---
title: QEUR22_CLIFF1:　Cliff_Walkingで遊ぼう（その２：JuliaRL環境編）
date: 2023-01-13
tags: ["QEUシステム", "メトリックス", "Julia言語", "Cliff Walking", "Flux", "ディープラーニング", "強化学習"]
excerpt: Julia言語を使った強化学習
---

## QEUR22_CLIFF1:　Cliff_Walkingで遊ぼう（その２：JuliaRL環境編）

## ～　Julia言語にはClassがない　～

QEU:FOUNDER(設定年齢65歳) ： “みなさん、こんばんは・・・。この番組は、「高齢者によるイノベーション」です。この番組はいい年したオッサンが慣れない手つきで、新しいことをやり、それを晒す番組です。今回は、Julia言語のReinforcementLearning.jl（以下RL.jl）の環境構築機能を使って、カスタム環境をつくってみましょう。”

[![MOVIE1](http://img.youtube.com/vi/ckIIxKM14Ow/0.jpg)](http://www.youtube.com/watch?v=ckIIxKM14Ow "ReinforcementLearning jl | Jun Tian | Julia User Group Munich - Share&Code")

QEU:FOUNDER ： “RL.jlには、環境のカスタム化について**マニュアル化**がされているので、ちょっとやってみましょう。”

![imageJRL1-10-1](/2023-01-13-QEUR22_CLIFF1/imageJRL1-10-1.jpg)

D先生(設定年齢65歳)  ： “今回は、Cliff_Walkingです。RL.jlの例題にあった例題では、このようなレイアウトでした。”

![imageJRL1-10-2](/2023-01-13-QEUR22_CLIFF1/imageJRL1-10-2.jpg)

QEU:FOUNDER ： “Python版とも少しレイアウトが変わっているんですよね。それにしても、**以前使ったRL.jl版の環境はひどすぎ**・・・。あれでは、Qテーブルでは学習できても、ディープ・ラーニングではとても学習できません。そこで、ロジックをかえるしかない。”

D先生 ： “はからずも、**「カスタマイズ」**することになったとか・・・(笑)。”

QEU:FOUNDER ： “それでは、環境(ENV)側のプログラムを晒すよ！”

```julia

# ---
# TRIAL【1】_Cliff_Walking_DQN
# ---
using ReinforcementLearning
using Flux
using Flux.Losses
using StableRNGs

# ----
# Cliff_Walking_Environment
# ----
NX, NY = 4, 12
PStart = [4, 1]
PGoal  = [4, 12]
LRUD = [
	[0, -1],  # 1->left
	[0, 1],   # 2->right
	[-1, 0],  # 3->up
	[1, 0],   # 4->down
]

# ------
function iscliff(p)
    x, y = p[1], p[2]
    if x == 4 && y > 1 && y < NY
		return true
	else
		return false
	end
end

# take a look at the wordmap
aaaa = zeros(Float32, NX, NY)
println(aaaa)
for i in 1:NX
	for j in 1:NY
		position  = [i,j]
		aaaa[i,j] = iscliff(position)
	end
end
println(aaaa)

# ----
# HEATMAPを描く
using Plots
heatmap(aaaa;yflip=true)
```

![imageJRL1-10-3](/2023-01-13-QEUR22_CLIFF1/imageJRL1-10-3.jpg)

```julia
# ------
# 環境用のユーティリティ関数群
using LinearAlgebra
function isoutlayout(p)
    x, y = p[1], p[2]
    if x < 1 || x > NX || y < 1 || y > NY
        return true
    else
        return false
	end
end

function calc_onehot(p)
	onehot = zeros(Float32, NX*NY)
    x, y = p[1], p[2]
    number::Int64  = x + y*NX
	onehot[number] = 1.0
	return onehot
end

function rev_onehot(p)
	arrseq = 1:NX*NY
	number = dot(arrseq, p)
	return number
end

function calc_rewards(p)
	reward::Float32 = 0.0
	if env.flg_layout == true
		reward = -10.0
	elseif env.flg_cliff == true
		reward = -100.0
	elseif p[1] == PGoal[1] && p[2] == PGoal[2]
		reward = 50
	else
		reward = -0.1
	end
	return reward
end

function calc_isterm(p,cnt)
    if p[1] == PGoal[1] && p[2] == PGoal[2]
        return true
	elseif cnt > 5
		return true
    else
        return false
	end
end

# ------
# 環境の構築
Base.@kwdef mutable struct CliffWalkingEnv <: AbstractEnv
	position = PStart
	count::Int = 0
	flg_layout = false
	flg_cliff  = false
end

function (env::CliffWalkingEnv)(a::Int)
	env.flg_layout = false
	env.flg_cliff  = false
	temp = LRUD[a]
	x, y = env.position[1] + temp[1], env.position[2] + temp[2]
	if isoutlayout([x,y]) == true
		env.position = env.position
		env.flg_layout = true
	elseif iscliff([x,y]) == true
		env.count     = env.count + 1
		env.position  = PStart
		env.flg_cliff = true
	else
		env.position = [x, y]
	end
end

function env_reset()
    env.position = PStart
    env.count = 0
	env.flg_layout = false
	env.flg_cliff  = false
end

# ------
RLBase.state(env::CliffWalkingEnv) = calc_onehot(env.position)
RLBase.state_space(env::CliffWalkingEnv) = Base.OneTo(length(calc_onehot(env.position)))
RLBase.action_space(env::CliffWalkingEnv) = Base.OneTo(length(LRUD))
RLBase.reward(env::CliffWalkingEnv) = calc_rewards(env.position)
RLBase.is_terminated(env::CliffWalkingEnv) = calc_isterm(env.position, env.count)
RLBase.reset!(env::CliffWalkingEnv) = env_reset()
env = CliffWalkingEnv()
```

![imageJRL1-10-4](/2023-01-13-QEUR22_CLIFF1/imageJRL1-10-4.jpg)

D先生 ： “環境が構築されるとTraitが表示されるのは、すでに定番になりましたねえ・・・。”

QEU:FOUNDER ： “今回の変更のメインは、**Cliff（崖）に落ちたら自動的にスタートラインに戻る**ようにしました。さらに、指定した回数落ちたら強制終了することにしました。”

D先生 ： “この回数上限指定のロジックは、PythonではAgent側に設けていました。わざわざ環境側に負担させた意味は？”

QEU:FOUNDER ： “もし、RL.jlの例題のAgentを使うとすると、このような細かい作業ができないですからね。今のところ・・・。”

D先生 ： “SIMULATIONで、この環境がどのように動くのかが知りたいですね。”

```julia
# -------
# SIMULATION
# 動きを確認する
# -------
#	[0, -1],  # 1->left
#	[0, 1],   # 2->right
#	[-1, 0],  # 3->up
#	[1, 0],   # 4->down
# -------
n_episode = 3

for epi in 1:n_episode
	reset!(env)
	count = 1
	while true
		action = rand([1,2,3,4])
		env(action)
		println("EPISODE: ", epi, ", CNT: ", count, ", ACT: ", action, ", ENV: ",env," REWARD: ",reward(env)," TERM: ",is_terminated(env))
		if is_terminated(env) == true
			break
		end
		count = count + 1
	end
end
```

![imageJRL1-10-5](/2023-01-13-QEUR22_CLIFF1/imageJRL1-10-5.jpg)

```julia
# -----
# 決まったアクションの場合
arr_action = [3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4]
n_episode = 3

for epi in 1:n_episode
	reset!(env)
	count = 1
	for action in arr_action
		env(action)
		println("EPISODE: ", epi, ", CNT: ", count, ", ACT: ", action, ", ENV: ",env," REWARD: ",reward(env)," TERM: ",is_terminated(env))
		if is_terminated(env) == true
			break
		end
		count = count + 1
	end
end
```

![imageJRL1-10-6](/2023-01-13-QEUR22_CLIFF1/imageJRL1-10-6.jpg)

D先生 ： “**ランダム・アクションと指定アクション**で動かしてみたわけですね。”

QEU:FOUNDER ： “実際動かしてみれば、自分がカスタマイズしたことの意味が分かるようになります。やっぱり、動作確認は必須ですね。Julia言語にはPythonのClassに当たる機能がないので、複数の関数をまとめて部品にするには、このような手順をとることがわかります。”

D先生 ： “ついでですから、強化学習できませんか？”

QEU:FOUNDER ： “前回のAgentを使って・・・？”

D先生 ： “ダメモトで・・・。”

QEU:FOUNDER  ： “じゃあ、やってみましょう。ドン・・・。”

```julia

# ----
# 強化学習をしてみる
# -----
# Agent DQN
# -----
seed = 12345
rng = StableRNG(seed)
ns, na = 48, 4

agent = Agent(
    policy = QBasedPolicy(
        learner = BasicDQNLearner(
            approximator = NeuralNetworkApproximator(
                model = Chain(
                    Dense(ns, 128, relu; initW = glorot_uniform(rng)),
                    Dense(128, na; initW = glorot_uniform(rng)),
                ) |> cpu,
                optimizer = ADAM(),
            ),
            batch_size = 32,
            min_replay_history = 100,
            loss_func = huber_loss,
            rng = rng,
        ),
        explorer = EpsilonGreedyExplorer(
            kind = :exp,
            ϵ_stable = 0.01,
            decay_steps = 500,
            rng = rng,
        ),
    ),
    trajectory = CircularArraySARTTrajectory(
        capacity = 1000,
        state = Vector{Float32} => (ns,),
    ),
)

# ---
# miscellaneous
stop_condition = StopAfterStep(80000, is_show_progress=!haskey(ENV, "CI"))
hook = TotalRewardPerEpisode()

# ---
# trial run
using Plots

run(agent, env, stop_condition, hook)
plot(hook.rewards)

```

![imageJRL1-10-7](/2023-01-13-QEUR22_CLIFF1/imageJRL1-10-7.jpg)

D先生 ： “ああ・・・、**エラーが出てしまいました**。なぜだろう・・・。環境は、問題のないものを作ったはずなんだが・・・。”

QEU:FOUNDER ： “Agentのカスタマイズには、もっとノウハウが必要なんでしょう・・・。我々には、まだ早すぎます。”

D先生 ： “じゃあ、次はこの環境を使って、AgentをHANDMADEで・・・。”



## ～　まとめ　～

QEU:FOUNDER ： “A国は、情報技術の最先端でさらに差をつけつつあるね・・・。”

[![MOVIE2](http://img.youtube.com/vi/JV38j5tQ5B4/0.jpg)](http://www.youtube.com/watch?v=JV38j5tQ5B4 "(中字) 傳微軟將再投資OpenAI高達100億美元！解釋整個計畫與用途！爆紅ChatGPT將改變世界？人工智慧成熟將可以取代百萬人工作！《蕭若元：蕭氏新聞台》2023-01-11")

C部長 : “あの、ChatGPTですね・・・。これって、あのC国のハッカー様も絡んでいるんじゃないですかね？なにしろ、M社だし・・・。”

QEU:FOUNDER ： “その可能性は大きいでしょうね・・・。ああ・・・、**「A-C対立」って、一体何なんだろう**（笑）？さて、Gym環境による強化学習の実装って勉強のために不可欠なんだけど、傍から見ている「な～んにもわかっていない人たち」を誤解させたんですよね。”

C部長 : “つまらんゲームばっかりやっちょる。RLは、やっぱりつかえない・・・。”

[![MOVIE3](http://img.youtube.com/vi/SszI8Rj4qUM/0.jpg)](http://www.youtube.com/watch?v=SszI8Rj4qUM "日本に変革が起きない理由は〇〇")

QEU:FOUNDER ： “使えないのはオマエだって・・・（笑）。**30年間J国の進歩を止めて、さらに止める気かね**・・・？”

