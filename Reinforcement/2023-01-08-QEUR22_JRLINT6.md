---
title: QEUR22_JRLINT6:　ReinforcementLearning.jlでDeepRLを・・・(その2)
date: 2023-01-08
tags: ["QEUシステム", "メトリックス", "Julia言語", "機械学習", "Flux", "ディープラーニング", "強化学習"]
excerpt: Julia言語を使った強化学習
---

## QEUR22_JRLINT6:　ReinforcementLearning.jlでDeepRLを・・・(その2)

## ～　「回り道」が大事です　～

### ・・・　前回のつづきです　・・・

D先生 ： “ENV(環境)が完成していることが確認されました。これで、あとはエージェント(ポリシー)側をいろいろ変えればいいだけですね。”

![imageJRL1-7-1](/2023-01-08-QEUR22_JRLINT6/imageJRL1-7-1.jpg)

QEU:FOUNDER ： “ちなみに、オフィシャル資料にはさらにCliffWalkingよりもシンプルな「宝くじ(LotteryEnv)」の事例があるので、この事例と比較すれば構造がわかりやすいです。”

![imageJRL1-7-2](/2023-01-08-QEUR22_JRLINT6/imageJRL1-7-2.jpg)

D先生 ： “あとは「彼のいうAgent構造（↑）」にディープラーニングを載せればいいだけなんだが・・・。”

QEU:FOUNDER （設定65歳） ： “ちなみに、もうやってみました。・・・ドン！！おっと、申し遅れました。この番組は、**「高齢者によるイノベーション」**です(笑)。”

![imageJRL1-7-3](/2023-01-08-QEUR22_JRLINT6/imageJRL1-7-3.jpg)

D先生（設定65歳） ： “結果はだめじゃないですか・・・。でも、この問題はなんなのかな？”

QEU:FOUNDER ： “失敗の原因を言えばきりがないです。実は・・・。でも、それを理解するには、ENV（環境）がどのようなデータを発生させるかを見るのが近道です。ここで、前回のENVに以下のコードを使って、ENVを動かしてみましょう。”

```julia
# -----
n_episode = 10

for epi in 1:n_episode
	count = 1
	while true
		action = rand(action_space(env))
        env(action)
		println("EPISODE: ", epi, " ,COUNT: ", count, " ,ACT: ", action, " ,ENV: ",state(env), " ,REWARD: ",reward(env))
		is_terminated(env) && break
		count = count + 1
	end
end
```

QEU:FOUNDER ： “仮に、我々がPythonで強化学習をやっているとして、**環境側の「この出力(↓)」はまずい**でしょう。”

![imageJRL1-7-4](/2023-01-08-QEUR22_JRLINT6/imageJRL1-7-4.jpg)

D先生 ： “あら～、**ワンホット(onehot)にすべき**だったか・・・。”

QEU:FOUNDER ： “まあ、他にもいろいろ（問題が）ありますよ。そこで、このようにプログラムを改造していました。ドン！！”

```julia
# ---
# developing_Cliff_Walking_DQN
# これは成功するか？(まだわからない)
# ---
using ReinforcementLearning
using Flux
using Flux.Losses
using StableRNGs

# ----
# Cliff_Walking
# ----
NX = 4
NY = 12
Start = [4.0, 1.0]
Goal = [4.0, 12.0]
LRUD = [
	[0.0, -1.0],  # left
	[0.0, 1.0],   # right
	[-1.0, 0.0],  # up
	[1.0, 0.0],   # down
]

# ------
function iscliff(p)
    x, y = p[1], p[2]
    x == 4 && y > 1 && y < NY
end

# ------
Base.@kwdef mutable struct CliffWalkingEnv <: AbstractEnv
	position::Array{Float32} = Start
end
function (env::CliffWalkingEnv)(a::Int)
	x, y = env.position[1] + LRUD[a][1], env.position[2] + LRUD[a][2]
	env.position::Array{Float32} = [min(max(x, 1), NX), min(max(y, 1), NY)]
end

# ------
RLBase.state(env::CliffWalkingEnv) = env.position
RLBase.state_space(env::CliffWalkingEnv) = Base.OneTo(length(env.position))
# ----
RLBase.action_space(env::CliffWalkingEnv) = Base.OneTo(length(LRUD))
RLBase.reward(env::CliffWalkingEnv) = env.position == Goal ? 0.0 : (iscliff(env.position) ? -100.0 : -1.0)
RLBase.is_terminated(env::CliffWalkingEnv) = env.position == Goal || iscliff(env.position)
RLBase.reset!(env::CliffWalkingEnv) = env.position = Start
env = CliffWalkingEnv()

# -----
# SIMULATION
# -----
n_episode = 10

for epi in 1:n_episode
	count = 1
	while true
		action = rand(action_space(env))
        env(action)
		println("EPISODE: ", epi, " ,COUNT: ", count, " ,ACT: ", action, " ,ENV: ",state(env), " ,REWARD: ",reward(env))
		is_terminated(env) && break
		count = count + 1
	end
end

```

QEU:FOUNDER ： “前回のテスト出力の結果と比較してほしい。前回の問題がかなり改良されています。**Stateは2次元にしているので、48次元のワンホットでなくていいのか**の疑問はあるんだけどさ・・・。”

![imageJRL1-7-5](/2023-01-08-QEUR22_JRLINT6/imageJRL1-7-5.jpg)

D先生 ： “前回のENV(state)の出力はCartesianIndexでしたよね、あの変数属性であればFluxには入力できなかったでしょう。そうか・・・、**同じ強化学習でも「テーブル解法」と「ディープラーニング解法」では全然違う**んだ。”

QEU:FOUNDER ： “あとは、軽い気持ちで見てほしい。これも良いかどうかわからないだけどさ・・・。”

```julia
# -----
# Agent DQN
# -----
seed = 12345
rng = StableRNG(seed)
ns, na = 2, 4

agent = Agent(
    policy = QBasedPolicy(
        learner = BasicDQNLearner(
            approximator = NeuralNetworkApproximator(
                model = Chain(
                    Dense(ns, 128, relu; initW = glorot_uniform(rng)),
                    Dense(128, na; initW = glorot_uniform(rng)),
                ) |> cpu,
                optimizer = ADAM(),
            ),
            batch_size = 32,
            min_replay_history = 100,
            loss_func = huber_loss,
            rng = rng,
        ),
        explorer = EpsilonGreedyExplorer(
            kind = :exp,
            ϵ_stable = 0.01,
            decay_steps = 500,
            rng = rng,
        ),
    ),
    trajectory = CircularArraySARTTrajectory(
        capacity = 3000,
        state = Vector{Float32} => (ns,),
    ),
)

# ---
# miscellaneous
stop_condition = StopAfterStep(80000, is_show_progress=!haskey(ENV, "CI"))
hook = TotalRewardPerEpisode()

# ---
# trial run
using Plots

run(agent, env, stop_condition, hook)
plot(hook.rewards)
```

![imageJRL1-7-6](/2023-01-08-QEUR22_JRLINT6/imageJRL1-7-6.jpg)

D先生 ： “一応うごきました。でかした！！でも・・・、なんか微妙な結果ですね。本当にうまくいっているのかな？”

QEU:FOUNDER ： “ドキュメンテーションが充実していないので、評価の部分がうまく行ってないんです。なによりも、コンパイラ言語なのでエラー・メッセージの解析が難しいのはキツイよね。”

D先生 ： “まあ、ともあれ途中経過を出しておくのも重要だと・・・。”

![imageJRL1-7-7](/2023-01-08-QEUR22_JRLINT6/imageJRL1-7-7.jpg)

QEU:FOUNDER ： “RL.Agentの構造がコード上でどのように表現されるかを理解しておくことが最も重要です。次は、Cliff_Walkingから外れて、ちょっと別の事をしますよ。”

D先生 ： “えっ！？（Cliff_Walkingの開発）を続けてやらないんですか？あと、この強化学習事例(↑)では、たった2行の命令で実行できるのですか？”

[![MOVIE1](http://img.youtube.com/vi/nL7f68DCQwg/0.jpg)](http://www.youtube.com/watch?v=nL7f68DCQwg "Building Interactive REPL-based Visualizations in GridWorlds.jl | Siddharth Bhatia | JuliaCon2021")

QEU:FOUNDER ： “まあ、それは後で・・・。**RL.jl用のメイズ(迷路)系のツール・パッケージ(GridWorlds.jl)**があるので、まずはそれを使ってみようと思います。世の中、回り道が近道になることもあります。”

D先生 ： “そんなもんですかねえ・・・。”


## ～　まとめ　～

QEU:FOUNDER ： “「例の東の横綱」の動画で、超ハッカー様が言っているでしょ？彼らは**「鶏と卵の問題」**に直面しているって・・・。”

[![MOVIE2](http://img.youtube.com/vi/ckIIxKM14Ow/0.jpg)](http://www.youtube.com/watch?v=ckIIxKM14Ow "ReinforcementLearning jl | Jun Tian | Julia User Group Munich - Share&Code")

C部長 : “この動画（↑）では、**ユーザーとコントリビュータ**の数のことですね。鶏はどちらなんですかね？”

QEU:FOUNDER ： “現時点でいえば、ユーザーが鶏だと思うよ。**「鶏は餌をたべれば太る」**し・・・（笑）。必要なのは、誰かがとりあえず（RL.ｊｌを）使うこと。さらにいえば、失敗しても成功してもいいのでWeb上に情報を提供することです。その事例が成功や失敗であるかについては、あまり重要じゃないです。成功失敗は読者（ユーザー）がPC上で動かして確認すればいいだけです。だから、我々はJulia言語版のQEUシステムの開発を、あえて回り道させながらやっているんです。”

C部長 : “うまい人がやれば、却ってドキュメントが難しくなったり、文章が少なかったりして逆効果になることもある。”

QEU:FOUNDER ： “B級グルメでも、とりあえず餌は多い方がいいの・・・（笑）。もったいぶって、**一握りの人間が技術を抱え込み、30年間もそれを進歩させないトコよりもずっとマシ**・・・。”

