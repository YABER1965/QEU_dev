---
title: QEUR22_JRLINT3:　POMDPs.jlのこととか・・・
date: 2023-01-04
tags: ["QEUシステム", "メトリックス", "Julia言語", "機械学習", "Flux", "ディープラーニング", "強化学習"]
excerpt: Julia言語を使った強化学習
---

## QEUR22_JRLINT3:　POMDPs.jlのこととか・・・

## ～　まるで「PyTorch」みたい・・・　～

QEU:FOUNDER （設定65歳） ： “さあて、この番組は「高齢者によるイノベーション」です。今回はPOMDPs.jlを使ってみましょう。まずは、この参考動画（↓）から始めましょう。”

[![MOVIE1](http://img.youtube.com/vi/uHEjez97BvE/0.jpg)](http://www.youtube.com/watch?v=uHEjez97BvE "MDPs: Markov Decision Processes | Decision Making Under Uncertainty using POMDPs.jl")

QEU:FOUNDER ： “ここでは、非常に簡単なGridWorldを解いてみましょう。”

![imageJRL1-4-1](/2023-01-04-QEUR22_JRLINT3/imageJRL1-4-1.jpg)

D先生 ： “Juliaユーザーって、レベルが高いですね。これがComplete Beginnerなんですから・・・。Grid Worldは前回も取り上げましたよね・・・。”

[![MOVIE1](http://img.youtube.com/vi/GXnCm_6uyeM/0.jpg)](http://www.youtube.com/watch?v=GXnCm_6uyeM "[05x12] Markov Decision Process (MDP) with POMDPs.jl | Julia Reinforcement Machine Learning")

QEU:FOUNDER ： “今回のコードの方がPOMDPs.jlの本来の使い方に近いんです。”

![imageJRL1-4-2](/2023-01-04-QEUR22_JRLINT3/imageJRL1-4-2.jpg)

D先生 ： “なんか、PyTorchのやり方に近いですね”

QEU:FOUNDER ： “それでは、プログラムをドン！！引用元とコードが全く同じですからね。ここでは、コードのコピペをしやすいようにまとめただけです。”

```julia
####
# MDPの例題です - Grid World
# Grid World Tutorial: POMDPs.jl for Complete Beginners
####
# first import the POMDPs.jl interface
using POMDPs
# POMDPModelTools has tools that help build the MDP definition
using POMDPModelTools
# POMDPPolicies provides functions to help define simple policies
using POMDPPolicies
# POMDPSimulators provide functions for running MDP simulations
using POMDPSimulators

# -----
# States
struct GridWorldState 
    x::Int64 # x position
    y::Int64 # y position
    done::Bool # are we in a terminal state?
end

# initial state constructor
GridWorldState(x::Int64, y::Int64) = GridWorldState(x,y,false)
# checks if the position of two states are the same
posequal(s1::GridWorldState, s2::GridWorldState) = s1.x == s2.x && s1.y == s2.y

# -----
# Actions
#action = :up # can also be :down, :left, :right

# -----
# MDP
# the grid world mdp type
mutable struct GridWorld <: MDP{GridWorldState, Symbol} # Note that our MDP is parametarized by the state and the action
    size_x::Int64 # x size of the grid
    size_y::Int64 # y size of the grid
    reward_states::Vector{GridWorldState} # the states in which agent recieves reward
    reward_values::Vector{Float64} # reward values for those states
    tprob::Float64 # probability of transitioning to the desired state
    discount_factor::Float64 # disocunt factor
end

#we use key worded arguments so we can change any of the values we pass in 
function GridWorld(;sx::Int64=10, # size_x
                    sy::Int64=10, # size_y
                    rs::Vector{GridWorldState}=[GridWorldState(4,3), GridWorldState(4,6), GridWorldState(9,3), GridWorldState(8,8)], # reward states
                    rv::Vector{Float64}=rv = [-10.,-5,10,3], # reward values
                    tp::Float64=0.7, # tprob
                    discount_factor::Float64=0.9)
    return GridWorld(sx, sy, rs, rv, tp, discount_factor)
end

# we can now create a GridWorld mdp instance like this:
mdp = GridWorld()
mdp.reward_states # mdp contains all the default values from the constructor

```

![imageJRL1-4-3](/2023-01-04-QEUR22_JRLINT3/imageJRL1-4-3.jpg)

```julia

# -----
# State Space
function POMDPs.states(mdp::GridWorld)
    s = GridWorldState[] # initialize an array of GridWorldStates
    # loop over all our states, remeber there are two binary variables:
    # done (d)
    for d = 0:1, y = 1:mdp.size_y, x = 1:mdp.size_x
        push!(s, GridWorldState(x,y,d))
    end
    return s
end;

mdp = GridWorld()
state_space = states(mdp);
state_space[1]

# -----
# Action Space
POMDPs.actions(mdp::GridWorld) = [:up, :down, :left, :right];

# -----
# Transition Model
# transition helpers
function inbounds(mdp::GridWorld,x::Int64,y::Int64)
    if 1 <= x <= mdp.size_x && 1 <= y <= mdp.size_y
        return true
    else
        return false
    end
end

inbounds(mdp::GridWorld, state::GridWorldState) = inbounds(mdp, state.x, state.y);

function POMDPs.transition(mdp::GridWorld, state::GridWorldState, action::Symbol)
    a = action
    x = state.x
    y = state.y
    
    if state.done
        return SparseCat([GridWorldState(x, y, true)], [1.0])
    elseif state in mdp.reward_states
        return SparseCat([GridWorldState(x, y, true)], [1.0])
    end

    neighbors = [
        GridWorldState(x+1, y, false), # right
        GridWorldState(x-1, y, false), # left
        GridWorldState(x, y-1, false), # down
        GridWorldState(x, y+1, false), # up
        ] # See Performance Note below
    
    targets = Dict(:right=>1, :left=>2, :down=>3, :up=>4) # See Performance Note below
    target = targets[a]
    
    probability = fill(0.0, 4)

    if !inbounds(mdp, neighbors[target])
        # If would transition out of bounds, stay in
        # same cell with probability 1
        return SparseCat([GridWorldState(x, y)], [1.0])
    else
        probability[target] = mdp.tprob

        oob_count = sum(!inbounds(mdp, n) for n in neighbors) # number of out of bounds neighbors

        new_probability = (1.0 - mdp.tprob)/(3-oob_count)

        for i = 1:4 # do not include neighbor 5
            if inbounds(mdp, neighbors[i]) && i != target
                probability[i] = new_probability
            end
        end
    end

    return SparseCat(neighbors, probability)
end;
     
# -----
# Reward Model
function POMDPs.reward(mdp::GridWorld, state::GridWorldState, action::Symbol, statep::GridWorldState) #deleted action
    if state.done
        return 0.0
    end
    r = 0.0
    n = length(mdp.reward_states)
    for i = 1:n
        if posequal(state, mdp.reward_states[i])
            r += mdp.reward_values[i]
        end
    end
    return r
end;

# -----
# Miscallenous Functions
POMDPs.discount(mdp::GridWorld) = mdp.discount_factor;

function POMDPs.stateindex(mdp::GridWorld, state::GridWorldState)
    sd = Int(state.done + 1)
    ci = CartesianIndices((mdp.size_x, mdp.size_y, 2))
    return LinearIndices(ci)[state.x, state.y, sd]
end

function POMDPs.actionindex(mdp::GridWorld, act::Symbol)
    if act==:up
        return 1
    elseif act==:down
        return 2
    elseif act==:left
        return 3
    elseif act==:right
        return 4
    end
    error("Invalid GridWorld action: $act")
end;

POMDPs.isterminal(mdp::GridWorld, s::GridWorldState) = s.done
POMDPs.initialstate(pomdp::GridWorld) = Deterministic(GridWorldState(1,1)) # TODO: define initial-istate for states, not distributions?

```

D先生 ： “ここまでが**問題(problem)の定義**です。”

QEU:FOUNDER ： “今回の例題はテーブルによる解法です。ディープラーニングを使うには後ろのコードを変えていくわけです。”

```julia

# -----
# Simulations
mdp = GridWorld()
mdp.tprob=1.0

policy = RandomPolicy(mdp)
left_policy = FunctionPolicy(s->:left)
right_policy = FunctionPolicy(s->:right)

for (s,a,r) in stepthrough(mdp, right_policy, "s,a,r", max_steps=10)
    @show s
    @show a
    @show r
    println()
end
```

![imageJRL1-4-4](/2023-01-04-QEUR22_JRLINT3/imageJRL1-4-4.jpg)

```julia
# -----
# Value Iteration Solver
# first let's load the value iteration module
using DiscreteValueIteration

# initialize the problem
mdp = GridWorld()

# initialize the solver
# max_iterations: maximum number of iterations value iteration runs for (default is 100)
# belres: the value of Bellman residual used in the solver (defualt is 1e-3)
solver = ValueIterationSolver(max_iterations=100, belres=1e-4; verbose=true)

# solve for an optimal policy
policy = solve(solver, mdp); 
```

![imageJRL1-4-5](/2023-01-04-QEUR22_JRLINT3/imageJRL1-4-5.jpg)

```julia
# -----
# say we are in state (9,2)
s = GridWorldState(9,2)
a = action(policy, s)

# -----
s = GridWorldState(8,3)
a = action(policy, s)
```

![imageJRL1-4-6](/2023-01-04-QEUR22_JRLINT3/imageJRL1-4-6.jpg)

```julia
# -----
# output chess-path
for (s,a,r) in stepthrough(mdp, policy, "s,a,r", max_steps=30)
    @show s
    @show a
    @show r
    println()
end

```

![imageJRL1-4-7](/2023-01-04-QEUR22_JRLINT3/imageJRL1-4-7.jpg)


D先生 ： “なるほど、こんな感じでパッケージを使うんですね。前半のGrid Worldの定義を改造するのは結構、簡単そうです。”

[![MOVIE3](http://img.youtube.com/vi/ITzRizRpHzI/0.jpg)](http://www.youtube.com/watch?v=ITzRizRpHzI "[05x13] SARSA and Q-learning Algorithms with POMDPs.jl | Julia Reinforcement Machine Learning")

QEU:FOUNDER ： “後半部分では、SARSAやQ-learningなどの他のスキームも使ってみたいですね。”



## ～　まとめ　～

QEU:FOUNDER ： “これ・・・、以前見ました。もう**何回も**・・・。”

[![MOVIE4](http://img.youtube.com/vi/Ak7IsdFXBLg/0.jpg)](http://www.youtube.com/watch?v=Ak7IsdFXBLg "安冨歩経済学を語る。2023年日本はどうなるのか？リカード、アダムスミス、ケインズ、マルクス。人々の意識と納得と価格について。安冨歩東大教授。一月万冊")

C部長 : “FOUNDERも下手の横好きですね。そして、なにか得たものは？”

QEU:FOUNDER ： “経済学者って、経済を悪くすることはできるが、良くすることはできないだろうとおもいました。このY先生は、その中でも**「（経済を）良くしよう！！」と思っている稀有な人**で・・・。”

[![MOVIE5](http://img.youtube.com/vi/9LK1tvlZ9MU/0.jpg)](http://www.youtube.com/watch?v=9LK1tvlZ9MU "新年特別講義・2023年生き残る知恵。生きるための探求。時間論、経済学、複雑系、宗教、満州史、ハラスメント、女性装、馬、キリスト教、再洗礼派、アーミッシュ･･･安冨歩東大教授。一月万冊")

C部長 : “さすがに、こうなると「経済学じゃない」でしょうに・・・。”

QEU:FOUNDER ： “実用的には、いらない学問なんでしょう・・・。たぶん・・・。”

